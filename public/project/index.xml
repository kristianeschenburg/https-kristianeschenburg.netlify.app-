<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | A Rambling On</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 25 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/Bayes.jpg</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>parcellearning</title>
      <link>/project/parcellearning/</link>
      <pubDate>Fri, 25 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/project/parcellearning/</guid>
      <description>&lt;p&gt;This is a python package to apply various graph neural network models to brain connectivity data to learn cortical brain maps.  These models address the issue of node classification, as commonly seen in the graph representation learning community.  These models are based on the 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Current models include the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Graph Convolution Network (
&lt;a href=&#34;https://arxiv.org/abs/1609.02907&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Graph Attention Network (
&lt;a href=&#34;https://arxiv.org/abs/1710.10903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Gaussian Kernel Graph Convolution Network (
&lt;a href=&#34;http://arxiv.org/abs/1803.10336&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Constrained Graph Attention Network (
&lt;a href=&#34;https://arxiv.org/abs/1910.11945&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Jumping Knowledge Representation Learning (
&lt;a href=&#34;https://arxiv.org/pdf/1806.03536.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>submet</title>
      <link>/project/submet/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/project/submet/</guid>
      <description>&lt;p&gt;See my 
&lt;a href=&#34;/post/comparing-subspaces/&#34;&gt;post&lt;/a&gt; with a brief algorithmic introduction for computing distances between pairs of subspaces.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;submet&lt;/code&gt; is a Python package for computing pairwise distances between equidimensional subspaces.  All of these subspace metrics are dependent on the principle angles between the two subspaces.  &lt;code&gt;submet&lt;/code&gt; implements an 
&lt;a href=&#34;&#34;&gt;sklearn&lt;/a&gt;-styled interface with a fitting method via a class called &lt;code&gt;SubspaceDistance&lt;/code&gt;, along with a variety of metrics in a class called &lt;code&gt;Metric&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import submet

# generate random subspaces
s1 = np.random.rand((10, 2))
s2 = np.random.rand((10, 2))

# instantiate SubspaceDistance object, using the Grassmann distance metric
S = submet.subspaces.SubspaceDistance(metric=&#39;Grassmann&#39;)

# compute the distance between two subspaces
S.fit(s1, s2)

# print computed distance
print(S.distance_)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Metric&lt;/code&gt; implements the following distance metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Asimov&lt;/li&gt;
&lt;li&gt;Binet-Cauchy&lt;/li&gt;
&lt;li&gt;Chordal&lt;/li&gt;
&lt;li&gt;Fubini-Study&lt;/li&gt;
&lt;li&gt;Martin&lt;/li&gt;
&lt;li&gt;Procrustes&lt;/li&gt;
&lt;li&gt;Projection&lt;/li&gt;
&lt;li&gt;Spectral&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;SubspaceDistance&lt;/code&gt; does not currently support pairwise distance computations between more than a single pair of subspaces, in an analogous way to 
&lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scipy&amp;rsquo;s&lt;/a&gt; 
&lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pdist&lt;/a&gt; or 
&lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html#scipy.spatial.distance.cdist&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cdist&lt;/a&gt; methods.  I am working on allowing pairwise computations using 
&lt;a href=&#34;https://numba.pydata.org/numba-doc/0.11/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;numba&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update as of 6/30/2020: the above issue has been fixed and &lt;code&gt;submet&lt;/code&gt; now allows for pairwise distance matrix computations, rather than only single subspace pair comparisons.  See this 
&lt;a href=&#34;https://github.com/kristianeschenburg/submet/pull/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pull request&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ddCRP</title>
      <link>/project/ddcrp/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/project/ddcrp/</guid>
      <description>&lt;p&gt;This is a package to apply the distance-dependent Chinese Restaurant Process (dd-CRP) to multi-dimensional graph-based data.  It is based roughly on code originally written by 
&lt;a href=&#34;https://github.com/cbaldassano/Parcellating-connectivity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christopher Baldassano&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My contributions to this package are three-fold:  In contrast to work presented by 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/25737822&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baldassano et al.&lt;/a&gt; and 
&lt;a href=&#34;https://arxiv.org/abs/1703.00981&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moyer et al.&lt;/a&gt;, whose dd-CRP methods both model the univariate similarities within and between clusters, this method models the clusters themselves, placing multidimensional priors on the features of each cluster.  It then explores the posterior parameter space using &lt;strong&gt;Collased Gibbs Sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;This version treats the high-dimensional feature vectors as being sampled from multivariate Gaussian distributions.  In contrast to 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/25737822&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baldassano et al.&lt;/a&gt; who sample similarities &lt;em&gt;between&lt;/em&gt; feature vectors from a univariate Gaussian, and 
&lt;a href=&#34;https://arxiv.org/abs/1703.00981&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moyer et al.&lt;/a&gt; who sample counts from a Poisson, this method &lt;strong&gt;models the full-dimensional data points themselves&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The prior distribution on features are based on an &lt;strong&gt;abstract class&lt;/strong&gt; contained in &lt;code&gt;PriorsBase.py&lt;/code&gt; called &lt;code&gt;Prior&lt;/code&gt; with the following 3 &lt;strong&gt;abstract methods&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Prior.sufficient_statistics&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Prior.posterior_parameters&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Prior.marginal_evidence&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Any model that implements the above three methods can be incorporated into &lt;code&gt;Priors.py&lt;/code&gt; alongside the &lt;em&gt;Normal-Inverse-Chi-Squared&lt;/em&gt; (&lt;code&gt;NIX2&lt;/code&gt;) and &lt;em&gt;Normal-Inverse-Wishart&lt;/em&gt; (&lt;code&gt;NIW&lt;/code&gt;) models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On a more aesthetic level, I have &lt;strong&gt;refactored&lt;/strong&gt; much of Balassano&amp;rsquo;s original Python code to make this version &lt;strong&gt;object-oriented&lt;/strong&gt; and more user-friendly under the hood.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;how-to-install-and-use&#34;&gt;How to install and use:&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/kristianeschenburg/ddCRP.git
cd  ./ddCRP
pip install .
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example-application-to-synthetic-data&#34;&gt;Example application to synthetic data:&lt;/h2&gt;
&lt;p&gt;We begin by importing the necessary modules.  Specifically, &lt;code&gt;ddCRP&lt;/code&gt; contains the actual algorithm, &lt;code&gt;Priors&lt;/code&gt; contain the prior distribution models, and &lt;code&gt;synthetic&lt;/code&gt; contains a suite of methods for generating synthetic data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from ddCRP import ddCRP
from ddCRP import Priors
from ddCRP import synthetic
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we define the hyperparameters, and instantiate our prior model with these parameters.  In this case, we are using the &lt;em&gt;Normal-Inverse-Chi-Squared&lt;/em&gt; model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;alpha=10
mu = 0
kappa = 0.0001
nu = 1
sigma = 1

nix = Priors.NIX2(mu0=mu, kappa0=kappa, nu0=nu, sigma0=sigma)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we sample &lt;strong&gt;5-dimensional&lt;/strong&gt; synthetic data from the same prior distribution.  The &lt;code&gt;synth&lt;/code&gt; object computes an adjacency list of our synthetic data (&lt;code&gt;synth.adj_list&lt;/code&gt;), as well as a ground-truth label map (&lt;code&gt;synth.z_&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# dimensionality of data
d = 5

# sample synthetic features for each label
# If you want to sample synthetic data from a different
# Normal-Inverse-Chi-Squared distribution,
# change kappa, mu, nu, and sigma
synth = synthetic.SampleSynthetic(kind=&#39;ell&#39;, d=d, mu0=mu, kappa0=kappa, nu0=nu, sigma0=sigma)
synth.fit()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we instantiate our &lt;code&gt;ddCRP&lt;/code&gt; model with the concentration parameter &lt;code&gt;alpha&lt;/code&gt;, our prior model &lt;code&gt;nix&lt;/code&gt;, along with some other parameters that govern the number of MCMC passes to perform and how often to sample diagnostic statistics about our model performance.  You can see&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# fit the ddCRP model
# once fitted, crp.map_z_ is the MAP label
crp = ddCRP.ddCRP(alpha, model=nix, mcmc_passes=30, stats_interval=200)
crp.fit(synth.features_, synth.adj_list, gt_z=synth.z_)
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-model-convergence-in-synthetic-data&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/project/ddcrp/ell_hu198df877147cc727f2f21bf1ceacd1a3_58174_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Model convergence in synthetic data.&#34;&gt;


  &lt;img data-src=&#34;/project/ddcrp/ell_hu198df877147cc727f2f21bf1ceacd1a3_58174_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;720&#34; height=&#34;360&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model convergence in synthetic data.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;For more information on the Chinese Restaurant Process, see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Baldassano et al. (2015), Parcellating Connectivity In Spatial Maps. PeerJ 3:e784; DOI 10.7717/peerj.784&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moyer et al. (2017), A Restaurant Process Mixture Model for Connectivity Based Parcellation of the Cortex. 	arXiv:1703.00981&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Blei, David M. et al (2010), The Nested Chinese Restaurant Process and Bayesian
Nonparametric Inference of Topic Hierarchies. JACM.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>fieldmodel</title>
      <link>/project/fieldmodel/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/project/fieldmodel/</guid>
      <description>&lt;p&gt;This is a Python package for fitting densities to scalar fields distributed on the domain of regular meshes.  This package is an implementation of the methodology described 
&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/23110879/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.  Rather than using the spatial coordinates of the mesh on which the data is distributed, this package fits isotropic distributions using the geodesic distances between mesh vertices.&lt;/p&gt;
&lt;p&gt;Given a scalar field assigning, let&amp;rsquo;s say, elevation values to latitude-longitude coordinates of Mount Rainier, we could use &lt;strong&gt;fieldmodel&lt;/strong&gt; to fit a Gaussian distribution to this elevation profile.  The output of this fitting procedure would be 2 parameters: a &lt;code&gt;mean&lt;/code&gt; parameter (centered on the true summit coordinates) and a &lt;code&gt;sigma&lt;/code&gt; parameter, a measure of how quickly the elevation profile decays as we move away from the summit.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-model-convergence-in-synthetic-data&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/project/fieldmodel/rainier_hu5d2e9382f73667cb57e9f1f1ea45c0f7_123667_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Model convergence in synthetic data.&#34;&gt;


  &lt;img data-src=&#34;/project/fieldmodel/rainier_hu5d2e9382f73667cb57e9f1f1ea45c0f7_123667_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;494&#34; height=&#34;490&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model convergence in synthetic data.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The code iterates over all possible &lt;code&gt;mean&lt;/code&gt; locations in a set, and optimizes the &lt;code&gt;sigma&lt;/code&gt; parameter using a &lt;strong&gt;BFGS&lt;/strong&gt; minimization procedure.  I provide 3  optimality criteria options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;maximization of spatial correlation&lt;/li&gt;
&lt;li&gt;maximization of signal amplitude&lt;/li&gt;
&lt;li&gt;minimization of least-squares error&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Along with fitting procedure, &lt;strong&gt;fieldmodel&lt;/strong&gt; also offers some basic plotting functionality to visualize the fitted densities alongside the data to which those densities were fit.&lt;/p&gt;
&lt;h2 id=&#34;how-to-install-and-use&#34;&gt;How to install and use:&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/kristianeschenburg/fieldmodel.git
cd  ./fieldmodel
pip install .
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;example-application-to-synthetic-data&#34;&gt;Example application to synthetic data:&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import scipy.io as sio

# Create fake distance matrix between and fake scalar map

# x coordinates for plotting
tx_file = &#39;../data/target.X.mat&#39;
tx = sio.loadmat(tx_file)[&#39;x&#39;].squeeze()

# y coordinates for plotting
ty_file = &#39;../data/target.Y.mat&#39;
ty = sio.loadmat(ty_file)[&#39;y&#39;].squeeze()

# geodesic distance matrix between vertices
dist_file = &#39;../data/distance.mat&#39;
dist = sio.loadmat(dist_file)[&#39;apsp&#39;]

# scalar field
field_file = &#39;../data/scalar_field.mat&#39;
field = sio.loadmat(field_file)[&#39;field&#39;].squeeze()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we can instantiate and fit our fieldmodel.  For more detail on the parameters, please refer to the &lt;code&gt;docstring&lt;/code&gt; or &lt;code&gt;Demo.ipynb&lt;/code&gt; in the &lt;code&gt;demos&lt;/code&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fieldmodel import GeodesicFieldModel as GFM

# instantiate field model
G = GFM.FieldModel(r=10, amplitude=False,
                    peak_size=15, hood_size=20,
                    verbose=False, metric=&#39;pearson&#39;)

# fit field model
G.fit(data=field, distances=dist, x=tx, y=ty)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the results of our model via the &lt;code&gt;plot&lt;/code&gt; method in &lt;code&gt;GFM.fieldmodel&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;G.plot(field=&#39;pdf&#39;)
G.plot(field=&#39;amplitude&#39;)
G.plot(field=&#39;sigma&#39;)
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-fitted-fieldmodel-density&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/project/fieldmodel/pdf_hu5198acae7b46ecb9e697721f643f0ffb_61407_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Fitted fieldmodel density.&#34;&gt;


  &lt;img data-src=&#34;/project/fieldmodel/pdf_hu5198acae7b46ecb9e697721f643f0ffb_61407_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;647&#34; height=&#34;302&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fitted fieldmodel density.
  &lt;/figcaption&gt;


&lt;/figure&gt;






  
  











&lt;figure id=&#34;figure-estimated-model-amplitude-for-each-candidate-mean-location&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/project/fieldmodel/amplitude_huefc18c93caf304dc635a2b50ed6a8bbc_60179_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Estimated model amplitude for each candidate mean location.&#34;&gt;


  &lt;img data-src=&#34;/project/fieldmodel/amplitude_huefc18c93caf304dc635a2b50ed6a8bbc_60179_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;641&#34; height=&#34;286&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Estimated model amplitude for each candidate mean location.
  &lt;/figcaption&gt;


&lt;/figure&gt;






  
  











&lt;figure id=&#34;figure-estimated-model-sigma-for-each-candidate-mean-location&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/project/fieldmodel/sigma_huefc18c93caf304dc635a2b50ed6a8bbc_57686_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Estimated model sigma for each candidate mean location.&#34;&gt;


  &lt;img data-src=&#34;/project/fieldmodel/sigma_huefc18c93caf304dc635a2b50ed6a8bbc_57686_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;625&#34; height=&#34;286&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Estimated model sigma for each candidate mean location.
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
  </channel>
</rss>
