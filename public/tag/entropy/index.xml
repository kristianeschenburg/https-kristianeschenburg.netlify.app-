<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>entropy | A Rambling On</title>
    <link>/tag/entropy/</link>
      <atom:link href="/tag/entropy/index.xml" rel="self" type="application/rss+xml" />
    <description>entropy</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 08 Oct 2021 15:24:17 -0700</lastBuildDate>
    <image>
      <url>/img/Bayes.jpg</url>
      <title>entropy</title>
      <link>/tag/entropy/</link>
    </image>
    
    <item>
      <title>Entropy and Mutual Information</title>
      <link>/post/information/</link>
      <pubDate>Fri, 08 Oct 2021 15:24:17 -0700</pubDate>
      <guid>/post/information/</guid>
      <description>&lt;meta name=&#34;thumbnail&#34; content=&#34;featured.png&#34;&gt;
&lt;p&gt;I&amp;rsquo;m interested in looking at some spatial mappings between pairs of cortical regions, and believe that these mappings are mediated, to some degree, by the temporal coupling between cortical areas.  I don&amp;rsquo;t necessarily know the functional form of these mappings, but neurobiologically predict that these mappings are not random and have some inherent structure.  I want to examine the relationship between spatial location and strength of temporal coupling.  I&amp;rsquo;m going to use mutual information to measure this association.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s been a while since I&amp;rsquo;ve worked with information-based statistics, so I thought I&amp;rsquo;d review some proofs here.&lt;/p&gt;
&lt;h1 id=&#34;entropy&#34;&gt;Entropy&lt;/h1&gt;
&lt;p&gt;Given a random variable $X$, we define the entropy of $X$ as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
H(X) = - \sum_{x} p(x) \cdot log(p(x))
\end{align}$$&lt;/p&gt;
&lt;p&gt;Entropy measures the degree of uncertainty in a probability distribution.  It is independent of the values $X$ takes, and is entirely dependent on the density of $X$.  We can think of entropy as measuring how &amp;ldquo;peaked&amp;rdquo; a distribution is. Assume we are given a binary random variable $Y$&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y \sim Bernoulli(p) \\&lt;br&gt;
\end{align}$$&lt;/p&gt;
&lt;p&gt;such that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y =
\begin{cases}
1 &amp;amp; \text{with probability $p$} \\&lt;br&gt;
0 &amp;amp; \text{with probability $1-p$}
\end{cases}
\end{align}$$&lt;/p&gt;
&lt;p&gt;If we compute $H(Y)$ as a function of $p$ and plot this result, we see the canonical curve:&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/information/entropy_hue80009bb191a784785003f9715471fdc_16228_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/information/entropy_hue80009bb191a784785003f9715471fdc_16228_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;424&#34; height=&#34;280&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Immediately evident is that the entropy curve peaks when $p=0.5$.  We are entirely uncertain what value $y$ will take if we have an equal chance of sampling either 0 or 1.  However, when $p = 0$ or $p=1$, we know exactly which value $y$ will take &amp;ndash; we aren&amp;rsquo;t uncertain at all.&lt;/p&gt;
&lt;p&gt;Entropy is naturally related to the conditional entropy.  Given two variables $X$ and $Y$, conditional entropy is defined as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
H(Y|X) &amp;amp;= -\sum_{x}\sum_{y} = p(x,y)  \cdot log(\frac{p(x,y)}{p(x)}) \\&lt;br&gt;
&amp;amp;= -\sum_{x}\sum_{y} p(y|x) \cdot p(x) \cdot log(p(y|x)) \\&lt;br&gt;
&amp;amp;= -\sum_{x} p(x) \sum_{y} p(y|X=x) \cdot log(p(y|X=x))
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $H(Y|X=x) = -\sum_{y} p(y|X=x) \cdot log(p(y|X=x))$, the conditional of entropy of $Y$ given that $X=x$.  Here, we&amp;rsquo;ve used the fact that $p(x,y) = p(y|x) \cdot p(x) = p(x|y) \cdot p(y)$.To compute $H(Y|X)$, we take the weighted average of these conditional entropies, where weights are defined by the marginal probabilities of $X$.&lt;/p&gt;
&lt;h1 id=&#34;mutual-information&#34;&gt;Mutual Information&lt;/h1&gt;
&lt;p&gt;Related to entropy is the idea of mutual information.  Mutual information is a measure of the mutual dependence between two variables.  We can ask the following question:  does knowing something about variable $X$ tell us anything about variable $Y$?&lt;/p&gt;
&lt;p&gt;The mutual information between $X$ and $Y$ is defined as:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
I(X,Y) &amp;amp;= \sum_{x}\sum_{y} p(x,y) \cdot log\Big(\frac{p(x,y)}{p(x) \cdot p(y)}\Big) \\&lt;br&gt;
&amp;amp;= \sum_{x}\sum_{y}p(x,y) \cdot \log(p(x,y)) - \sum_{x}\sum_{y}p(x,y) \cdot log(p(x)) - \sum_{x}\sum_{y}p(x,y) \cdot log(p(y)) \\&lt;br&gt;
&amp;amp;= -H(X,Y) - \sum_{x}p(x) \cdot log(p(x)) - \sum_{y}p(y) \cdot log(p(y)) \\&lt;br&gt;
&amp;amp;= H(X) + H(Y) - H(X,Y)
\end{align}$$&lt;/p&gt;
&lt;p&gt;$I(X,Y)$ is symmetric in $X$ and $Y$:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
I(X,Y) &amp;amp;= \sum_{x}\sum_{y} p(x,y) \cdot log\Big(\frac{p(x,y)}{p(x) \cdot p(y)}\Big) \\&lt;br&gt;
&amp;amp;= \sum_{x}\sum_{y} p(x,y) \cdot log\Big(\frac{p(x|y)}{p(y)}\Big) \\&lt;br&gt;
&amp;amp;= \sum_{x}\sum_{y} p(x,y) \cdot log(p(x|y)) - \sum_{x}\sum_{y}p(x,y) \cdot log(p(x)) \\&lt;br&gt;
&amp;amp;= -H(X|Y) - \sum_{x}\sum_{y} p(x|y) \cdot p(y) \cdot log(p(x)) \\&lt;br&gt;
&amp;amp;= -H(X|Y) - \sum_{x}log(p(x) \sum_{y}p(x|y) \cdot p(y) \\&lt;br&gt;
&amp;amp;= -H(X|Y) - \sum_{x} p(x) \cdot log(p(x)) \\&lt;br&gt;
&amp;amp;= H(X) - H(X|Y) \\&lt;br&gt;
&amp;amp;= H(Y) - H(Y|X)
\end{align}$$&lt;/p&gt;
&lt;p&gt;We interpret the above to mean the following: if we are given any information about X (Y), can we reduce the uncertainty around what Y (X) should be?  We understand how much variability there is in each variable independently &amp;ndash; this is measured by the marginal entropy $H(Y)$.  If knowing $X$ reduces this uncertainty, then the conditional entropy $H(Y|X)$ should be small.  If knowing $X$ does not reduce this uncertainty, then $H(Y|X)$ can be at most as large as $H(Y)$, and we have learned nothing about our dependent variable $Y$.&lt;/p&gt;
&lt;p&gt;Put another way, if $I(X,Y) = H(Y) - H(Y|X)$ is large, then the mutual information between $X$ and $Y$ is large, indicating that $X$ is informative of $Y$.  However, if $I(X,Y)$ is small, then the mutual information is small, and $X$ is not informative of $Y$.&lt;/p&gt;
&lt;h1 id=&#34;application&#34;&gt;Application&lt;/h1&gt;
&lt;p&gt;For my problem, I&amp;rsquo;m given two variables, $Z$ and $C$.  I&amp;rsquo;m interested in examining how knowledge of $C$ might reduce our uncertainty about $Z$.  $C$ itself is defined by a pair of variables, $A$, and $B$, such that we have $C_{1} = (a_{1}, b_{1})$.  $Z$ is distributed over the tensor-product space of $A$ and $B$, that is:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Z = f(A \otimes B)
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $A \otimes B$ is defined as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
A \otimes B =
\begin{bmatrix}
(a_{1},b_{1}) &amp;amp; \dots &amp;amp; (a_{1},b_{k}) \\&lt;br&gt;
\vdots &amp;amp; \ddots &amp;amp; \vdots \\&lt;br&gt;
(a_{p}, b_{1}) &amp;amp; \dots &amp;amp; (a_{p}, b_{k})
\end{bmatrix}
\end{align}$$&lt;/p&gt;
&lt;p&gt;such that $z_{i,j} = f(a_{i}, b_{j}$)&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/information/domain_hudf6752327912411f9654bba4b0b0185d_22907_2000x2000_fit_lanczos_2.png&#34; &gt;


  &lt;img data-src=&#34;/post/information/domain_hudf6752327912411f9654bba4b0b0185d_22907_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;424&#34; height=&#34;424&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We define the mutual information between $Z$ and $C$ as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
I(Z,C) &amp;amp;= \sum_{z}\sum_{c} p(z,c) \cdot log\Big(\frac{p(z,c)}{p(z)p(c)} \Big) \\&lt;br&gt;
&amp;amp;= H(Z) - H(Z|C) \\&lt;br&gt;
&amp;amp;= H(Z) - H(X|(A,B)) \\&lt;br&gt;
&amp;amp;= H(Z) - \sum_{a,b} p(a,b) \sum_{z} p(z|(A,B)=(a,b)) \cdot log(p(z|(A,B)=(a,b)))
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the pair $(a,b)$ represents a bin or subsection of the tensor-product space.  The code for this approach can be found below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def entropy(sample):
    
    &amp;quot;&amp;quot;&amp;quot;
    Compute the entropy of a given data sample.  
    Bins are estimated using `Freedman Diaconis` Estimator.
    
    Args:
        sample: float, NDarray
        data from which to compute entropy of
    Returns:
        H: entropy
    &amp;quot;&amp;quot;&amp;quot;
    
    if np.ndim(sample) &amp;gt; 1:
        sample = np.reshape(sample, np.product(sample.shape))
    
    edges = np.histogram_bin_edges(sample, bins=&#39;fd&#39;)
    [counts,_] = np.histogram(sample, bins=edges)
    
    # compute marginal distribution of sample
    m_sample = counts/counts.sum()
    
    return (-1)*np.nansum(m_sample*np.ma.log2(m_sample))
    

def mutual_information_grid(X,Y,Z):
    
    &amp;quot;&amp;quot;&amp;quot;
    Compute the mutual information of a dependent variable over a grid 
    defined by two indepedent variables.
    
    Args:
        X,Y: float, NDarray
            coordinates over which dependent variable is distributed
        Z: float, array
            dependent variable
    Returns:
        estimates: dict
          keys:
            I: float
                mutual information I(Z; (X,Y))
            W: float, NDarray
                matrix of weighted conditional entropies
            marginal: float
                marginal entropy
            conditional: float
                conditional entropy
    &amp;quot;&amp;quot;&amp;quot;
    
    x_edges = np.histogram_bin_edges(X, bins=&#39;fd&#39;)
    [xc, _] = np.histogram(X, bins=x_edges)
    xc = xc/xc.sum()
    nx = x_edges.shape[0]-1
    
    y_edges = np.histogram_bin_edges(Y, bins=&#39;fd&#39;)
    [yc, _] = np.histogram(Y, bins=y_edges)
    yc = yc/yc.sum()
    ny = y_edges.shape[0]-1
    
    # matrix of conditional entropies for each bin
    H = np.zeros((nx, ny))
    
    # compute pairwise marginal probability of X/Y bins
    mxy = xc[:,None]*yc[None,:]
    
    for x_bin in range(nx):
        for y_bin in range(ny):
            
            x_idx = np.where((X&amp;gt;=x_edges[x_bin]) &amp;amp; (X&amp;lt;x_edges[x_bin+1]))[0]
            y_idx = np.where((Y&amp;gt;=y_edges[y_bin]) &amp;amp; (Y&amp;lt;y_edges[y_bin+1]))[0]
            
            bin_samples = Z[x_idx,:][:,y_idx]
            bin_samples = np.reshape(bin_samples, 
                                     np.product(bin_samples.shape))
            
            H[x_bin, y_bin] = entropy(bin_samples)

    W = H*mxy
    conditional = np.nansum(W)
    marginal = entropy(Z)
    
    I = marginal - conditional
    
    estimates = {&#39;mi&#39;: I,
                 &#39;weighted-conditional&#39;: W,
                 &#39;marginal&#39;: marginal,
                 &#39;conditional&#39;: conditional}
    
    return estimates
    
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
