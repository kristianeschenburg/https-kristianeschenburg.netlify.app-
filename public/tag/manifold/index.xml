<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>manifold | A Rambling On</title>
    <link>/tag/manifold/</link>
      <atom:link href="/tag/manifold/index.xml" rel="self" type="application/rss+xml" />
    <description>manifold</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 24 Jun 2020 15:27:11 -0700</lastBuildDate>
    <image>
      <url>/img/Bayes.jpg</url>
      <title>manifold</title>
      <link>/tag/manifold/</link>
    </image>
    
    <item>
      <title>Distances Between Subspaces</title>
      <link>/post/comparing-subspaces/</link>
      <pubDate>Wed, 24 Jun 2020 15:27:11 -0700</pubDate>
      <guid>/post/comparing-subspaces/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m working with some multi-dimensional float-valued data &amp;ndash; I&amp;rsquo;ll call a single instance of this data $X \in \mathbb{R}^{n \times k}$.  I have multiple samples $X_{1}, X_{2}&amp;hellip;X_{t}$, and want to compare these subspaces &amp;ndash; namely, I want to compute the distance between pairs of subspaces.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s assume that our subspaces are not rank-deficient &amp;ndash; i.e. for a given subspace sample, all of our dimensions are linearly independent.  Thus, the $k$ vectors form a basis set that spans some $k$-d subspace in $\mathbb{R}^{n}$.  We can think of each $k$-d subspace as a hyperplane in $(k+1)$-d space, just as we can think of a 2-d plane in 3-d space.  One way to compare these subspaces is by using the &amp;ldquo;principle angles between subspaces&amp;rdquo; (or 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Angles_between_flats&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;angles between flats&lt;/a&gt;).  We can compare the &amp;ldquo;angles&amp;rdquo; between these hyperplanes, which will tell us how &amp;ldquo;far apart&amp;rdquo; the two subspaces are.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-intersecting-2d-linear-subspaceshttpswwwresearchgatenetpublication327930102_optimal_exploitation_of_subspace_prior_information_in_matrix_sensing&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/comparing-subspaces/subspaces_hub081a28bf258255fb45bfa4cda66f295_17202_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Intersecting 2D linear 
subspaces.&#34;&gt;


  &lt;img data-src=&#34;/post/comparing-subspaces/subspaces_hub081a28bf258255fb45bfa4cda66f295_17202_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;483&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Intersecting 2D linear 
&lt;a href=&#34;https://www.researchgate.net/publication/327930102_Optimal_Exploitation_of_Subspace_Prior_Information_in_Matrix_Sensing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;subspaces&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This comparison is effectively based on the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/QR_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QR decomposition&lt;/a&gt; and the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Singular Value Decomposition&lt;/a&gt;.  For two subspaces $[U, W]$, we compute the QR decomposition of both:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
U &amp;amp;= Q_{u}R_{u}\\
W &amp;amp;= Q_{w}R_{w}\\
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $Q_{u}$ and $Q_{w} \in \mathbb{R}^{n \times k}$ are orthonormal bases such that $Q_{u}^{T}Q_{u} = Q_{w}^{T}Q_{w} = I_{k}$ that span the same subspace as the original columns of $U$ and $W$, and $R_{u}$ and $R_{w} \in \mathbb{R}^{k \times k}$ are lower triangular matrices.  Next, we compute the matrix $D = \langle Q_{u}, Q_{w} \rangle = Q_{u}^{T} Q_{w} \in \mathbb{R}^{k \times k}$, and then apply the singular value decomposition:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
D = USV^{T}
\end{align}$$&lt;/p&gt;
&lt;p&gt;We can sort of think of $D$ as the cross-covariance matrix.  As such, the singular vectors represent the main orthogonal axes of cross-covariation between our two subspaces, while the singular values represent angles.  In order to compute the principle angles of our subspaces, we simply take&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\theta &amp;amp;= cos^{-1}(S) \\
&amp;amp;=cos^{-1}[\sigma_{1}, \sigma_{2}&amp;hellip;\sigma_{k}]
\end{align}$$&lt;/p&gt;
&lt;p&gt;which gives us the principle angles (in radians).  Because the SVD is invariant to sign (+/-), the principle angles range between $\Big[0, \frac{\pi}{2}\Big]$.  This means that subspaces that span the same space have a principle angle of 0, and subspaces that are orthogonal (maximally far apart) to one another have a principle angle of $\frac{\pi}{2}$.&lt;/p&gt;
&lt;p&gt;In order to compute the &amp;ldquo;distance&amp;rdquo; between our subspaces, we can apply 
&lt;a href=&#34;https://galton.uchicago.edu/~lekheng/work/schubert.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;various metrics&lt;/a&gt; to our vector of principle angles.  The simplest approach is to apply the $L2$ norm to our vector of principle angles, $\theta$, as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
d(X_{i}, X_{j}) = \sqrt{\sum_{n=1}^{k} cos^{-1}(\sigma_{n})^{2}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;This metric is called the 
&lt;a href=&#34;http://www.eeci-institute.eu/GSC2011/Photos-EECI/EECI-GSC-2011-M5/book_AMS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grassmann Distance&lt;/a&gt; and is formally related to the geodesic distance between subspaces distributed on the Grassmannian manifold.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-grassmann-manifold-and-its-tangent-spacehttpsdeepaiorgpublicationautomatic-recognition-of-space-time-constellations-by-learning-on-the-grassmann-manifold-extended-version&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/comparing-subspaces/grassmann_hu2cd3f58cc8fbe46f24a9a6bd61a80bd2_41873_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Grassmann manifold and its 
tangent space.&#34;&gt;


  &lt;img data-src=&#34;/post/comparing-subspaces/grassmann_hu2cd3f58cc8fbe46f24a9a6bd61a80bd2_41873_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;398&#34; height=&#34;249&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Grassmann manifold and its 
&lt;a href=&#34;https://deepai.org/publication/automatic-recognition-of-space-time-constellations-by-learning-on-the-grassmann-manifold-extended-version&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tangent space&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This, however, is a topic for another future blog post.  There are a variety of metrics we can use to compute the pairwise distance between subspaces, some of which are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Asimov: $\; max(\theta)$&lt;/li&gt;
&lt;li&gt;Fubini-Study:  $\; cos^{-1}(\prod sin(\theta))$&lt;/li&gt;
&lt;li&gt;Spectral:  $\; 2 sin(\frac{max(\theta)}{2})$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;but all are fundamentally based on some function of our vector of principle angles, $\theta$.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
