<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>graph convolution networks | A Rambling On</title>
    <link>/tag/graph-convolution-networks/</link>
      <atom:link href="/tag/graph-convolution-networks/index.xml" rel="self" type="application/rss+xml" />
    <description>graph convolution networks</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 25 Dec 2020 23:24:17 -0700</lastBuildDate>
    <image>
      <url>/img/Bayes.jpg</url>
      <title>graph convolution networks</title>
      <link>/tag/graph-convolution-networks/</link>
    </image>
    
    <item>
      <title>Constrained Graph Attention Networks</title>
      <link>/post/constrained-gat/</link>
      <pubDate>Fri, 25 Dec 2020 23:24:17 -0700</pubDate>
      <guid>/post/constrained-gat/</guid>
      <description>&lt;p&gt;In their recent 
&lt;a href=&#34;https://arxiv.org/abs/1910.11945&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click 
&lt;a href=&#34;#Implementation&#34;&gt;here&lt;/a&gt;).  Briefly, GATs are a 
&lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recently-developed&lt;/a&gt; neural network architecture applied to data distributed over a graph domain.  We can think of graph convolutional networks as progressively transforming and aggregating signals from within a local neighborhood of a node.  At each iteration of this process, we implicitly merge signals from larger and larger neighborhoods of the node of interest, and thereby learn unique representations of nodes that are dependent on their surroundings.&lt;/p&gt;
&lt;p&gt;GATs incorporate the seminal idea of &amp;ldquo;attention&amp;rdquo; into this learning process.  In each message-passing step, rather than updating the features of a source-node via equally-weighted contributions of neighborhood nodes, GAT models learn an attention function &amp;ndash; i.e. they learn how to differentially pay attention to various signals in the neighborhood.  In this way, the algorithm can learn to focus on imporant signals and disregard superfluous signals.  If we consider neural networks as universal funtion approximators, the attention mechanism improves the approximating ability by incorporating multiplicative weight factors into the learning.&lt;/p&gt;















&lt;figure id=&#34;figure-figure-from-velickovic-et-alhttpsarxivorgpdf171010903pdf--for-a-source-node-i-and-destination-node-j-vectors-vech_i-and-vech_j-are-the-input-feature-vectors-of-nodes-i-and-j-in-layer-l--mathbfw-is-a-learned-affine-projection-matrix--mathbfveca-is-the-learned-attention-function--the-source-and-destination-node-input-features-are-pushed-through-the-attention-layer-as-alpha_ij--sigmabigvecatmathbfwbigvech_i--vech_jbigbig-where-sigma-is-an-activation-function-and-alpha_ij-the-unnormalized-attention-that-node-i-pays-to-node-j-in-layer-l--attention-weights-are-then-passed-through-a-softmax-layer-mapping-the-attentions-between-01&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./attention_mechanism.png&#34; data-caption=&#34;Figure from 
Velickovic et al.  For a source node $i$ and destination node $j$, vectors $\vec{h_{i}}$ and $\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$.  $\mathbf{W}$ is a learned affine projection matrix.  $\mathbf{\vec{a}}$ is the learned attention function.  The source and destination node input features are pushed through the attention layer as $\alpha_{i,j} = \sigma\Big(\vec{a}^{T}\mathbf{W}\Big(\vec{h_{i}} || \vec{h_{j}}\Big)\Big)$ where $\sigma$ is an activation function, and $\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$.  Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].&#34;&gt;


  &lt;img src=&#34;./attention_mechanism.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure from 
&lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Velickovic et al&lt;/a&gt;.  For a source node $i$ and destination node $j$, vectors $\vec{h_{i}}$ and $\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$.  $\mathbf{W}$ is a learned affine projection matrix.  $\mathbf{\vec{a}}$ is the learned attention function.  The source and destination node input features are pushed through the attention layer as $\alpha_{i,j} = \sigma\Big(\vec{a}^{T}\mathbf{W}\Big(\vec{h_{i}} || \vec{h_{j}}\Big)\Big)$ where $\sigma$ is an activation function, and $\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$.  Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;However, GATs are not without their pitfals, as noted by Wang et al.  Notably, the authors point to two important issues that GATs suffer from: overfitting of attention values and oversmoothing of signals across class boundaries.  The authors propose that GATs overfit the attention function because the learning process is driven only by classification error, with complexity $O(|V|)$ i.e. the number of nodes in the graph.  With regards to oversmoothing, the authors note that a single attention layer can be viewed as a form of Laplacian smoothing:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y = AX^{l}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $A_{n \times n}$ is the attention weight matrix with $A_{i,j} = \alpha_{i,j}$ if $j \in \mathcal{N_{i}}$ and $0$ otherwise.  Because $\sum_{j\in \mathcal{N_{i}}} \alpha_{i,j} = 1$, we can view $A$ as a random walk transition probability matrix.  If we assume that graph $G=(V,E)$ has $K$ connected components, repeated application of $A$ to $X$ distributed over $G$ will result in a stationary distribution of node features within each connected component &amp;ndash; that is, the features vectors of the nodes within each connected component will converge on the component mean.  However, as the authors point out, we typically have multiple layers $l_{1}\dots l_{j}$, each with their own attention matrix $A_{1} \dots A_{j}$, each representing a unique transition probability matrix.  Because we generally do not have disconnected components, nodes from different classes will be connected &amp;ndash; consequentially, deep GAT networks will mix and smooth signals from different adjacent components, resulting in classification performance that worsens with network depth.  Importantly, multi-head attention networks do not alleviate this convergence issue &amp;ndash; each head can be viewed as a unique probability transition matrix, which all suffer from the same oversmoothing issue as $l \rightarrow \infty$.&lt;/p&gt;
&lt;p&gt;Wang et al. propose to incorporate two margin-based constraints into the learning process.  The first constraint, $\mathcal{L_{g}}$, addresses the overfitting issue, by enforcing that learned attentions between adjacent pairs of nodes be higher than attentions between distant pairs of nodes.&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\mathcal{L_{g}} &amp;amp;= \sum_{i \in V} \sum_{j \in \mathcal{N_{i}} \setminus \mathcal{N_{i}^{-}}} \sum_{k \in V\setminus \mathcal{N_{i}}} max(0, \phi(v_{i},v_{k}) + \zeta_{g} - \phi(v_{i},v_{j}))
\end{align}$$&lt;/p&gt;
&lt;p&gt;The second constraint, $\mathcal{L_{b}}$, address the oversmoothing issue, by enforcing that learned attentions between pairs of adjacent nodes with the same label be higher than attention between pairs of adjacent nodes with different labels:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\mathcal{L_{b}} &amp;amp;= \sum_{i \in V}\sum_{j \in \mathcal{N_{i}^{+}}} \sum_{k \in \mathcal{N_{i}^{-}}} max(0, \phi(v_{i},v_{k}) + \zeta_{b} - \phi(v_{i},v_{j}))
\end{align}$$&lt;/p&gt;
&lt;p&gt;In both cases, $\phi(,)$ is the attention function between a pair of nodes, $\mathcal{N_{i}^{+}}$ and $\mathcal{N_{i}^{-}}$ are the nodes adjacent to node $i$ with the same (+) and different (-) labels as $i$, and $\zeta_{g}$ and $\zeta_{b}$ are slack variables controlling the margin between attention values.  The first loss function, $\mathcal{L_{g}}$, can be implemented via negative sampling of nodes (the authors actually perform importance-based negative sampling based on attention-weighted node degrees, but showed that this only marginally improved classification accuracy in benchmark datasets).&lt;/p&gt;
&lt;p&gt;The authors propose one final addition to alleviate the oversmoothing issue posed by vanilla GATs.  Rather than aggregating over all adjacent nodes in a neighborhood, the authors propose to aggregate over the nodes with the $K$ greatest attention values.  Because the class boundary loss $\mathcal{L_{b}}$ enforces large attentions on nodes of the same label and small attention on nodes of different labels, aggregating over the top $K$ nodes will tend to exclude nodes of different labels than the source node in the message passing step, thereby preventing oversmoothing.  The authors show that this constrained aggregation approach is preferable to attention dropout proposed in the original 
&lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAT paper&lt;/a&gt;.  &lt;a name=&#34;Implementation&#34;&gt;
Taken together, the authors deem these margin-based loss and constrained aggregation &amp;ldquo;Constrained Graph Attention Network&amp;rdquo; (C-GAT).&lt;/p&gt;
&lt;/a&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;I wasn&amp;rsquo;t able to find an implementation of the Constrained Graph Attention Network for my own purposes, so I&amp;rsquo;ve implemented the algorithm myself in 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt; (DGL) &amp;ndash; the source code for this convolutional layer can be found 
&lt;a href=&#34;https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/conv/cgatconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.  This implementation makes use of the original DGL &lt;code&gt;GATConv&lt;/code&gt; layer structure, with modifications made for the constraints and aggregations.  Specifically, the API for &lt;code&gt;CGATConv&lt;/code&gt; has the following modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
CGATCONV(in_feats, 
         out_feats, 
         num_heads, 
         feat_drop=0., 
         graph_margin=0.1, # graph structure loss slack variable
         class_margin=0.1, # class boundary loss slack variable
         top_k=3, # number of messages to aggregate over
         negative_slope=0.2,
         residual=False,
         activation=None,
         allow_zero_in_degree=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of note is the fact that the &lt;code&gt;attn_drop&lt;/code&gt; parameter has been substituted by the &lt;code&gt;top_k&lt;/code&gt; parameter in order to mitigate oversmoothing, and the two slack variables $\zeta_{g}$ and $\zeta_{b}$ are provided as &lt;code&gt;graph_margin&lt;/code&gt; and &lt;code&gt;class_margin&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With regards to the loss functions, the authors compute all-pairs differences between all edges incident on a source node, instead of summing over the positive / negative sample attentions ($\mathcal{L_{g}}$) and same / different label attentions ($\mathcal{L_{b}}$) and then differencing these summations.  In this way, the C-GAT model anchors the loss values to specific nodes, promoting learning of more generalizable attention weights.  The graph structure loss function $\mathcal{L_{g}}$ is implemented with the &lt;code&gt;graph_loss&lt;/code&gt; reduction function below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def graph_loss(nodes):
            
    &amp;quot;&amp;quot;&amp;quot;
    Loss function on graph structure.
    
    Enforces high attention to adjacent nodes and 
    lower attention to distant nodes via negative sampling.
    &amp;quot;&amp;quot;&amp;quot;

    msg = nodes.mailbox[&#39;m&#39;]

    pw = msg[:, :, :, 0, :].unsqueeze(1)
    nw = msg[:, :, :, 1, :].unsqueeze(2)

    loss = (nw + self._graph_margin - pw).clamp(0)
    loss = loss.sum(1).sum(1).squeeze()

    return {&#39;graph_loss&#39;: loss}
.
.
.
graph.srcdata.update({&#39;ft&#39;: feat_src, &#39;el&#39;: el})
graph.dstdata.update({&#39;er&#39;: er})
graph.apply_edges(fn.u_add_v(&#39;el&#39;, &#39;er&#39;, &#39;e&#39;))
e = self.leaky_relu(graph.edata.pop(&#39;e&#39;))

# construct the negative graph by shuffling edges
# does not assume a single graph or blocked graphs
# see cgatconv.py for ```construct_negative_graph``` function
neg_graph = [construct_negative_graph(i, k=1) for i in dgl.unbatch(graph)]
neg_graph = dgl.batch(neg_graph)

neg_graph.srcdata.update({&#39;ft&#39;: feat_src, &#39;el&#39;: el})
neg_graph.dstdata.update({&#39;er&#39;: er})
neg_graph.apply_edges(fn.u_add_v(&#39;el&#39;, &#39;er&#39;, &#39;e&#39;))
ne = self.leaky_relu(neg_graph.edata.pop(&#39;e&#39;))

combined = th.stack([e, ne]).transpose(0, 1).transpose(1, 2)
graph.edata[&#39;combined&#39;] = combined
graph.update_all(fn.copy_e(&#39;combined&#39;, &#39;m&#39;), graph_loss)

# compute graph structured loss
Lg = graph.ndata[&#39;graph_loss&#39;].sum() / (graph.num_nodes() * self._num_heads)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, the class boundary loss function $\mathcal{L_{b}}$ is implemented with the following message and reduce functions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def adjacency_message(edges):
            
    &amp;quot;&amp;quot;&amp;quot;
    Compute binary message on edges.

    Compares whether source and destination nodes
    have the same or different labels.
    &amp;quot;&amp;quot;&amp;quot;

    l_src = edges.src[&#39;l&#39;]
    l_dst = edges.dst[&#39;l&#39;]

    if l_src.ndim &amp;gt; 1:
        adj = th.all(l_src == l_dst, dim=1)
    else:
        adj = (l_src == l_dst)

    return {&#39;adj&#39;: adj.detach()}

def class_loss(nodes):
    
    &amp;quot;&amp;quot;&amp;quot;
    Loss function on class boundaries.
    
    Enforces high attention to adjacent nodes with the same label
    and lower attention to adjacent nodes with different labels.
    &amp;quot;&amp;quot;&amp;quot;

    m = nodes.mailbox[&#39;m&#39;]

    w = m[:, :, :-1]
    adj = m[:, :, -1].unsqueeze(-1).bool()

    same_class = w.masked_fill(adj == 0, np.nan).unsqueeze(2)
    diff_class = w.masked_fill(adj == 1, np.nan).unsqueeze(1)

    difference = (diff_class + self._class_margin - same_class).clamp(0)
    loss = th.nansum(th.nansum(difference, 1), 1)

    return {&#39;boundary_loss&#39;: loss}
.
.
.
graph.ndata[&#39;l&#39;] = label
graph.apply_edges(adjacency_message)
adj = graph.edata.pop(&#39;adj&#39;).float()

combined = th.cat([e.squeeze(), adj.unsqueeze(-1)], dim=1)
graph.edata[&#39;combined&#39;] = combined
graph.update_all(fn.copy_e(&#39;combined&#39;, &#39;m&#39;), class_loss)
Lb = graph.ndata[&#39;boundary_loss&#39;].sum() / (graph.num_nodes() * self._num_heads)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, the constrained message aggregation is implemented using the following reduction function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def topk_reduce_func(nodes):
    
    `&amp;quot;&amp;quot;&amp;quot;
    Aggregate attention-weighted messages over the top-K 
    attention-valued destination nodes
    &amp;quot;&amp;quot;&amp;quot;

    K = self._top_k

    m = nodes.mailbox[&#39;m&#39;]
    [m,_] = th.sort(m, dim=1, descending=True)
    m = m[:,:K,:,:].sum(1)

    return {&#39;ft&#39;: m}
.
.
.
# message passing
if self._top_k is not None:
    graph.update_all(fn.u_mul_e(&#39;ft&#39;, &#39;a&#39;, &#39;m&#39;), 
                    topk_reduce_func)
else:
    graph.update_all(fn.u_mul_e(&#39;ft&#39;, &#39;a&#39;, &#39;m&#39;),
                    fn.sum(&#39;m&#39;, &#39;ft&#39;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Graph Convolutional Networks</title>
      <link>/post/gaussian-kernel-convolution/</link>
      <pubDate>Mon, 07 Dec 2020 23:24:17 -0700</pubDate>
      <guid>/post/gaussian-kernel-convolution/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m using 
&lt;a href=&#34;https://tkipf.github.io/graph-convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;graph convolutional networks&lt;/a&gt; as a tool to segment the cortical surface of the brain.  This research resides in the domain of &lt;em&gt;node classification&lt;/em&gt; using &lt;em&gt;inductive learning&lt;/em&gt;.  By node classification, I mean that we wish to assign a discrete label to cortical surface locations (nodes / vertices in a graph) on the basis of some feature data and brain network topology.  By inductive learning, I mean that we will train, validate, and test on datasets with possibly different graph topologies &amp;ndash; this is in contrast to &lt;em&gt;transductive learning&lt;/em&gt; that learns models that do not generalize to arbitrary network topology.&lt;/p&gt;
&lt;p&gt;In conventional convolutions over regular grid domains, such as images, using approaches like
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convolutional_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ConvNet&lt;/a&gt;, we learn the parameters of a sliding filter that convolves the signal around a pixel of interest $p_{i,j}$, such that we aggregate the information from pixels $p_{\Delta i, \Delta j}$ for some fixed distance $\Delta$ away from $p$. Oftentimes, however, we encounter data that is distributed over a graphical domain, such as social networks, journal citations, brain connectivity, or the electrical power grid.  In such cases, concepts like &amp;ldquo;up&amp;rdquo;, &amp;ldquo;down&amp;rdquo;, &amp;ldquo;left&amp;rdquo;, and &amp;ldquo;right&amp;rdquo; do not make sense &amp;ndash; what does it mean to be &amp;ldquo;up&amp;rdquo; from something in a network? &amp;ndash; so we need some other notion of neighborhood.&lt;/p&gt;
&lt;p&gt;In come graph convolutional networks (GCNs).  GCNs generalize the idea of neighborhood aggregation to the graph domain by utilizing the adjacency structure of a network &amp;ndash; we can now aggregate signals near a node by using some neighborhood around it.  While vanilla GCNs learn rotationally-invariant filters, recent developments in the world of 
&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer networks&lt;/a&gt; have opened up the door for much more flexible and inductive models (see: 
&lt;a href=&#34;https://arxiv.org/abs/1710.10903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Attention Networks&lt;/a&gt;, 
&lt;a href=&#34;https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GraphSAGE&lt;/a&gt;).&lt;/p&gt;















&lt;figure id=&#34;figure-demonstration-of-graph-convolution-network-from-thomas-kipfhttpstkipfgithubiograph-convolutional-networks&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png&#34; data-caption=&#34;Demonstration of graph convolution network from 
Thomas Kipf.&#34;&gt;


  &lt;img src=&#34;https://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Demonstration of graph convolution network from 
&lt;a href=&#34;https://tkipf.github.io/graph-convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas Kipf&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;I was specifically interested in applying the methodology described 
&lt;a href=&#34;http://arxiv.org/abs/1803.10336&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, where the authors utilitize Gaussian kernels as filters over the neighborhood of nodes.  However, the authors did not open-source their code &amp;ndash; as such, I needed to implement this method myself.  Assume our input data to layer $l$ is $Y^{(l)} \in \mathbb{R}^{N \times q}$ for $N$ nodes in the graph.  We can define the Gaussian kernel-weighted convolution as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
z_{i,p}^{(l)} = \sum_{j \in \mathcal{N}&lt;em&gt;{i}} \sum&lt;/em&gt;{q=1}^{M_{(l)}} \sum_{k=1}^{K_{(l)}} w_{p,q,k}^{(l)} \cdot y_{j,q}^{(l)} \cdot \phi(\hat{\mu}&lt;em&gt;{i}, \hat{\mu}&lt;/em&gt;{j}; \Theta_{k}^{(l)}) + b_{p}^{(l)}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Above, $y_{j,q}^{(l)}$ is the $q$-th input feature of neighboring node $j$, $w_{p,q,k}^{(l)}$ is the linear weight assigned to this feature for the $k$-th kernel, and $\phi(\hat{\mu}&lt;em&gt;{i}, \hat{\mu}&lt;/em&gt;{j}; \Theta_{k}^{(l)})$ is the $k$-th kernel weight between node $i$ and node $j$, defined as:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\phi(\hat{\mu_{i}}, \hat{\mu_{j}}; \sigma_{k}^{(l)}, \mu_{k}^{(l)}  ) = \exp^{-\sigma_{k}^{(l)} \left\Vert (\hat{\mu_{i}} - \hat{\mu_{j}}) - \mu_{k}^{(l)} \right\Vert^{2}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Extrinsically, the kernel weights are represented by edges in a sparse affinity matrix, such that index $(i,j)$ is the Gaussian kernel weight between node $i$ and node $j$ for the $k$-th kernel in the $l$-th layer, where nodes $j$ are restricted to be within a certain neighborhood or distance of node $i$.  This can be seen more clearly here:&lt;/p&gt;















&lt;figure id=&#34;figure-figure-from-wu-et-alhttpswwwncbinlmnihgovpmcarticlespmc7052684--v_i-is-our-voxel-of-interest-and-v_ki-for-demonstration-purposes-is-an-adjacent-node--both-v_i-and-v_ki-are-characterized-by-embedding-vectors-e_i-e_ki-in-mathbbrq-from-which-we-compute-the-kernel-weight-phi_ik-characterizing-how-similar-the-two-vertices-embedding-vectors-are&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./gaussian_radius.png&#34; data-caption=&#34;Figure from 
Wu et al..  $v_{i}$ is our voxel of interest, and $v_{k}^{i}$, for demonstration purposes, is an adjacent node.  Both $v_{i}$ and $v_{k}^{i}$ are characterized by embedding vectors $e_{i}, e_{k}^{i} \in \mathbb{R}^{q}$, from which we compute the kernel weight $\phi_{i,k}$ characterizing how similar the two vertices&amp;rsquo; embedding vectors are.&#34;&gt;


  &lt;img src=&#34;./gaussian_radius.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure from 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7052684/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wu et al.&lt;/a&gt;.  $v_{i}$ is our voxel of interest, and $v_{k}^{i}$, for demonstration purposes, is an adjacent node.  Both $v_{i}$ and $v_{k}^{i}$ are characterized by embedding vectors $e_{i}, e_{k}^{i} \in \mathbb{R}^{q}$, from which we compute the kernel weight $\phi_{i,k}$ characterizing how similar the two vertices&amp;rsquo; embedding vectors are.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;I implemented a new convolutional layer called &lt;code&gt;GAUSConv&lt;/code&gt; (available 
&lt;a href=&#34;https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/layers/gausconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).  To implement this algorithm, I utilized the 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt; (DGL), which offers a stellar single unifed API based on message passing (I&amp;rsquo;m using 
&lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch&lt;/a&gt; as the backend).  I noticed that I could formulate this problem using attention mechanisms described in the 
&lt;a href=&#34;https://arxiv.org/abs/1710.10903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Attention Network&lt;/a&gt; paper &amp;ndash; however, instead of computing attention weights using a fully connected layer as described in that work, I would compute kernel weights using Gaussian filters.  Similarly, just as the GAT paper describes &lt;em&gt;multi-head attention&lt;/em&gt; for multiple attention channels, I could analogize my fomulation to &lt;em&gt;multi-head kernels&lt;/em&gt; for multiple kernel channels.  To this end, I could make use of the 
&lt;a href=&#34;https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/gatconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;GATConv&lt;/code&gt;&lt;/a&gt; API quite easily by replacing the attention computations with the Gaussian kernel filtrations.  Likewise, I utilized the 
&lt;a href=&#34;https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/graphconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;GraphConv&lt;/code&gt;&lt;/a&gt; API to incorporate linear weights from the 
&lt;a href=&#34;https://arxiv.org/pdf/1609.02907.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Convolution Network&lt;/a&gt; paper.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;GAUSConv&lt;/code&gt; layer is similar to both the &lt;code&gt;GraphConv&lt;/code&gt; and &lt;code&gt;GATConv&lt;/code&gt; layers but differs in a few places.  Rather than initializing the layer with attention heads, we initialize it with the number of kernels and a kernel dropout probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
GAUSConv(in_feats, # number of input dimensions
         out_feats, # number of output features
         num_kernels, # number of kernels for current layer
         feat_drop=0., # dropout probability of features
         kernel_drop=0., # dropout probability of kernels
         negative_slope=0.2, # leakly relu slope
         activation=None, # activation function to apply after forward pass
         random_seed=None, # for example / reproducibility purposes
         allow_zero_in_degree=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Importantly, in the layer instantiation, we define &lt;strong&gt;linear weights&lt;/strong&gt; &lt;em&gt;and&lt;/em&gt; &lt;strong&gt;kernel mean and sigma parameters&lt;/strong&gt;, &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;.  We initialize both kernel parameters with the flag &lt;code&gt;require_grad=True&lt;/code&gt;, which enables us to update these kernel parameters during the backward pass of the layer.  Both parameters are initialized with values in the &lt;code&gt;reset_parameters&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# initialize feature weights and bias vector
self.weights = nn.Parameter(
    th.Tensor(num_kernels, in_feats, out_feats), requires_grad=True)
self.bias = nn.Parameter(
    th.Tensor(num_kernels, out_feats), requires_grad=True)

# initialize kernel perameters
self.mu = nn.Parameter(
    th.Tensor(1, num_kernels, in_feats), requires_grad=True)
self.sigma = nn.Parameter(
    th.Tensor(num_kernels, 1), requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now here is the clever part, and where the 
&lt;a href=&#34;https://docs.dgl.ai/en/0.4.x/api/python/function.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DGL message passing interface&lt;/a&gt; really shines through.  DGL fuses the &lt;code&gt;send&lt;/code&gt; and &lt;code&gt;receive&lt;/code&gt; messages so that no messages between nodes are ever explicitly stored, using built-in &lt;strong&gt;message&lt;/strong&gt; and &lt;strong&gt;reduce&lt;/strong&gt; functions.  To compute the kernel weights between all pairs of source and destrination nodes, we use these built-in functions.  The important steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;compute node feature differences between all source / destination node pairs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;aggregate and reduce incoming messages from destination nodes scaled by the kernel weights, to update the source node features&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the forward pass of our layer, we perform the following steps:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;### forward pass of GAUSConv layer ###

# compute all pairwise differences between adjacent node features
graph.ndata[&#39;h&#39;] = feat
graph.apply_edges(fn.u_sub_v(&#39;h&#39;, &#39;h&#39;, &#39;diff&#39;))

# compute kernel weights for each source / desintation pair
e = graph.edata[&#39;diff&#39;].unsqueeze(1) - mu
e = -1*sigma*th.norm(e, dim=2).unsqueeze(2)
e = e.exp()
graph.edata[&#39;e&#39;] = e

# apply kernel weights to destination node features
graph.apply_edges(fn.v_mul_e(&#39;h&#39;, &#39;e&#39;, &#39;kw&#39;))

# apply linear projection to kernel-weighted destination node features
a = th.sum(th.matmul(graph.edata[&#39;kw&#39;].transpose(1, 0), weights), dim=0)

# apply kernel dropout
a = self.kernel_drop(a)
graph.edata[&#39;a&#39;] = a

# final message-passing and reduction step
# aggregate weighted destination node features to update source node features
graph.update_all(fn.copy_e(&#39;a&#39;, &#39;m&#39;), fn.sum(&#39;m&#39;, &#39;h&#39;))
rst = graph.ndata[&#39;h&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, given a graph and features, we instantiate a &lt;code&gt;GAUSConv&lt;/code&gt; layer and propogate our features through the network via:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# set random seed
random_seed=1

# define arbitrary input/output feature shape
n_samples = 4
in_feats=4
out_feats=2
features = th.ones(n_samples, in_feats)

# define number of kernels
num_kernels=2

# create graph structure
u, v = th.tensor([0, 0, 0, 1]), th.tensor([1, 2, 3, 3])
g = dgl.graph((u, v))
g = dgl.to_bidirected(g)
g = dgl.add_self_loop(g)

# instantiate layer
GausConv = GAUSConv(in_feats=in_feats,
                    out_feats=out_feats,
                    random_seed=random_seed,
                    num_kernels=num_kernels,
                    feat_drop=0,
                    kernel_drop=0)
                    
# forward pass of layer
logits = GausConv(g, features)

print(logits)
tensor([[0.1873, 0.7217],
        [0.1405, 0.5413],
        [0.0936, 0.3608],
        [0.1405, 0.5413]], grad_fn=&amp;lt;AddBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
