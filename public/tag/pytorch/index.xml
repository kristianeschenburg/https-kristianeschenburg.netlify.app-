<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pytorch | A Rambling On</title>
    <link>/tag/pytorch/</link>
      <atom:link href="/tag/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <description>pytorch</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 14 Feb 2021 23:24:17 -0700</lastBuildDate>
    <image>
      <url>/img/Bayes.jpg</url>
      <title>pytorch</title>
      <link>/tag/pytorch/</link>
    </image>
    
    <item>
      <title>Jumping-Knowledge Representation Learning With LSTMs</title>
      <link>/post/jumping-knowledge/</link>
      <pubDate>Sun, 14 Feb 2021 23:24:17 -0700</pubDate>
      <guid>/post/jumping-knowledge/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;As I mentioned in my previous post on 
&lt;a href=&#34;/post/constrained-gat/&#34;&gt;constrained graph attention networks&lt;/a&gt;, graph neural networks suffer from overfitting and oversmoothing as network depth increases.  These issues can ultimately be linked to the local topologies of the graph.&lt;/p&gt;
&lt;p&gt;If we consider a 2d image as a graph (i.e. pixels become nodes), we see that images are highly 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Regular_graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;regular&lt;/a&gt; &amp;ndash; that is, each node has the same number of neighbors, except for those at the image periphery.  When we apply convolution kernels over node signals, filters at any given layer are aggregating information from the same sized neighborhods irrespective of their location.&lt;/p&gt;
&lt;p&gt;However, if we consider a graph, there is no guarantee that the graph will be regular.  In fact, in many situations, graphs are highly &lt;em&gt;irregular&lt;/em&gt;, and are characterized by unique topological neighborhood properties such as tree-like structures or 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Expander_graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expander graphs&lt;/a&gt;, that are sparse yet highly connected.  If we compare an expander node to a node whose local topology is more regular, we would find that the number of signals that each node implicitly convolves at each network layer would vary considerably.  These topological discrepancies have important implications when we consider problems like node and graph classification, as well as edge prediction.  The problem ultimately boils down to one of flexibility: can we account for unique local topologies of a graph in order to dynamically aggregate local information on a node-by-node basis?&lt;/p&gt;















&lt;figure id=&#34;figure-node-signal-aggregation-as-a-function-of-network-depth--at-each-layer-the-neural-network-implicitly-aggregates-signals-over-an-increasingly-larger-neighborhood--in-this-example-the-network-is-highly-regular----however-not-all-graphs-are&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./InfluenceRadius.png&#34; data-caption=&#34;Node signal aggregation as a function of network depth.  At each layer, the neural network implicitly aggregates signals over an increasingly-larger neighborhood.  In this example, the network is highly regular &amp;ndash; however, not all graphs are.&#34;&gt;


  &lt;img src=&#34;./InfluenceRadius.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Node signal aggregation as a function of network depth.  At each layer, the neural network implicitly aggregates signals over an increasingly-larger neighborhood.  In this example, the network is highly regular &amp;ndash; however, not all graphs are.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In a recent paper, the authors propose one approach to address this question, which they call &amp;ldquo;jumping knowledge representation learning&amp;rdquo;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  Instead of utilizing the output of the last convolution layer to inform the prediction, jumping-knowledge networks aggregate the embeddings from all hidden layers to inform the final prediction.  The authors develop an approach to study the &amp;ldquo;influence distribution&amp;rdquo; of nodes: for a given node $x$, the influence distribution $I_{x}$ characterizes how much the final embedding of node $x$ is influenced by the input features of every other node:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
I(x,y) &amp;amp;= \sum_{i=1}^{m} |\frac{\delta h_{x}^{k}}{\delta h_{y}^{0}}|_{i} \\&lt;br&gt;
I_{x}(y) &amp;amp;= I(x,y) \Big/\sum_{z} I(x,z)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;They show that influence distribution $I_{x}$ for a $k$-layer graph convolution network is equal, in expectation, to the $k$-step random walk distribution.  They point out that the random walk distribution of expander-like nodes converge quickly &amp;ndash; the final embeddings of these nodes are represenative of the whole graph and carry global information &amp;ndash; while the random-walk distribution of nodes with tree-like topology converge slowly &amp;ndash; these nodes carry more-local information&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.  These two conclusions are related to the spectral gap of the graph &amp;ndash; the smallest non-zero eigenvalue of the graph Laplacian.  A large spectral gap indicates high-connectivity, while a low spectral gap indicates low connectivity.  From a graph theory perspective, this local connectivity is related to the idea of centrality.  Nodes with high centrality will easily saturate their random walk distribution, but will also aggregate information from large neighborhoods quickly.  For graph neural networks with fixed aggregation kernels, this has important implications for representation learning, because the feature distributions of nodes with different topologies will not correspond to the same degree of locality, which may not lead to the best learned representations for all nodes.  A radius that is too large may result in over-smoothing of node features, while a radius that is too small may not be robust enough to learn optimal node embeddings.&lt;/p&gt;
&lt;p&gt;The jumping knowledge network architecture is conceptually similar to other graph neural networks, and we can, in fact, simply incorporate the jumping knowledge mechanism as an additional layer.  The goal is to adaptively learn the effective neighborhood size on a node-by-node basis, rather than enforcing the same aggregation radius for every node (remember, we want to account for local topological and feature variations).  The authors suggest three possible aggregation functions: concatentation, max-pooling, and an LSTM-attention mechanism &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.  Each aggregator learns an optimal combination of the hidden embeddings, which is then pushed through a linear layer to generate the final network output.  Concatenation determines the optimal linear combination of hidden embeddings for the entire dataset simultaneously, so it is not a node-specific aggregator.  Max-pooling selects the most important hidden layer for each feature element on a node-by-node basis &amp;ndash; however, empirically, I found that max-pooling was highly unstable in terms of model learning.  The LSTM-attention aggregator treats the hidden embeddings as a sequence of elements, and assigns a unique attention score to each hidden embedding &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;















&lt;figure id=&#34;figure-schematic-of-a-jumping-knowledge-network--the-neural-network-generates-an-embedding-for-each-hidden-layer--the-aggregator-function-then-optimally-combines-these-hidden-embeddinggs-to-learn-the-optimal-abstraction-of-input-information--some-alternative-aggregation-functions-include-max-pooling-concatenation-and-an-lstm-layer--in-the-case-of-an-lstm-layer-coupled-with-an-attention-mechanism-the-aggregator-computes-a-convex-combination-of-hidden-embeddings&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./JKGAT_LSTM.png&#34; data-caption=&#34;Schematic of a jumping-knowledge network.  The neural network generates an embedding for each hidden layer.  The aggregator function then optimally combines these hidden embeddinggs to learn the optimal abstraction of input information.  Some alternative aggregation functions include max-pooling, concatenation, and an LSTM layer.  In the case of an LSTM layer coupled with an attention mechanism, the aggregator computes a convex combination of hidden embeddings.&#34;&gt;


  &lt;img src=&#34;./JKGAT_LSTM.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Schematic of a jumping-knowledge network.  The neural network generates an embedding for each hidden layer.  The aggregator function then optimally combines these hidden embeddinggs to learn the optimal abstraction of input information.  Some alternative aggregation functions include max-pooling, concatenation, and an LSTM layer.  In the case of an LSTM layer coupled with an attention mechanism, the aggregator computes a convex combination of hidden embeddings.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;long-short-term-memory&#34;&gt;Long-Short Term Memory&lt;/h4&gt;
&lt;p&gt;Briefly, given a sequence of samples $X_{1}, X_{2}, \dots X_{t}$, the LSTM cell learns temporal dependencies between elements of a sequence by maintaining a memory of previously observed elements &amp;ndash; in our case, the sequence elements are the embeddings learned by each consecutive hidden layer.  An LSTM cell is characterized by three gates controlling information flow between elements in the sequence: input, forget, and output, as well as a cell state vector, which captures the memory and temporal dependencies between sequence elements&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;















&lt;figure id=&#34;figure-schematic-of-an-lstm-cell--the-cell-controls-what-information-is-remembered-from-previous-elements-in-a-sequence-and-what-information-is-incorporated-into-memory-given-a-new-element-in-the-sequence&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./LSTM_cell.png&#34; data-caption=&#34;Schematic of an LSTM cell.  The cell controls what information is remembered from previous elements in a sequence, and what information is incorporated into memory given a new element in the sequence.&#34;&gt;


  &lt;img src=&#34;./LSTM_cell.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Schematic of an LSTM cell.  The cell controls what information is remembered from previous elements in a sequence, and what information is incorporated into memory given a new element in the sequence.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;$$
\begin{align}
f_{t} &amp;amp;= \sigma(W_{f}X_{t} + U_{f}h_{t-1} + b_{f}) \\&lt;br&gt;
i_{t} &amp;amp;= \sigma(W_{i}X_{t} + U_{i}h_{t-1} + b_{i}) \\&lt;br&gt;
o_{t} &amp;amp;= \sigma(W_{o}X_{t} + U_{o}h_{t-1} + b_{o}) \\&lt;br&gt;
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where $W$, $U$, and $b$ are learnable parameters of the gates.  Here, $X_{t}$ is the $t$-th sequence element, $h_{t-1}$ represents the learned LSTM cell embedding for element $t-1$, and $C_{t-1}$ represents the current memory state, given the previous $1, 2 \dots t-1$ elements.  The input and forget gates determine which aspects of a sequence element are informative / uninformative, and decide what information to keep / forget, while the output gate combines the previous memory state with our new knowledge.  We can roughly think of this process as updating our prior beliefs, in the Bayesian sense, with new incoming data.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\tilde{c_{t}} &amp;amp;= \sigma(W_{c}X_{t} + U_{c}h_{t-1} + b_{c}) \\&lt;br&gt;
c_{t} &amp;amp;= f_{t}\circ c_{t-1} + i_{t} \tilde{c}_{t} \\&lt;br&gt;
h_{t} &amp;amp;= o_{t} \circ tanh(c_{t})
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The embeddings for each element learned by the LSTM cell are represented by $h_{t}$.  In the original paper&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, the authors propose to apply a bi-directional LSTM to simultaneously learn forwards and backwards embeddings, which are concatenated and pushed through a single-layer perceptron to compute layer-specific attention weights for each node:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\alpha_{i}^{t} &amp;amp;= \sigma(\vec{w}_{t}^{T}(h^{F}_{i, t} || h^{B}_{i, t})) \\&lt;br&gt;
\alpha_{i}^{t} &amp;amp;= \frac{\exp{(\alpha_{i}^{t})}}{\sum_{t=1}^{L} \exp{(\alpha_{i}^{t})}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The softmax-normalized attention weights represent a probability distribution over attention weights&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\sum_{t=1}^{L} \alpha_{i}^{t} &amp;amp;= 1 \\&lt;br&gt;
\\&lt;br&gt;
\alpha_{i}^{t} &amp;amp;&amp;gt;= 0
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where $\alpha_{i}^{t}$ represents how much node $i$ attends to the embedding of hidden layer $t$.  The optimal embedding is then computed as the attention-weighted convex combination of hidden embeddings:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
X_{i, \mu} = \sum_{t=1}^{L} \alpha_{i}^{t}X_{i, t}
\end{align}
$$&lt;/p&gt;
&lt;h2 id=&#34;an-application-of-jumping-knowledge-networks-to-cortical-segmentation&#34;&gt;An Application of Jumping Knowledge Networks to Cortical Segmentation&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ve implemented the jumping knowledge network using DGL 
&lt;a href=&#34;https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/jkgat/jkgat.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.  Below, I&amp;rsquo;ll demonstrate the application of jumping knowledge representation learning to a cortical segmentation task.  Neuroscientifically, we have reason to believe that the influence radius will vary along the cortical manifold, even if the mesh structure is highly regular.  As such, I am specifically interested in examining the importance that each node assigns to the embeddings of each hidden layer.  To that end, I utilize the LSTM-attention aggregator.  Similarly, as the jumping-knowledge mechanism can be incorporated as an additional layer to any general graph neural network, I will use graph attention networks (GAT) as the base network architecture, and compare vanilla GAT performance to GATs with a jumping knowledge mechanism (JKGAT).&lt;/p&gt;
&lt;p&gt;Below, I show the prediction generated by a 9-layer JKGAT model, with 4 attention heads and 32 hidden channels per layer, with respect to the &amp;ldquo;known&amp;rdquo; or &amp;ldquo;true&amp;rdquo; cortical map.  We find slight differences in the performance of our JKGAT model with respect to the ground truth map, notably in the lateral occipital cortex and the medial prefrontal cortex.&lt;/p&gt;















&lt;figure id=&#34;figure-comparison-of-the-group-average-predicted-cortical-segmentation-produced-by-the-jkgat-model-to-the-ground-truth-cortical-segmentation--the-ground-truth-was-previously-generated-herehttpswwwncbinlmnihgovpmcarticlespmc4990127--the-consensus-cortical-map-corresponds-very-well-to-the-true-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./prediction_G.png&#34; data-caption=&#34;Comparison of the group-average predicted cortical segmentation produced by the JKGAT model, to the ground truth cortical segmentation.  The ground truth was previously generated here.  The consensus cortical map corresponds very well to the true map.&#34;&gt;


  &lt;img src=&#34;./prediction_G.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Comparison of the group-average predicted cortical segmentation produced by the JKGAT model, to the ground truth cortical segmentation.  The ground truth was previously generated &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4990127/&#34;&gt;here&lt;/a&gt;.  The consensus cortical map corresponds very well to the true map.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;When we consider the accuracies for various parameterizations of our models, we see that the JKGAT performs quite well.  Notably, it performs better than the GAT model in most cases.  Likewise, as hypothesized, the JKGAT performs better than the GAT model as network depth increases, specifically because we are able to dynamically learn the optimal influence radii for each node, rather than constraining the same radius size for the entire graph.  This allows us to learn more abstract representations of the input features by mitigating oversmoothing and by accounting for node topological variability, which is important for additional use-cases like graph classification.&lt;/p&gt;















&lt;figure id=&#34;figure-model-accuracy-comparison-between-gat-and-jkgat-models-on-a-node-classification-problem-for-cortical-segmentation--accuracy-is-represented-as-the-fraction-of-correctly-labeled-nodes-in-a-graph-across-150-validation-subjects--each-node-in-the-graph-has-80-features-and-each-graph-has-30k-nodes&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./validation.accuracy.png&#34; data-caption=&#34;Model accuracy comparison between GAT and JKGAT models on a node classification problem for cortical segmentation.  Accuracy is represented as the fraction of correctly-labeled nodes in a graph, across 150 validation subjects.  Each node in the graph has 80 features, and each graph has 30K nodes.&#34;&gt;


  &lt;img src=&#34;./validation.accuracy.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model accuracy comparison between GAT and JKGAT models on a node classification problem for cortical segmentation.  Accuracy is represented as the fraction of correctly-labeled nodes in a graph, across 150 validation subjects.  Each node in the graph has 80 features, and each graph has 30K nodes.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Similarly, we find that JKGAT networks generate segmentation predictions that are more reproducible and consistent across resampled datasets.  This is important, especially in the case where we might acquire data on an individual multiple times, and want to generate a cortical map for each acquisition instance.  Unless an individual suffers from an accelerating neurological disorder, experiences a traumatic neurological injury, or the time between consecutive scans is very long (on the order of years), we expect the cortical map of any given individual to remain quite static (though examining how the &amp;ldquo;map&amp;rdquo; of an individual changes over time is still an open-ended topic).&lt;/p&gt;















&lt;figure id=&#34;figure-model-reproducibility-comparison-between-gat-and-jkgat-models-on-a-node-classification-problem-for-cortical-segmentation-using-150-validation-subjects--each-subject-has-four-repeated-datasets--within-a-given-subject-the-topology-of-each-graph-is-the-same-but-the-node-features-are-re-sampled-for-each-graph--reproducibility-is-computed-using-the-f1-score-between-all-pairs-of-predicted-node-classifications-such-that-we-compute-6-f1-scores-for-each-subject&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./validation.reproducibility.png&#34; data-caption=&#34;Model reproducibility comparison between GAT and JKGAT models on a node classification problem for cortical segmentation, using 150 validation subjects.  Each subject has four repeated datasets.  Within a given subject, the topology of each graph is the same, but the node features are re-sampled for each graph.  Reproducibility is computed using the F1-score between all pairs of predicted node classifications, such that we compute 6 F1 scores for each subject.&#34;&gt;


  &lt;img src=&#34;./validation.reproducibility.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model reproducibility comparison between GAT and JKGAT models on a node classification problem for cortical segmentation, using 150 validation subjects.  Each subject has four repeated datasets.  Within a given subject, the topology of each graph is the same, but the node features are re-sampled for each graph.  Reproducibility is computed using the F1-score between all pairs of predicted node classifications, such that we compute 6 F1 scores for each subject.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Finally, when we consider the importance that each cortical node assigns to the unique embedding at the $k$-th layer via the LSTM-attention aggregation function, we see very interesting results.  Notably, we see high spatial auto-correlation of the attention weights.  Even more striking, is that this spatial correlation seems to correspond to well-studied patterns of resting-state networks identified using functional MRI. Apart from the adjacency structure of our graphs, we do not encode any &lt;em&gt;a priori&lt;/em&gt; information of brain connectivity.  That the LSTM-attention aggregator of the jumping-knowledge layer idenfities maps corresponding reasonably well to known functional networks of the human brain is indicative, to some extent, of how the model is learning, and more importantly, of which features are useful in distinguishing cortical areas from one another.&lt;/p&gt;
&lt;p&gt;Let us consider the attention map for layer 4.  We can interpret the maps as follows: for a given network architecture (in this case, a network with 9 layers), we find that areas in the primary motor (i.e. Brodmann areas 3a and banks of area 4) and primary auditory cortex (Broddmann areas A1 and R1) preferentially attend to the embedding of hidden layer 4, relative to the rest of the cortex &amp;ndash; this indicates that the implicit aggregation over an influence radius of 4 layers is deemed more informative for the classification of nodes in the primary motor and auditory regions than for orther cortical areas.  However, whether this says anything about the implicit complexitiy of the cortical signals of these areas remains to be studied.&lt;/p&gt;















&lt;figure id=&#34;figure-maps-of-learned-lstm-attention-aggregator-weights--each-inset-corresponds-to-the-weights-learned-by-every-cortical-node-for-the-k-th-layer-hidden-embedding-black-low-red-high--we-see-that-most-of-the-attention-mass-is-distributed-over-layers-4-7-indicating-that-most-nodes-assign-maximal-importance-to-intermediate-levels-of-abstraction--however-we-do-see-spatially-varying-attention--notably-within-a-given-attention-map-we-find-that-nodes-of-the-lateral-default-mode-network-preferentially-attend-to-the-embeddings-of-layers-1-3-while-layer-4-is-preferentially-attended-to-by-the-primary-motor-and-auditory-areas&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./attentions.png&#34; data-caption=&#34;Maps of learned LSTM-attention aggregator weights.  Each inset corresponds to the weights learned by every cortical node for the $k$-th layer hidden embedding (black: low, red: high).  We see that most of the attention mass is distributed over layers 4-7, indicating that most nodes assign maximal importance to intermediate levels of abstraction.  However, we do see spatially varying attention.  Notably, within a given attention map, we find that nodes of the lateral Default Mode Network preferentially attend to the embeddings of layers 1-3, while layer 4 is preferentially attended to by the primary motor and auditory areas.&#34;&gt;


  &lt;img src=&#34;./attentions.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Maps of learned LSTM-attention aggregator weights.  Each inset corresponds to the weights learned by every cortical node for the $k$-th layer hidden embedding (black: low, red: high).  We see that most of the attention mass is distributed over layers 4-7, indicating that most nodes assign maximal importance to intermediate levels of abstraction.  However, we do see spatially varying attention.  Notably, within a given attention map, we find that nodes of the lateral Default Mode Network preferentially attend to the embeddings of layers 1-3, while layer 4 is preferentially attended to by the primary motor and auditory areas.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Xu et al. 
&lt;a href=&#34;https://arxiv.org/pdf/1806.03536.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Representation Learning on Graphs with Jumping Knowledge Networks&lt;/a&gt;. 2018. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Dinitz et a. 
&lt;a href=&#34;https://arxiv.org/pdf/1611.01755.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large Low-Diameter Graphs are Good Expanders&lt;/a&gt;. 2017. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Lutzeyer et al. 
&lt;a href=&#34;https://arxiv.org/pdf/1712.03769.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Comparing Graph Spectra of Adjacency and Laplacian Matrices&lt;/a&gt;. 2017. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Gers, Felix.  
&lt;a href=&#34;http://www.felixgers.de/papers/phd.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Long Short-Term Memory in Recurrent Neural Networks&lt;/a&gt;. 2001. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Fan et al. 
&lt;a href=&#34;https://www.mdpi.com/2073-4441/12/1/175/htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Comparison of Long Short Term Memory Networks and the Hydrological Model in Runoff Simulation&lt;/a&gt;.  2020. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Constrained Graph Attention Networks</title>
      <link>/post/constrained-gat/</link>
      <pubDate>Fri, 25 Dec 2020 23:24:17 -0700</pubDate>
      <guid>/post/constrained-gat/</guid>
      <description>&lt;p&gt;In their recent 
&lt;a href=&#34;https://arxiv.org/abs/1910.11945&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click 
&lt;a href=&#34;#Implementation&#34;&gt;here&lt;/a&gt;).  Briefly, GATs are a 
&lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recently-developed&lt;/a&gt; neural network architecture applied to data distributed over a graph domain.  We can think of graph convolutional networks as progressively transforming and aggregating signals from within a local neighborhood of a node.  At each iteration of this process, we implicitly merge signals from larger and larger neighborhoods of the node of interest, and thereby learn unique representations of nodes that are dependent on their surroundings.&lt;/p&gt;
&lt;p&gt;GATs incorporate the seminal idea of &amp;ldquo;attention&amp;rdquo; into this learning process.  In each message-passing step, rather than updating the features of a source-node via equally-weighted contributions of neighborhood nodes, GAT models learn an attention function &amp;ndash; i.e. they learn how to differentially pay attention to various signals in the neighborhood.  In this way, the algorithm can learn to focus on imporant signals and disregard superfluous signals.  If we consider neural networks as universal funtion approximators, the attention mechanism improves the approximating ability by incorporating multiplicative weight factors into the learning.&lt;/p&gt;















&lt;figure id=&#34;figure-figure-from-velickovic-et-alhttpsarxivorgpdf171010903pdf--for-a-source-node-i-and-destination-node-j-vectors-vech_i-and-vech_j-are-the-input-feature-vectors-of-nodes-i-and-j-in-layer-l--mathbfw-is-a-learned-affine-projection-matrix--mathbfveca-is-the-learned-attention-function--the-source-and-destination-node-input-features-are-pushed-through-the-attention-layer-as-alpha_ij--sigmabigvecatmathbfwbigvech_i--vech_jbigbig-where-sigma-is-an-activation-function-and-alpha_ij-the-unnormalized-attention-that-node-i-pays-to-node-j-in-layer-l--attention-weights-are-then-passed-through-a-softmax-layer-mapping-the-attentions-between-01&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./attention_mechanism.png&#34; data-caption=&#34;Figure from Velickovic et al.  For a source node $i$ and destination node $j$, vectors $\vec{h_{i}}$ and $\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$.  $\mathbf{W}$ is a learned affine projection matrix.  $\mathbf{\vec{a}}$ is the learned attention function.  The source and destination node input features are pushed through the attention layer as $\alpha_{i,j} = \sigma\Big(\vec{a}^{T}\mathbf{W}\Big(\vec{h_{i}} || \vec{h_{j}}\Big)\Big)$ where $\sigma$ is an activation function, and $\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$.  Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].&#34;&gt;


  &lt;img src=&#34;./attention_mechanism.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure from &lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34;&gt;Velickovic et al&lt;/a&gt;.  For a source node $i$ and destination node $j$, vectors $\vec{h_{i}}$ and $\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$.  $\mathbf{W}$ is a learned affine projection matrix.  $\mathbf{\vec{a}}$ is the learned attention function.  The source and destination node input features are pushed through the attention layer as $\alpha_{i,j} = \sigma\Big(\vec{a}^{T}\mathbf{W}\Big(\vec{h_{i}} || \vec{h_{j}}\Big)\Big)$ where $\sigma$ is an activation function, and $\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$.  Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;However, GATs are not without their pitfals, as noted by Wang et al.  Notably, the authors point to two important issues that GATs suffer from: overfitting of attention values and oversmoothing of signals across class boundaries.  The authors propose that GATs overfit the attention function because the learning process is driven only by classification error, with complexity $O(|V|)$ i.e. the number of nodes in the graph.  With regards to oversmoothing, the authors note that a single attention layer can be viewed as a form of Laplacian smoothing:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y = AX^{l}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $A_{n \times n}$ is the attention weight matrix with $A_{i,j} = \alpha_{i,j}$ if $j \in \mathcal{N_{i}}$ and $0$ otherwise.  Because $\sum_{j\in \mathcal{N_{i}}} \alpha_{i,j} = 1$, we can view $A$ as a random walk transition probability matrix.  If we assume that graph $G=(V,E)$ has $K$ connected components, repeated application of $A$ to $X$ distributed over $G$ will result in a stationary distribution of node features within each connected component &amp;ndash; that is, the features vectors of the nodes within each connected component will converge on the component mean.  However, as the authors point out, we typically have multiple layers $l_{1}\dots l_{j}$, each with their own attention matrix $A_{1} \dots A_{j}$, each representing a unique transition probability matrix.  Because we generally do not have disconnected components, nodes from different classes will be connected &amp;ndash; consequentially, deep GAT networks will mix and smooth signals from different adjacent components, resulting in classification performance that worsens with network depth.  Importantly, multi-head attention networks do not alleviate this convergence issue &amp;ndash; each head can be viewed as a unique probability transition matrix, which all suffer from the same oversmoothing issue as $l \rightarrow \infty$.&lt;/p&gt;
&lt;p&gt;Wang et al. propose to incorporate two margin-based constraints into the learning process.  The first constraint, $\mathcal{L_{g}}$, addresses the overfitting issue, by enforcing that learned attentions between adjacent pairs of nodes be higher than attentions between distant pairs of nodes.&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\mathcal{L_{g}} &amp;amp;= \sum_{i \in V} \sum_{j \in \mathcal{N_{i}} \setminus \mathcal{N_{i}^{-}}} \sum_{k \in V\setminus \mathcal{N_{i}}} max(0, \phi(v_{i},v_{k}) + \zeta_{g} - \phi(v_{i},v_{j}))
\end{align}$$&lt;/p&gt;
&lt;p&gt;The second constraint, $\mathcal{L_{b}}$, address the oversmoothing issue, by enforcing that learned attentions between pairs of adjacent nodes with the same label be higher than attention between pairs of adjacent nodes with different labels:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\mathcal{L_{b}} &amp;amp;= \sum_{i \in V}\sum_{j \in \mathcal{N_{i}^{+}}} \sum_{k \in \mathcal{N_{i}^{-}}} max(0, \phi(v_{i},v_{k}) + \zeta_{b} - \phi(v_{i},v_{j}))
\end{align}$$&lt;/p&gt;
&lt;p&gt;In both cases, $\phi(,)$ is the attention function between a pair of nodes, $\mathcal{N_{i}^{+}}$ and $\mathcal{N_{i}^{-}}$ are the nodes adjacent to node $i$ with the same (+) and different (-) labels as $i$, and $\zeta_{g}$ and $\zeta_{b}$ are slack variables controlling the margin between attention values.  The first loss function, $\mathcal{L_{g}}$, can be implemented via negative sampling of nodes (the authors actually perform importance-based negative sampling based on attention-weighted node degrees, but showed that this only marginally improved classification accuracy in benchmark datasets).&lt;/p&gt;
&lt;p&gt;The authors propose one final addition to alleviate the oversmoothing issue posed by vanilla GATs.  Rather than aggregating over all adjacent nodes in a neighborhood, the authors propose to aggregate over the nodes with the $K$ greatest attention values.  Because the class boundary loss $\mathcal{L_{b}}$ enforces large attentions on nodes of the same label and small attention on nodes of different labels, aggregating over the top $K$ nodes will tend to exclude nodes of different labels than the source node in the message passing step, thereby preventing oversmoothing.  The authors show that this constrained aggregation approach is preferable to attention dropout proposed in the original 
&lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAT paper&lt;/a&gt;.  &lt;a name=&#34;Implementation&#34;&gt;
Taken together, the authors deem these margin-based loss and constrained aggregation &amp;ldquo;Constrained Graph Attention Network&amp;rdquo; (C-GAT).&lt;/p&gt;
&lt;/a&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;I wasn&amp;rsquo;t able to find an implementation of the Constrained Graph Attention Network for my own purposes, so I&amp;rsquo;ve implemented the algorithm myself in 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt; (DGL) &amp;ndash; the source code for this convolutional layer can be found 
&lt;a href=&#34;https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/conv/cgatconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.  This implementation makes use of the original DGL &lt;code&gt;GATConv&lt;/code&gt; layer structure, with modifications made for the constraints and aggregations.  Specifically, the API for &lt;code&gt;CGATConv&lt;/code&gt; has the following modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
CGATCONV(in_feats, 
         out_feats, 
         num_heads, 
         feat_drop=0., 
         graph_margin=0.1, # graph structure loss slack variable
         class_margin=0.1, # class boundary loss slack variable
         top_k=3, # number of messages to aggregate over
         negative_slope=0.2,
         residual=False,
         activation=None,
         allow_zero_in_degree=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of note is the fact that the &lt;code&gt;attn_drop&lt;/code&gt; parameter has been substituted by the &lt;code&gt;top_k&lt;/code&gt; parameter in order to mitigate oversmoothing, and the two slack variables $\zeta_{g}$ and $\zeta_{b}$ are provided as &lt;code&gt;graph_margin&lt;/code&gt; and &lt;code&gt;class_margin&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With regards to the loss functions, the authors compute all-pairs differences between all edges incident on a source node, instead of summing over the positive / negative sample attentions ($\mathcal{L_{g}}$) and same / different label attentions ($\mathcal{L_{b}}$) and then differencing these summations.  In this way, the C-GAT model anchors the loss values to specific nodes, promoting learning of more generalizable attention weights.  The graph structure loss function $\mathcal{L_{g}}$ is implemented with the &lt;code&gt;graph_loss&lt;/code&gt; reduction function below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def graph_loss(nodes):
            
    &amp;quot;&amp;quot;&amp;quot;
    Loss function on graph structure.
    
    Enforces high attention to adjacent nodes and 
    lower attention to distant nodes via negative sampling.
    &amp;quot;&amp;quot;&amp;quot;

    msg = nodes.mailbox[&#39;m&#39;]

    pw = msg[:, :, :, 0, :].unsqueeze(1)
    nw = msg[:, :, :, 1, :].unsqueeze(2)

    loss = (nw + self._graph_margin - pw).clamp(0)
    loss = loss.sum(1).sum(1).squeeze()

    return {&#39;graph_loss&#39;: loss}
.
.
.
graph.srcdata.update({&#39;ft&#39;: feat_src, &#39;el&#39;: el})
graph.dstdata.update({&#39;er&#39;: er})
graph.apply_edges(fn.u_add_v(&#39;el&#39;, &#39;er&#39;, &#39;e&#39;))
e = self.leaky_relu(graph.edata.pop(&#39;e&#39;))

# construct the negative graph by shuffling edges
# does not assume a single graph or blocked graphs
# see cgatconv.py for ```construct_negative_graph``` function
neg_graph = [construct_negative_graph(i, k=1) for i in dgl.unbatch(graph)]
neg_graph = dgl.batch(neg_graph)

neg_graph.srcdata.update({&#39;ft&#39;: feat_src, &#39;el&#39;: el})
neg_graph.dstdata.update({&#39;er&#39;: er})
neg_graph.apply_edges(fn.u_add_v(&#39;el&#39;, &#39;er&#39;, &#39;e&#39;))
ne = self.leaky_relu(neg_graph.edata.pop(&#39;e&#39;))

combined = th.stack([e, ne]).transpose(0, 1).transpose(1, 2)
graph.edata[&#39;combined&#39;] = combined
graph.update_all(fn.copy_e(&#39;combined&#39;, &#39;m&#39;), graph_loss)

# compute graph structured loss
Lg = graph.ndata[&#39;graph_loss&#39;].sum() / (graph.num_nodes() * self._num_heads)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, the class boundary loss function $\mathcal{L_{b}}$ is implemented with the following message and reduce functions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def adjacency_message(edges):
            
    &amp;quot;&amp;quot;&amp;quot;
    Compute binary message on edges.

    Compares whether source and destination nodes
    have the same or different labels.
    &amp;quot;&amp;quot;&amp;quot;

    l_src = edges.src[&#39;l&#39;]
    l_dst = edges.dst[&#39;l&#39;]

    if l_src.ndim &amp;gt; 1:
        adj = th.all(l_src == l_dst, dim=1)
    else:
        adj = (l_src == l_dst)

    return {&#39;adj&#39;: adj.detach()}

def class_loss(nodes):
    
    &amp;quot;&amp;quot;&amp;quot;
    Loss function on class boundaries.
    
    Enforces high attention to adjacent nodes with the same label
    and lower attention to adjacent nodes with different labels.
    &amp;quot;&amp;quot;&amp;quot;

    m = nodes.mailbox[&#39;m&#39;]

    w = m[:, :, :-1]
    adj = m[:, :, -1].unsqueeze(-1).bool()

    same_class = w.masked_fill(adj == 0, np.nan).unsqueeze(2)
    diff_class = w.masked_fill(adj == 1, np.nan).unsqueeze(1)

    difference = (diff_class + self._class_margin - same_class).clamp(0)
    loss = th.nansum(th.nansum(difference, 1), 1)

    return {&#39;boundary_loss&#39;: loss}
.
.
.
graph.ndata[&#39;l&#39;] = label
graph.apply_edges(adjacency_message)
adj = graph.edata.pop(&#39;adj&#39;).float()

combined = th.cat([e.squeeze(), adj.unsqueeze(-1)], dim=1)
graph.edata[&#39;combined&#39;] = combined
graph.update_all(fn.copy_e(&#39;combined&#39;, &#39;m&#39;), class_loss)
Lb = graph.ndata[&#39;boundary_loss&#39;].sum() / (graph.num_nodes() * self._num_heads)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, the constrained message aggregation is implemented using the following reduction function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def topk_reduce_func(nodes):
    
    `&amp;quot;&amp;quot;&amp;quot;
    Aggregate attention-weighted messages over the top-K 
    attention-valued destination nodes
    &amp;quot;&amp;quot;&amp;quot;

    K = self._top_k

    m = nodes.mailbox[&#39;m&#39;]
    [m,_] = th.sort(m, dim=1, descending=True)
    m = m[:,:K,:,:].sum(1)

    return {&#39;ft&#39;: m}
.
.
.
# message passing
if self._top_k is not None:
    graph.update_all(fn.u_mul_e(&#39;ft&#39;, &#39;a&#39;, &#39;m&#39;), 
                    topk_reduce_func)
else:
    graph.update_all(fn.u_mul_e(&#39;ft&#39;, &#39;a&#39;, &#39;m&#39;),
                    fn.sum(&#39;m&#39;, &#39;ft&#39;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cross-Entropy With Structure</title>
      <link>/post/structured-cross-entropy/</link>
      <pubDate>Wed, 09 Dec 2020 01:12:32 -0700</pubDate>
      <guid>/post/structured-cross-entropy/</guid>
      <description>&lt;p&gt;As I mentioned in my previous 
&lt;a href=&#34;/post/gaussian-kernel-convolution/&#34;&gt;post&lt;/a&gt;, I work with cortical surface segmentation data.  Due to the biology of the human brain, there is considerable reproducible structure and function across individuals (thankfully!).  One manifestion of this reproducibility is exemplified by the neocortex a.k.a. the thin (~2.5mm) gray matter layer of cell-bodies at the periphery of the brain.  The neocortex is well known to have local homogeneity in terms of types of neuronal cells, protein and gene expression, and large-scale function, for example.  Naturally, researchers have been trying to identify discrete delineations of the cortex for nearly 100 years, by looking for regions of local homogeneity of various features along the cortical manifold.&lt;/p&gt;
&lt;p&gt;As in my previous post, I&amp;rsquo;m working on this problem using graph convolution networks (GCN).  Given the logits output by a forward pass of a GCN, I want to classify a cortical node as belonging to some previously identified cortical area.  Using categorical cross-entropy, we can calculate the loss of a given foward pass of the model $h(X; \Theta)$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
L = -\sum_{k=1}^{K} \sum_{l \in \mathcal{L}} x_{l}^{k} \cdot log(\sigma(x^{k})_{l})
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where $x^{k}$ is the output of the model for a single node, $x_{l}^{k}$ is the one-hot-encoding value of the true labels, and $\sigma$ is the softmax function.  Importantly, the cross-entropy cost is high when the probability assigned to the true label of a node is small i.e. $log(0) = \infty$, while $log(1) = 0$ &amp;ndash; as such, the cross-entropy tries to minimize the rate of false negatives.&lt;/p&gt;
&lt;p&gt;However, we can incorporate more &lt;em&gt;structure&lt;/em&gt; into this loss function.  As I mentioned previously, we know that the brain is highly reproducible across individuals.  In our case, we have years of biological evidence pointing to the fact that functional brain areas i.e. like the primary visual area (V1), will always be in the same anatomical location i.e. posterior occipital cortex &amp;ndash; and will always be adjacent to a small subjset of other functionally-defined areas, like the secondary visual area (V2), for example.&lt;/p&gt;















&lt;figure id=&#34;figure-various-maps-of-the-primate-visual-cortex--tootell-et-al-2003httpswwwjneurosciorgcontent23103981&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.jneurosci.org/content/jneuro/23/10/3981/F1.large.jpg?width=800&amp;amp;height=600&amp;amp;carousel=1&#34; data-caption=&#34;Various maps of the primate visual cortex.  Tootell et al, 2003.&#34;&gt;


  &lt;img src=&#34;https://www.jneurosci.org/content/jneuro/23/10/3981/F1.large.jpg?width=800&amp;amp;height=600&amp;amp;carousel=1&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Various maps of the primate visual cortex.  &lt;a href=&#34;https://www.jneurosci.org/content/23/10/3981&#34;&gt;Tootell et al, 2003&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This leads us to the idea of assigning a high cost when nodes which should be in V1, for example, are assigned labels of regions that are not adjacent to V1.  We do so by by defining another cost function:&lt;/p&gt;
&lt;p&gt;\begin{align}
G = -\sum_{k=1}^{k}\sum_{l \in \mathcal{L}} \sum_{h \in \mathcal{L} \setminus \mathcal{N_{l}}} w_{l}^{k} \cdot log(1-\sigma(x^{k})_{l})
\end{align}&lt;/p&gt;
&lt;p&gt;where $w_{l}^{k}$ is the probability weight assigned to label $h \in \mathcal{L}\setminus \mathcal{N_{l}}$ i.e. the set of labels not adjacent to label $l$.  In order to follow the idea of a cross-entropy, we enforce the following constraints on weights $\mathbf{w}$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
w_{l}^{k} &amp;gt;&amp;amp;= 0 \\&lt;br&gt;
\sum_{l \in \mathcal{L}} w_{l}^{k} &amp;amp;= 1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;such that the vector $\mathbf{w}$ is a probability distribution over labels.  Importantly, if we  consider more closely what this loss-function is doing, we are encouraging the predicted label of $x^{k}$ to &lt;strong&gt;not&lt;/strong&gt; be in the set $\mathcal{L} \setminus \mathcal{N_{l}}$.  Assume, for example, that the true label of $x^{k}$ is $t$, and that label $j$ is not adjacent to label $t$ on the cortical surface.  If the softmax function assigns a probability $p(x^{k}_{l} = j) = 0.05$, then $log(1-p(x^{k}_{l} = j))$ will be small.  However, if $p(x^{k}_{l} = j) = 0.95$, then $log(1-p(x^{k}_{l} = j))$ will be large.  Consequentially, we penalize higher probabilities assigned to labels not adjacent to our true label &amp;ndash; i.e. ones that are not even biologically plausible.  If a candidate label of $x^{k}_{l} \in \mathcal{N_{t}}$, we simply set $w_{l}^{k} = 0$ &amp;ndash; that is, we do not penalize the true label (obviously), or labels adjacent to the true label, since these are the regions we really want to consider.&lt;/p&gt;
&lt;p&gt;Below, I&amp;rsquo;ve implemented this loss function using 
&lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch&lt;/a&gt; and 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt;.  Assume that we are given the adjacency matrix of our mesh, the logits of our model, and the true label of our training data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

import dgl
import dgl.function as fn
import torch.nn.functional as F

import torch as th

def structured_cross_entropy(graph, logits, target):
    
    &amp;quot;&amp;quot;&amp;quot;
    Compute a structured cross-entropy loss.
    
    Loss penalizes high logit probabilities assigned to labels
    that are not directly adjacent to the true label.
    
    Parameters:
    - - - - -
    graph: DGL graph
        input graph structure
    input: torch tensor
        logits from model
    target: torch tensor
        true node labeling
    Returns:
    - - - -
    loss: torch tensor
        structured cross-entropy loss
    &amp;quot;&amp;quot;&amp;quot;
    
    # compute one-hot encoding of true labels
    hot_encoding = F.one_hot(target).float()
    
    # identify adjacent labels
    weight = th.matmul(hot_encoding.t(), 
                               th.matmul(graph.adjacency_matrix(), hot_encoding))
    weight = (1-(weight&amp;gt;0).float())

    # compute inverted encoding (non-adjacent labels receive value of 1)
    inv_encoding = weight[target]
    # weight by 1/(# non adjacent)
    # all non-adjacent labels receive the same probability
    # adjacent labels and self-label receive probability of 0
    inv_encoding = inv_encoding / inv_encoding.sum(1).unsqueeze(1)
    loss = th.sum(inv_encoding*th.log(1-F.softmax(logits)), 1)

    return -loss.mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to use this loss function in conjunction with another loss, like the usual cross-entropy, we could perform something like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# define a regularizing parameter
gamma = 0.1
# define the usual cross-entropy loss function
loss_fcn = torch.nn.CrossEntropyLoss()
loss = loss_function(logits, target) + gamma*structured_cross_entropy(graph, logits, target)

# because our new loss functions performs computations using Pytorch
# the computation history is stored, and we can compute the gradient 
# with respect to this combined loss as

optimizer.zero_grad() # zero the gradients (no history)
loss.backward() # compute new gradients
optimizer.step() # update weights and parameters w.r.t new gradient

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we&amp;rsquo;re optimizing two loss functions now i.e. the global accuracy of the model as defined using the conventional cross-entropy, &lt;strong&gt;and&lt;/strong&gt; the desire for predicted labels to &lt;em&gt;not&lt;/em&gt; be far away from the true label using the structured cross-entropy, this combination of loss functions will likely have the effect of slightly reducing global accuracy &amp;ndash; however, it will have the effect of generating predictions showing fewer anatomically spurious labels i.e. we are less likely to see vertices in the frontal lobe labeled as V1, or vertices in the lateral parietal cortex labeled as Anterior Cingulate.  Global predictions will be more biologically plausible.  While GCNs as a whole are alreadly better-able to incorporate local spatial information than other models due to the fact that they convolve signals based on the adjacency structure of the network in question, I have found empirically that these anatomically spurious predictions are still possible &amp;ndash; hence the need for this more-structured regularization.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Graph Convolutional Networks</title>
      <link>/post/gaussian-kernel-convolution/</link>
      <pubDate>Mon, 07 Dec 2020 23:24:17 -0700</pubDate>
      <guid>/post/gaussian-kernel-convolution/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m using 
&lt;a href=&#34;https://tkipf.github.io/graph-convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;graph convolutional networks&lt;/a&gt; as a tool to segment the cortical surface of the brain.  This research resides in the domain of &lt;em&gt;node classification&lt;/em&gt; using &lt;em&gt;inductive learning&lt;/em&gt;.  By node classification, I mean that we wish to assign a discrete label to cortical surface locations (nodes / vertices in a graph) on the basis of some feature data and brain network topology.  By inductive learning, I mean that we will train, validate, and test on datasets with possibly different graph topologies &amp;ndash; this is in contrast to &lt;em&gt;transductive learning&lt;/em&gt; that learns models that do not generalize to arbitrary network topology.&lt;/p&gt;
&lt;p&gt;In conventional convolutions over regular grid domains, such as images, using approaches like
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convolutional_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ConvNet&lt;/a&gt;, we learn the parameters of a sliding filter that convolves the signal around a pixel of interest $p_{i,j}$, such that we aggregate the information from pixels $p_{\Delta i, \Delta j}$ for some fixed distance $\Delta$ away from $p$. Oftentimes, however, we encounter data that is distributed over a graphical domain, such as social networks, journal citations, brain connectivity, or the electrical power grid.  In such cases, concepts like &amp;ldquo;up&amp;rdquo;, &amp;ldquo;down&amp;rdquo;, &amp;ldquo;left&amp;rdquo;, and &amp;ldquo;right&amp;rdquo; do not make sense &amp;ndash; what does it mean to be &amp;ldquo;up&amp;rdquo; from something in a network? &amp;ndash; so we need some other notion of neighborhood.&lt;/p&gt;
&lt;p&gt;In come graph convolutional networks (GCNs).  GCNs generalize the idea of neighborhood aggregation to the graph domain by utilizing the adjacency structure of a network &amp;ndash; we can now aggregate signals near a node by using some neighborhood around it.  While vanilla GCNs learn rotationally-invariant filters, recent developments in the world of 
&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer networks&lt;/a&gt; have opened up the door for much more flexible and inductive models (see: 
&lt;a href=&#34;https://arxiv.org/abs/1710.10903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Attention Networks&lt;/a&gt;, 
&lt;a href=&#34;https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GraphSAGE&lt;/a&gt;).&lt;/p&gt;















&lt;figure id=&#34;figure-demonstration-of-graph-convolution-network-from-thomas-kipfhttpstkipfgithubiograph-convolutional-networks&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png&#34; data-caption=&#34;Demonstration of graph convolution network from Thomas Kipf.&#34;&gt;


  &lt;img src=&#34;https://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Demonstration of graph convolution network from &lt;a href=&#34;https://tkipf.github.io/graph-convolutional-networks/&#34;&gt;Thomas Kipf&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;I was specifically interested in applying the methodology described 
&lt;a href=&#34;http://arxiv.org/abs/1803.10336&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, where the authors utilitize Gaussian kernels as filters over the neighborhood of nodes.  However, the authors did not open-source their code &amp;ndash; as such, I needed to implement this method myself.  Assume our input data to layer $l$ is $Y^{(l)} \in \mathbb{R}^{N \times q}$ for $N$ nodes in the graph.  We can define the Gaussian kernel-weighted convolution as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
z_{i,p}^{(l)} = \sum_{j \in \mathcal{N}_{i}} \sum_{q=1}^{M_{(l)}} \sum_{k=1}^{K_{(l)}} w_{p,q,k}^{(l)} \cdot y_{j,q}^{(l)} \cdot \phi(\hat{\mu}_{i}, \hat{\mu}_{j}; \Theta_{k}^{(l)}) + b_{p}^{(l)}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Above, $y_{j,q}^{(l)}$ is the $q$-th input feature of neighboring node $j$, $w_{p,q,k}^{(l)}$ is the linear weight assigned to this feature for the $k$-th kernel, and $\phi(\hat{\mu}_{i}, \hat{\mu}_{j}; \Theta_{k}^{(l)})$ is the $k$-th kernel weight between node $i$ and node $j$, defined as:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\phi(\hat{\mu_{i}}, \hat{\mu_{j}}; \sigma_{k}^{(l)}, \mu_{k}^{(l)}  ) = \exp^{-\sigma_{k}^{(l)} \left\Vert (\hat{\mu_{i}} - \hat{\mu_{j}}) - \mu_{k}^{(l)} \right\Vert^{2}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Extrinsically, the kernel weights are represented by edges in a sparse affinity matrix, such that index $(i,j)$ is the Gaussian kernel weight between node $i$ and node $j$ for the $k$-th kernel in the $l$-th layer, where nodes $j$ are restricted to be within a certain neighborhood or distance of node $i$.  This can be seen more clearly here:&lt;/p&gt;















&lt;figure id=&#34;figure-figure-from-wu-et-alhttpswwwncbinlmnihgovpmcarticlespmc7052684--v_i-is-our-voxel-of-interest-and-v_ki-for-demonstration-purposes-is-an-adjacent-node--both-v_i-and-v_ki-are-characterized-by-embedding-vectors-e_i-e_ki-in-mathbbrq-from-which-we-compute-the-kernel-weight-phi_ik-characterizing-how-similar-the-two-vertices-embedding-vectors-are&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./gaussian_radius.png&#34; data-caption=&#34;Figure from Wu et al..  $v_{i}$ is our voxel of interest, and $v_{k}^{i}$, for demonstration purposes, is an adjacent node.  Both $v_{i}$ and $v_{k}^{i}$ are characterized by embedding vectors $e_{i}, e_{k}^{i} \in \mathbb{R}^{q}$, from which we compute the kernel weight $\phi_{i,k}$ characterizing how similar the two vertices&amp;rsquo; embedding vectors are.&#34;&gt;


  &lt;img src=&#34;./gaussian_radius.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure from &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7052684/&#34;&gt;Wu et al.&lt;/a&gt;.  $v_{i}$ is our voxel of interest, and $v_{k}^{i}$, for demonstration purposes, is an adjacent node.  Both $v_{i}$ and $v_{k}^{i}$ are characterized by embedding vectors $e_{i}, e_{k}^{i} \in \mathbb{R}^{q}$, from which we compute the kernel weight $\phi_{i,k}$ characterizing how similar the two vertices&amp;rsquo; embedding vectors are.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;I implemented a new convolutional layer called &lt;code&gt;GAUSConv&lt;/code&gt; (available 
&lt;a href=&#34;https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/layers/gausconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).  To implement this algorithm, I utilized the 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt; (DGL), which offers a stellar single unifed API based on message passing (I&amp;rsquo;m using 
&lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch&lt;/a&gt; as the backend).  I noticed that I could formulate this problem using attention mechanisms described in the 
&lt;a href=&#34;https://arxiv.org/abs/1710.10903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Attention Network&lt;/a&gt; paper &amp;ndash; however, instead of computing attention weights using a fully connected layer as described in that work, I would compute kernel weights using Gaussian filters.  Similarly, just as the GAT paper describes &lt;em&gt;multi-head attention&lt;/em&gt; for multiple attention channels, I could analogize my fomulation to &lt;em&gt;multi-head kernels&lt;/em&gt; for multiple kernel channels.  To this end, I could make use of the 
&lt;a href=&#34;https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/gatconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;GATConv&lt;/code&gt;&lt;/a&gt; API quite easily by replacing the attention computations with the Gaussian kernel filtrations.  Likewise, I utilized the 
&lt;a href=&#34;https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/graphconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;GraphConv&lt;/code&gt;&lt;/a&gt; API to incorporate linear weights from the 
&lt;a href=&#34;https://arxiv.org/pdf/1609.02907.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Convolution Network&lt;/a&gt; paper.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;GAUSConv&lt;/code&gt; layer is similar to both the &lt;code&gt;GraphConv&lt;/code&gt; and &lt;code&gt;GATConv&lt;/code&gt; layers but differs in a few places.  Rather than initializing the layer with attention heads, we initialize it with the number of kernels and a kernel dropout probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
GAUSConv(in_feats, # number of input dimensions
         out_feats, # number of output features
         num_kernels, # number of kernels for current layer
         feat_drop=0., # dropout probability of features
         kernel_drop=0., # dropout probability of kernels
         negative_slope=0.2, # leakly relu slope
         activation=None, # activation function to apply after forward pass
         random_seed=None, # for example / reproducibility purposes
         allow_zero_in_degree=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Importantly, in the layer instantiation, we define &lt;strong&gt;linear weights&lt;/strong&gt; &lt;em&gt;and&lt;/em&gt; &lt;strong&gt;kernel mean and sigma parameters&lt;/strong&gt;, &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;.  We initialize both kernel parameters with the flag &lt;code&gt;require_grad=True&lt;/code&gt;, which enables us to update these kernel parameters during the backward pass of the layer.  Both parameters are initialized with values in the &lt;code&gt;reset_parameters&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# initialize feature weights and bias vector
self.weights = nn.Parameter(
    th.Tensor(num_kernels, in_feats, out_feats), requires_grad=True)
self.bias = nn.Parameter(
    th.Tensor(num_kernels, out_feats), requires_grad=True)

# initialize kernel perameters
self.mu = nn.Parameter(
    th.Tensor(1, num_kernels, in_feats), requires_grad=True)
self.sigma = nn.Parameter(
    th.Tensor(num_kernels, 1), requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now here is the clever part, and where the 
&lt;a href=&#34;https://docs.dgl.ai/en/0.4.x/api/python/function.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DGL message passing interface&lt;/a&gt; really shines through.  DGL fuses the &lt;code&gt;send&lt;/code&gt; and &lt;code&gt;receive&lt;/code&gt; messages so that no messages between nodes are ever explicitly stored, using built-in &lt;strong&gt;message&lt;/strong&gt; and &lt;strong&gt;reduce&lt;/strong&gt; functions.  To compute the kernel weights between all pairs of source and destrination nodes, we use these built-in functions.  The important steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;compute node feature differences between all source / destination node pairs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;aggregate and reduce incoming messages from destination nodes scaled by the kernel weights, to update the source node features&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the forward pass of our layer, we perform the following steps:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;### forward pass of GAUSConv layer ###

# compute all pairwise differences between adjacent node features
graph.ndata[&#39;h&#39;] = feat
graph.apply_edges(fn.u_sub_v(&#39;h&#39;, &#39;h&#39;, &#39;diff&#39;))

# compute kernel weights for each source / desintation pair
e = graph.edata[&#39;diff&#39;].unsqueeze(1) - mu
e = -1*sigma*th.norm(e, dim=2).unsqueeze(2)
e = e.exp()
graph.edata[&#39;e&#39;] = e

# apply kernel weights to destination node features
graph.apply_edges(fn.v_mul_e(&#39;h&#39;, &#39;e&#39;, &#39;kw&#39;))

# apply linear projection to kernel-weighted destination node features
a = th.sum(th.matmul(graph.edata[&#39;kw&#39;].transpose(1, 0), weights), dim=0)

# apply kernel dropout
a = self.kernel_drop(a)
graph.edata[&#39;a&#39;] = a

# final message-passing and reduction step
# aggregate weighted destination node features to update source node features
graph.update_all(fn.copy_e(&#39;a&#39;, &#39;m&#39;), fn.sum(&#39;m&#39;, &#39;h&#39;))
rst = graph.ndata[&#39;h&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, given a graph and features, we instantiate a &lt;code&gt;GAUSConv&lt;/code&gt; layer and propogate our features through the network via:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# set random seed
random_seed=1

# define arbitrary input/output feature shape
n_samples = 4
in_feats=4
out_feats=2
features = th.ones(n_samples, in_feats)

# define number of kernels
num_kernels=2

# create graph structure
u, v = th.tensor([0, 0, 0, 1]), th.tensor([1, 2, 3, 3])
g = dgl.graph((u, v))
g = dgl.to_bidirected(g)
g = dgl.add_self_loop(g)

# instantiate layer
GausConv = GAUSConv(in_feats=in_feats,
                    out_feats=out_feats,
                    random_seed=random_seed,
                    num_kernels=num_kernels,
                    feat_drop=0,
                    kernel_drop=0)
                    
# forward pass of layer
logits = GausConv(g, features)

print(logits)
tensor([[0.1873, 0.7217],
        [0.1405, 0.5413],
        [0.0936, 0.3608],
        [0.1405, 0.5413]], grad_fn=&amp;lt;AddBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
