<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  

  
  
  
    
  
  <meta name="description" content="In their recent paper, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click here). Briefly, GATs are a recently-developed neural network architecture applied to data distributed over a graph domain.">

  
  <link rel="alternate" hreflang="en-us" href="/post/constrained-gat/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-186337108-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-186337108-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hue3d34eb7f27f0837dcea880d4d221db4_377404_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hue3d34eb7f27f0837dcea880d4d221db4_377404_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/post/constrained-gat/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="A Rambling On">
  <meta property="og:url" content="/post/constrained-gat/">
  <meta property="og:title" content="Constrained Graph Attention Networks | A Rambling On">
  <meta property="og:description" content="In their recent paper, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click here). Briefly, GATs are a recently-developed neural network architecture applied to data distributed over a graph domain."><meta property="og:image" content="/img/Bayes.jpg">
  <meta property="twitter:image" content="/img/Bayes.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-12-25T23:24:17-07:00">
    
    <meta property="article:modified_time" content="2020-12-25T23:24:17-07:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/constrained-gat/"
  },
  "headline": "Constrained Graph Attention Networks",
  
  "datePublished": "2020-12-25T23:24:17-07:00",
  "dateModified": "2020-12-25T23:24:17-07:00",
  
  "publisher": {
    "@type": "Organization",
    "name": "A Rambling On",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hue3d34eb7f27f0837dcea880d4d221db4_377404_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "In their recent paper, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click here). Briefly, GATs are a recently-developed neural network architecture applied to data distributed over a graph domain."
}
</script>

  

  


  


  





  <title>Constrained Graph Attention Networks | A Rambling On</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">A Rambling On</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">A Rambling On</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/personal"><span>Personal</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/Resume.pdf"><span>Resume</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Constrained Graph Attention Networks</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Dec 25, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  
  

  
  

</div>

    













<div class="btn-links mb-3">
  
  








  









  
  <a class="btn btn-outline-primary my-1 mr-1" href="/project/parcellearning/">
    Project
  </a>
  











</div>


  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>In their recent 
<a href="https://arxiv.org/abs/1910.11945" target="_blank" rel="noopener">paper</a>, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click 
<a href="#Implementation">here</a>).  Briefly, GATs are a 
<a href="https://arxiv.org/pdf/1710.10903.pdf" target="_blank" rel="noopener">recently-developed</a> neural network architecture applied to data distributed over a graph domain.  We can think of graph convolutional networks as progressively transforming and aggregating signals from within a local neighborhood of a node.  At each iteration of this process, we implicitly merge signals from larger and larger neighborhoods of the node of interest, and thereby learn unique representations of nodes that are dependent on their surroundings.</p>
<p>GATs incorporate the seminal idea of &ldquo;attention&rdquo; into this learning process.  In each message-passing step, rather than updating the features of a source-node via equally-weighted contributions of neighborhood nodes, GAT models learn an attention function &ndash; i.e. they learn how to differentially pay attention to various signals in the neighborhood.  In this way, the algorithm can learn to focus on imporant signals and disregard superfluous signals.  If we consider neural networks as universal funtion approximators, the attention mechanism improves the approximating ability by incorporating multiplicative weight factors into the learning.</p>















<figure id="figure-figure-from-velickovic-et-alhttpsarxivorgpdf171010903pdf--for-a-source-node-i-and-destination-node-j-vectors-vech_i-and-vech_j-are-the-input-feature-vectors-of-nodes-i-and-j-in-layer-l--mathbfw-is-a-learned-affine-projection-matrix--mathbfveca-is-the-learned-attention-function--the-source-and-destination-node-input-features-are-pushed-through-the-attention-layer-as-alpha_ij--sigmabigvecatmathbfwbigvech_i--vech_jbigbig-where-sigma-is-an-activation-function-and-alpha_ij-the-unnormalized-attention-that-node-i-pays-to-node-j-in-layer-l--attention-weights-are-then-passed-through-a-softmax-layer-mapping-the-attentions-between-01">


  <a data-fancybox="" href="./attention_mechanism.png" data-caption="Figure from Velickovic et al.  For a source node $i$ and destination node $j$, vectors $\vec{h_{i}}$ and $\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$.  $\mathbf{W}$ is a learned affine projection matrix.  $\mathbf{\vec{a}}$ is the learned attention function.  The source and destination node input features are pushed through the attention layer as $\alpha_{i,j} = \sigma\Big(\vec{a}^{T}\mathbf{W}\Big(\vec{h_{i}} || \vec{h_{j}}\Big)\Big)$ where $\sigma$ is an activation function, and $\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$.  Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].">


  <img src="./attention_mechanism.png" alt=""  >
</a>


  
  
  <figcaption>
    Figure from <a href="https://arxiv.org/pdf/1710.10903.pdf">Velickovic et al</a>.  For a source node $i$ and destination node $j$, vectors $\vec{h_{i}}$ and $\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$.  $\mathbf{W}$ is a learned affine projection matrix.  $\mathbf{\vec{a}}$ is the learned attention function.  The source and destination node input features are pushed through the attention layer as $\alpha_{i,j} = \sigma\Big(\vec{a}^{T}\mathbf{W}\Big(\vec{h_{i}} || \vec{h_{j}}\Big)\Big)$ where $\sigma$ is an activation function, and $\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$.  Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].
  </figcaption>


</figure>

<p>However, GATs are not without their pitfals, as noted by Wang et al.  Notably, the authors point to two important issues that GATs suffer from: overfitting of attention values and oversmoothing of signals across class boundaries.  The authors propose that GATs overfit the attention function because the learning process is driven only by classification error, with complexity $O(|V|)$ i.e. the number of nodes in the graph.  With regards to oversmoothing, the authors note that a single attention layer can be viewed as a form of Laplacian smoothing:</p>
<p>$$\begin{align}
Y = AX^{l}
\end{align}$$</p>
<p>where $A_{n \times n}$ is the attention weight matrix with $A_{i,j} = \alpha_{i,j}$ if $j \in \mathcal{N_{i}}$ and $0$ otherwise.  Because $\sum_{j\in \mathcal{N_{i}}} \alpha_{i,j} = 1$, we can view $A$ as a random walk transition probability matrix.  If we assume that graph $G=(V,E)$ has $K$ connected components, repeated application of $A$ to $X$ distributed over $G$ will result in a stationary distribution of node features within each connected component &ndash; that is, the features vectors of the nodes within each connected component will converge on the component mean.  However, as the authors point out, we typically have multiple layers $l_{1}\dots l_{j}$, each with their own attention matrix $A_{1} \dots A_{j}$, each representing a unique transition probability matrix.  Because we generally do not have disconnected components, nodes from different classes will be connected &ndash; consequentially, deep GAT networks will mix and smooth signals from different adjacent components, resulting in classification performance that worsens with network depth.  Importantly, multi-head attention networks do not alleviate this convergence issue &ndash; each head can be viewed as a unique probability transition matrix, which all suffer from the same oversmoothing issue as $l \rightarrow \infty$.</p>
<p>Wang et al. propose to incorporate two margin-based constraints into the learning process.  The first constraint, $\mathcal{L_{g}}$, addresses the overfitting issue, by enforcing that learned attentions between adjacent pairs of nodes be higher than attentions between distant pairs of nodes.</p>
<p>$$\begin{align}
\mathcal{L_{g}} &amp;= \sum_{i \in V} \sum_{j \in \mathcal{N_{i}} \setminus \mathcal{N_{i}^{-}}} \sum_{k \in V\setminus \mathcal{N_{i}}} max(0, \phi(v_{i},v_{k}) + \zeta_{g} - \phi(v_{i},v_{j}))
\end{align}$$</p>
<p>The second constraint, $\mathcal{L_{b}}$, address the oversmoothing issue, by enforcing that learned attentions between pairs of adjacent nodes with the same label be higher than attention between pairs of adjacent nodes with different labels:</p>
<p>$$\begin{align}
\mathcal{L_{b}} &amp;= \sum_{i \in V}\sum_{j \in \mathcal{N_{i}^{+}}} \sum_{k \in \mathcal{N_{i}^{-}}} max(0, \phi(v_{i},v_{k}) + \zeta_{b} - \phi(v_{i},v_{j}))
\end{align}$$</p>
<p>In both cases, $\phi(,)$ is the attention function between a pair of nodes, $\mathcal{N_{i}^{+}}$ and $\mathcal{N_{i}^{-}}$ are the nodes adjacent to node $i$ with the same (+) and different (-) labels as $i$, and $\zeta_{g}$ and $\zeta_{b}$ are slack variables controlling the margin between attention values.  The first loss function, $\mathcal{L_{g}}$, can be implemented via negative sampling of nodes (the authors actually perform importance-based negative sampling based on attention-weighted node degrees, but showed that this only marginally improved classification accuracy in benchmark datasets).</p>
<p>The authors propose one final addition to alleviate the oversmoothing issue posed by vanilla GATs.  Rather than aggregating over all adjacent nodes in a neighborhood, the authors propose to aggregate over the nodes with the $K$ greatest attention values.  Because the class boundary loss $\mathcal{L_{b}}$ enforces large attentions on nodes of the same label and small attention on nodes of different labels, aggregating over the top $K$ nodes will tend to exclude nodes of different labels than the source node in the message passing step, thereby preventing oversmoothing.  The authors show that this constrained aggregation approach is preferable to attention dropout proposed in the original 
<a href="https://arxiv.org/pdf/1710.10903.pdf" target="_blank" rel="noopener">GAT paper</a>.  <a name="Implementation">
Taken together, the authors deem these margin-based loss and constrained aggregation &ldquo;Constrained Graph Attention Network&rdquo; (C-GAT).</p>
</a>
<h2 id="implementation">Implementation</h2>
<p>I wasn&rsquo;t able to find an implementation of the Constrained Graph Attention Network for my own purposes, so I&rsquo;ve implemented the algorithm myself in 
<a href="https://www.dgl.ai/" target="_blank" rel="noopener">Deep Graph Library</a> (DGL) &ndash; the source code for this convolutional layer can be found 
<a href="https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/conv/cgatconv.py" target="_blank" rel="noopener">here</a>.  This implementation makes use of the original DGL <code>GATConv</code> layer structure, with modifications made for the constraints and aggregations.  Specifically, the API for <code>CGATConv</code> has the following modifications:</p>
<pre><code class="language-python">
CGATCONV(in_feats, 
         out_feats, 
         num_heads, 
         feat_drop=0., 
         graph_margin=0.1, # graph structure loss slack variable
         class_margin=0.1, # class boundary loss slack variable
         top_k=3, # number of messages to aggregate over
         negative_slope=0.2,
         residual=False,
         activation=None,
         allow_zero_in_degree=False)
</code></pre>
<p>Of note is the fact that the <code>attn_drop</code> parameter has been substituted by the <code>top_k</code> parameter in order to mitigate oversmoothing, and the two slack variables $\zeta_{g}$ and $\zeta_{b}$ are provided as <code>graph_margin</code> and <code>class_margin</code>.</p>
<p>With regards to the loss functions, the authors compute all-pairs differences between all edges incident on a source node, instead of summing over the positive / negative sample attentions ($\mathcal{L_{g}}$) and same / different label attentions ($\mathcal{L_{b}}$) and then differencing these summations.  In this way, the C-GAT model anchors the loss values to specific nodes, promoting learning of more generalizable attention weights.  The graph structure loss function $\mathcal{L_{g}}$ is implemented with the <code>graph_loss</code> reduction function below:</p>
<pre><code class="language-python">def graph_loss(nodes):
            
    &quot;&quot;&quot;
    Loss function on graph structure.
    
    Enforces high attention to adjacent nodes and 
    lower attention to distant nodes via negative sampling.
    &quot;&quot;&quot;

    msg = nodes.mailbox['m']

    pw = msg[:, :, :, 0, :].unsqueeze(1)
    nw = msg[:, :, :, 1, :].unsqueeze(2)

    loss = (nw + self._graph_margin - pw).clamp(0)
    loss = loss.sum(1).sum(1).squeeze()

    return {'graph_loss': loss}
.
.
.
graph.srcdata.update({'ft': feat_src, 'el': el})
graph.dstdata.update({'er': er})
graph.apply_edges(fn.u_add_v('el', 'er', 'e'))
e = self.leaky_relu(graph.edata.pop('e'))

# construct the negative graph by shuffling edges
# does not assume a single graph or blocked graphs
# see cgatconv.py for ```construct_negative_graph``` function
neg_graph = [construct_negative_graph(i, k=1) for i in dgl.unbatch(graph)]
neg_graph = dgl.batch(neg_graph)

neg_graph.srcdata.update({'ft': feat_src, 'el': el})
neg_graph.dstdata.update({'er': er})
neg_graph.apply_edges(fn.u_add_v('el', 'er', 'e'))
ne = self.leaky_relu(neg_graph.edata.pop('e'))

combined = th.stack([e, ne]).transpose(0, 1).transpose(1, 2)
graph.edata['combined'] = combined
graph.update_all(fn.copy_e('combined', 'm'), graph_loss)

# compute graph structured loss
Lg = graph.ndata['graph_loss'].sum() / (graph.num_nodes() * self._num_heads)
</code></pre>
<p>Similarly, the class boundary loss function $\mathcal{L_{b}}$ is implemented with the following message and reduce functions:</p>
<pre><code class="language-python">def adjacency_message(edges):
            
    &quot;&quot;&quot;
    Compute binary message on edges.

    Compares whether source and destination nodes
    have the same or different labels.
    &quot;&quot;&quot;

    l_src = edges.src['l']
    l_dst = edges.dst['l']

    if l_src.ndim &gt; 1:
        adj = th.all(l_src == l_dst, dim=1)
    else:
        adj = (l_src == l_dst)

    return {'adj': adj.detach()}

def class_loss(nodes):
    
    &quot;&quot;&quot;
    Loss function on class boundaries.
    
    Enforces high attention to adjacent nodes with the same label
    and lower attention to adjacent nodes with different labels.
    &quot;&quot;&quot;

    m = nodes.mailbox['m']

    w = m[:, :, :-1]
    adj = m[:, :, -1].unsqueeze(-1).bool()

    same_class = w.masked_fill(adj == 0, np.nan).unsqueeze(2)
    diff_class = w.masked_fill(adj == 1, np.nan).unsqueeze(1)

    difference = (diff_class + self._class_margin - same_class).clamp(0)
    loss = th.nansum(th.nansum(difference, 1), 1)

    return {'boundary_loss': loss}
.
.
.
graph.ndata['l'] = label
graph.apply_edges(adjacency_message)
adj = graph.edata.pop('adj').float()

combined = th.cat([e.squeeze(), adj.unsqueeze(-1)], dim=1)
graph.edata['combined'] = combined
graph.update_all(fn.copy_e('combined', 'm'), class_loss)
Lb = graph.ndata['boundary_loss'].sum() / (graph.num_nodes() * self._num_heads)
</code></pre>
<p>And finally, the constrained message aggregation is implemented using the following reduction function:</p>
<pre><code class="language-python">def topk_reduce_func(nodes):
    
    `&quot;&quot;&quot;
    Aggregate attention-weighted messages over the top-K 
    attention-valued destination nodes
    &quot;&quot;&quot;

    K = self._top_k

    m = nodes.mailbox['m']
    [m,_] = th.sort(m, dim=1, descending=True)
    m = m[:,:K,:,:].sum(1)

    return {'ft': m}
.
.
.
# message passing
if self._top_k is not None:
    graph.update_all(fn.u_mul_e('ft', 'a', 'm'), 
                    topk_reduce_func)
else:
    graph.update_all(fn.u_mul_e('ft', 'a', 'm'),
                    fn.sum('m', 'ft'))
</code></pre>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/ai/">AI</a>
  
  <a class="badge badge-light" href="/tag/pytorch/">pytorch</a>
  
  <a class="badge badge-light" href="/tag/graph-convolution-networks/">graph convolution networks</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/constrained-gat/&amp;text=Constrained%20Graph%20Attention%20Networks" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/constrained-gat/&amp;t=Constrained%20Graph%20Attention%20Networks" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Constrained%20Graph%20Attention%20Networks&amp;body=/post/constrained-gat/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/constrained-gat/&amp;title=Constrained%20Graph%20Attention%20Networks" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/avatar_hu1d33f7c1c3956865c98823fdaf357458_38773_270x270_fill_q90_lanczos_center.jpg" alt="">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/"></a></h5>
        <h6 class="card-subtitle">Data Scientist</h6>
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:keschenb@uw.edu" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/keschh" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/kristianeschenburg" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/kristianeschenburg/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  












  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/gaussian-kernel-convolution/">Gaussian Graph Convolutional Networks</a></li>
      
      <li><a href="/post/structured-cross-entropy/">Cross-Entropy With Structure</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
