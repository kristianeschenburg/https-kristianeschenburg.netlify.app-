<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | A Rambling On</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 15 Jun 2023 02:14:14 -0700</lastBuildDate>
    <image>
      <url>/img/Bayes.jpg</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>CI/CD Part 4: Container Registries</title>
      <link>/post/cicd-4-cr/</link>
      <pubDate>Thu, 15 Jun 2023 02:14:14 -0700</pubDate>
      <guid>/post/cicd-4-cr/</guid>
      <description>&lt;p&gt;This is the last post in a mini-series on 
&lt;a href=&#34;/post/cicd-1-config/&#34;&gt;designing Gitlab CI/CD pipelines&lt;/a&gt;.  We&amp;rsquo;ve discussed the basic anatomy of a &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file, how to set up authentication tokens and files for building and pushing packages to a registry, and designing a Dockerfile for building images from a package in the context of a CI/CD pipeline.  In this post, I&amp;rsquo;m going to show how to push your package to a remote AWS Elastic Container Registry (ECR).&lt;/p&gt;
&lt;p&gt;The structure of this CI/CD job will be analogous to the CI/CD job defined at the end of our first 
&lt;a href=&#34;/post/cicd-1-config/&#34;&gt;post&lt;/a&gt;, where we pushed an image to the Gitlab Container Registry.&lt;/p&gt;
&lt;h4 id=&#34;setting-up-aws-variables&#34;&gt;Setting up AWS variables&lt;/h4&gt;
&lt;p&gt;To build images, tag them, and push them to the remote AWS ECR, I used the definition of a CI/CD job below.  In addition to pre-defined variables that are set internally by Gitlab, we can also manually pre-define variables.  In this case, I&amp;rsquo;ve set a few that allow me to interact with AWS via the command line:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AWS_DEFAULT_REGION&lt;/code&gt;: self-explanatory&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ECR_REPO_LAMBDA&lt;/code&gt;: &lt;code&gt;${AWS_ACCOUNT_ID}&lt;/code&gt;.dkr.ecr.&lt;code&gt;${AWS_DEFAULT_REGION}&lt;/code&gt;.amazonaws.com/&lt;code&gt;${YOUR_ECR_REPO_NAME}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AWS_ACCOUNT_ID&lt;/code&gt;: AWS account ID&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AWS_ACCESS_KEY&lt;/code&gt;: this is the information contained in the downloaded *.pem file&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;: this is the information contained in the downloaded *.pem file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To set variables that are accessible by CI/CD jobs, go to your &lt;strong&gt;Project/Group &amp;gt; Settings &amp;gt; CI/CD &amp;gt; Variables &amp;gt; Expand&lt;/strong&gt; and define the variables of interest:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./CICD.VariableTab.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;./CICD.Variables.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you define these variables at the Gitlab Group level, they will be propagated down to the project level, so long as the Project falls under the Group scope.&lt;/p&gt;
&lt;h4 id=&#34;pushing-to-aws-ecr-via-cicd-job&#34;&gt;Pushing to AWS ECR via CI/CD job&lt;/h4&gt;
&lt;p&gt;Below, we define the actual CI/CD job.   There were two aspects here that I needed to solve.  First, I needed access to a Docker-in-Docker build image e.g. an image that had Docker installed.  And second, this image also needed to have the AWS CLI tool installed.  To that end, I used the &lt;code&gt;bentolor/docker-dind-awscli&lt;/code&gt; 
&lt;a href=&#34;https://github.com/bentolor/docker-dind-awscli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;image&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;build-image-ecr:
  stage: deploy 
  image: bentolor/docker-dind-awscli
  services:
    - docker:dind
  variables:
    # convenience variable indicating name of the image with respect to the ECR repo and unique tag ID
    IMAGE_TAG: $ECR_REPO_LAMBDA:$CI_COMMIT_SHORT_SHA

  before_script:
    - docker info
    # authenticate docker with your AWS ECR account
    - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com
  # will push Docker image to AWS ECR
  script:
    # build the docker imagae
    - docker build --compress -t ${IMAGE_TAG} .
    # tag the image with a unique name
    - docker tag ${IMAGE_TAG} $ECR_REPO_LAMBDA:latest
    # push the image to the ECR
    - docker push ${IMAGE_TAG}

  # here, we only build and push the image if this is a merge event into the &amp;quot;main&amp;quot; branch
  rules:
    - if: $CI_PIPELINE_SOURCE == &#39;merge_request_event&#39; &amp;amp;&amp;amp; $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == &amp;quot;main&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And voila!  You have now pushed your built image to a remote container registry!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CI/CD Part 3: Building containers with Docker</title>
      <link>/post/cicd-3-docker/</link>
      <pubDate>Tue, 30 May 2023 02:14:14 -0700</pubDate>
      <guid>/post/cicd-3-docker/</guid>
      <description>&lt;p&gt;This is the third post in a mini-series on 
&lt;a href=&#34;/post/cicd-1-config/&#34;&gt;designing Gitlab CI/CD pipelines&lt;/a&gt;.  In the last 
&lt;a href=&#34;/post/cicd-2-packages/&#34;&gt;post&lt;/a&gt;, we discussed setting up your &lt;code&gt;.pypirc&lt;/code&gt; and &lt;code&gt;.netrc&lt;/code&gt; files in the context of a Gitlab CI/CD pipeline to enable building and pushing packages to a package registry, as well as for installing code from a private registry.  In this post, we&amp;rsquo;ll discuss putting together a Dockerfile for building containers in the context of a Gitlab CI/CD pipeline.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m using what&amp;rsquo;s called a &amp;ldquo;multi-stage&amp;rdquo; build.  This is exactly what it sounds like &amp;ndash; it breaks the process of building a Docker image into multiple stages.  In doing so, we often have the benefit of a final image that is smaller than a single-stage build, because we only include the artifacts needed to run our containerized application.&lt;/p&gt;
&lt;p&gt;Similarly, we can leverage multi-stage Docker builds to minimize duplicated code in Dockerfiles.  For example, let&amp;rsquo;s say we have a scenario where we want to build an image for a &lt;code&gt;Production&lt;/code&gt; environment as well as a &lt;code&gt;Test&lt;/code&gt; environment.  The &lt;code&gt;Test&lt;/code&gt; environment might include some additional dependencies, scripts, exports, etc. that the Production environment doesn&amp;rsquo;t.  Instead of creating two Dockerfiles, one for each environment, we can define a single-stage that encompasses the overlapping parts of both the &lt;code&gt;Production&lt;/code&gt; and &lt;code&gt;Test&lt;/code&gt; images, and then define the extra stuff in a separate stage to build the &lt;code&gt;Test&lt;/code&gt; image.&lt;/p&gt;
&lt;p&gt;In the example below, we have a three-stage Docker build, with stage names:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;base&lt;/code&gt;: sets up some basic environment variables&lt;/li&gt;
&lt;li&gt;&lt;code&gt;python-deps&lt;/code&gt;: installs your package and creates a virtual environment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;runtime&lt;/code&gt;: the actual application you want to run, with only the necessary files for running it&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;create-a-base-image&#34;&gt;Create a base image&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# set base image
# bigger base images yield slower image load times, and have more security vulnerabilities
FROM python:3.9-slim as base

# install virtual environment in ${project_dir}/.venv
ENV PIPENV_VENV_IN_PROJECT 1
# dont write .pyc files
ENV PYTHONDONTWRITEBYTECODE 1
# get some more information about faults when building images
ENV PYTHONFAULTHANDLER 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;install-package-dependencies&#34;&gt;Install package dependencies&lt;/h4&gt;
&lt;p&gt;If you were building your Docker image locally, you&amp;rsquo;d have access to any authentication tokens or SSH keys necessary to pull from remote or private repositories.  However, Docker is naive to these variables &amp;ndash; we have to explicitly provide them at build time. Within the Dockerfile, we define three environment variables using the &lt;code&gt;ARG&lt;/code&gt; keyword:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;CI_DEPLOY_USER&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CI_DEPLOY_PASSWORD&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CI_JOB_TOKEN&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you think that these variables look familiar, you&amp;rsquo;re right.  They&amp;rsquo;re the same pre-defined variables that exist in the context of a Gitlab CI/CD pipeline that act as authentication tokens for a &lt;code&gt;.pypric&lt;/code&gt; file and &lt;code&gt;.netrc&lt;/code&gt; &amp;ndash; they&amp;rsquo;re utilized by the &lt;code&gt;setup_tokens.sh&lt;/code&gt; script to setup the &lt;code&gt;.pypirc&lt;/code&gt; and &lt;code&gt;.netrc&lt;/code&gt; files &lt;em&gt;within&lt;/em&gt; the Docker image.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# image for installing dependencies
# we only need .venv and app `runtime` image, not all other bloat
FROM base AS python-deps

####################################
# ----------------------------------
ARG CI_DEPLOY_USER
ARG CI_DEPLOY_PASSWORD
ARG CI_JOB_TOKEN
# ----------------------------------
####################################
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These variable are available outside of the Docker image, but not within the image itself, so we need to &amp;ldquo;show&amp;rdquo; them to Docker at build time via the &lt;code&gt;--build-arg&lt;/code&gt; flag:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build --build-arg CI_DEPLOY_USER=$CI_DEPLOY_URDER \
             --build-arg CI_DEPLOY_PASSWORD=$CI_DEPLOY_PASSWORD \
             --build-arg CI_JOB_TOKEN=$CI_JOB_TOKEN \
             ...
             ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now they are contained within the Docker image and can be provided to the &lt;code&gt;setup_tokens.sh&lt;/code&gt; script, which then allows us to pull packages down from our remote package registry.  We also no longer need the SSH keys, since we&amp;rsquo;re authenticating through Gitlab itself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# install pipenv in `python-deps` image
RUN python3 -m pip install pipenv
RUN apt-get update \
    &amp;amp;&amp;amp; apt-get install --yes --no-install-recommends gcc g++ libffi-dev

# Dependency installation looks a little different for local packages
WORKDIR /home/app


# copy files to `python-deps` image
COPY setup.py setup_tokens.sh Pipfile Pipfile.lock ./
# copy over application-specific code that you want to install
# this is unique to my specific project -- use your own directories here
COPY templateci/ templateci/

# run setup_tokens script to setup .pypirc and .netrc within image
RUN chmod +x ./setup_tokens.sh &amp;amp;&amp;amp; ./setup_tokens.sh

# authentication tokens are now available to pipenv
RUN python3 -m pipenv install --deploy --dev

# get rid of unnecessary libraries after install
RUN apt-get autoremove --yes gcc g++ libffi-dev \
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;create-your-final-runtime-image&#34;&gt;Create your final runtime image&lt;/h4&gt;
&lt;p&gt;Above, we created the virtual environment that allows our application to run.  As such, we no longer need the raw source code or any other random files that were contained in the original project directory that might have been needed to build the virtual environment.  Now we create a stage called &lt;code&gt;runtime&lt;/code&gt; in which we copy over the generated virtual environment from the previous stage&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# image for running the application
FROM base AS runtime

# Copy virtual environment from `python-deps` image to `runtime` image
COPY --from=python-deps /home/app/.venv /.venv
# add virtual environment to PATH
ENV PATH=&amp;quot;/.venv/bin:$PATH&amp;quot;

# Create new user -- app will run as new user
RUN useradd --create-home -u 1099 user
WORKDIR /home/user/app
USER user

COPY . .

CMD [&amp;quot;python3&amp;quot;, &amp;quot;-m&amp;quot;, &amp;quot;pytest&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the actual image that get&amp;rsquo;s run when we call&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run ${IMAGE_NAME}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>CI/CD Part 2: Building and pushing packages</title>
      <link>/post/cicd-2-packages/</link>
      <pubDate>Fri, 12 May 2023 02:14:14 -0700</pubDate>
      <guid>/post/cicd-2-packages/</guid>
      <description>&lt;p&gt;This is the second post in a mini-series on 
&lt;a href=&#34;/post/cicd-1-config/&#34;&gt;designing Gitlab CI/CD pipelines&lt;/a&gt;.  In order to build packages and push them to a remote package registry, we use the &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;twine&lt;/code&gt; packages.  &lt;code&gt;build&lt;/code&gt; generates a package, and &lt;code&gt;twine&lt;/code&gt; pushes this package to a registry (or &amp;ldquo;index&amp;rdquo;).  &lt;code&gt;twine&lt;/code&gt; requires access to authentication usernames, passwords, and a registry URL in order to do so.  &lt;code&gt;twine&lt;/code&gt; can access these tokens from a &lt;code&gt;.pypirc&lt;/code&gt; file &amp;ndash; the tokens are generated by the registry, and ensure that the submitting user has permissions to perform a certain action.&lt;/p&gt;
&lt;p&gt;Other processes, such as pulling or pushing code from a remote repository, often require additional usernames and passwords.  In order to alleviate the need to consistently provide these variables at request time, we can save them in a &lt;code&gt;.netrc&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;These are straightforward to set up locally.  But we also need to set these up to ensure a properly functional CI/CD workflow.  I&amp;rsquo;ve put together a basic script, called &lt;code&gt;setup_tokens.sh&lt;/code&gt; that does just that:&lt;/p&gt;
&lt;h4 id=&#34;setup_tokenssh&#34;&gt;setup_tokens.sh&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

# generate .pypirc file
echo &amp;quot;[distutils]
index-servers =
    personal

[personal]
repository = https://gitlab.com/api/v4/projects/$PACKAGE_REGISTRY_ID/packages/pypi
username = $CI_DEPLOY_USER
password = $CI_DEPLOY_PASSWORD&amp;quot; &amp;gt; ~/.pypirc

# generate .netrc file
echo &amp;quot;machine gitlab.com
login gitlab-ci-token
password $CI_JOB_TOKEN&amp;quot; &amp;gt; ~/.netrc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;.pypirc&lt;/code&gt; refers to your Project Registry via a previously generated authentication token and password, and allows your to build and upload Python packages to that registry.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;.netrc&lt;/code&gt; file enables you to pull private packages from that same registry.  In the context of our work, we&amp;rsquo;ll want to build and push packages to the registry first so that they are available for pulling.  For example, in the &lt;code&gt;Pipfile&lt;/code&gt; for this template project, we have the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[[source]]
url = &amp;quot;https://pypi.org/simple&amp;quot;
verify_ssl = true
name = &amp;quot;pypi&amp;quot;

[[source]]
url = &amp;quot;https://${CI_DEPLOY_USER}:${CI_DEPLOY_PASSWORD}@gitlab.com/api/v4/projects/${$PACKAGE_REGISTRY_ID}/packages/pypi/simple&amp;quot;
verify_ssl = true
name = &amp;quot;personal&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see the same user authentication happening, along with the reference to the Package Registry ID variable.  For local installation of your package, and in order to make sure that your &lt;code&gt;Pipfile&lt;/code&gt; and &lt;code&gt;Pipfile.lock&lt;/code&gt; are in sync, you&amp;rsquo;ll need to define the following local environment variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CI_DEPLOY_USER&lt;/code&gt;: generated user token&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CI_DEPLOY_PASSWORD&lt;/code&gt;: generated token password&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PACKAGE_REGISTRY_ID&lt;/code&gt;: the ID of the repository that you created that will store your packages&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Building a package locally is straightforward, but doing so within a Gitlab CI/CD pipeline is a little more complicated.  But, we can imagine adding this type of task to a CI/CD pipeline, and conditioning it on a merge request (or something of that kind).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve called this job &lt;code&gt;build-package&lt;/code&gt; and it belongs to a stage also called &lt;code&gt;build-package&lt;/code&gt;.  We first set up the &lt;code&gt;.pypirc&lt;/code&gt; and &lt;code&gt;.netrc&lt;/code&gt; files in image running the job, and then install the &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;twine&lt;/code&gt; libraries in the &lt;code&gt;before_script&lt;/code&gt; attribute.  Then, using the &lt;code&gt;script&lt;/code&gt; attribute, we build our package, and push it to our package registry (we&amp;rsquo;ve defined the registry in our &lt;code&gt;.pypirc&lt;/code&gt; file &amp;ndash; here, it&amp;rsquo;s referred to as the &amp;ldquo;personal&amp;rdquo; registry.)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;image: python:3.9-slim
variables:
  PACKAGE_REGISTRY_NAME: &amp;quot;personal&amp;quot;

stages:
  - build-package

#### BUILDING PACKAGE AND PUSHING TO GITLAB PACKAGE REGISTRY
build-package:
  stage: build-package
  before_script:
    - chmod +x ./setup_tokens.sh; ./setup_tokens.sh
    - apt-get update
    - apt-get install --yes --no-install-recommends gcc g++ libffi-dev
    - python3 -m pip install build twine
  script:
    - python3 -m build
    - python3 -m twine upload --repository ${PACKAGE_REGISTRY_NAME} dist/* --verbose
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>CI/CD Part 1: Gitlab Pipelines</title>
      <link>/post/cicd-1-config/</link>
      <pubDate>Tue, 02 May 2023 02:14:14 -0700</pubDate>
      <guid>/post/cicd-1-config/</guid>
      <description>&lt;p&gt;I recently developed a template workflow to help our team adopt a CI/CD-based development strategy.  Many of our web applications and tools were based on simple repository structures.  With growing datasets and ever-increasing use by outside teams, we found ourselves needing to add new features more frequently to many of these tools and believed that continuous integration and deployment could help us not just develop more quickly, but also more intelligently.  Since we use Gitlab to store our code, we decided to use the Gitlab CI/CD tools.&lt;/p&gt;
&lt;p&gt;Documentation on much of this process was scattered and/or sparse, so I decided to put what I learned and implemented into a more coherent set of notes.  The current post relates to a basic setup of the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file and using Gitlab CI/CD.  The next few posts are about what I learned in the process of setting this up, where I&amp;rsquo;ll discuss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;/post/cicd-2-packages/&#34;&gt;Setting&lt;/a&gt; up authentication for pushing packages to a package registry and for pulling down private repositories&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;/post/cicd-3-docker/&#34;&gt;Designing&lt;/a&gt; Dockerfiles for building images using CI/CD&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;/post/cicd-4-cr/&#34;&gt;Pushing&lt;/a&gt; images to a container registry&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;p&gt;I did all of my testing using my personal Gitlab account.  To separate things out, I created a new Project called &amp;ldquo;Package Registry”, as well as a test repository that was used for building a local Python project called &amp;ldquo;TemplateCI&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;Package Registry&amp;rdquo; Project serves as just that &amp;ndash; an all-inclusive location for any software packages your CI/CD pipelines build.  You can find the built packages by clicking &lt;strong&gt;${Project Name} &amp;gt; Deploy &amp;gt; Package Registry&lt;/strong&gt;.  Every &amp;ldquo;Project&amp;rdquo; in Gitlab has the ability to store packages in its own registry, but I felt it cleaner to store everything in one repo.  Similarly, the actual code that I&amp;rsquo;ll be packaging will be stored in the &amp;ldquo;TemplateCI&amp;rdquo; repo.&lt;/p&gt;
&lt;h2 id=&#34;basic-jobs&#34;&gt;Basic jobs&lt;/h2&gt;
&lt;p&gt;The basis of a Gitlab CI/CD pipeline is the &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; file, which is composed of a set of explicitly-defined and temporally-ordered “stages”.  A stage is composed of a set of “jobs”.  Jobs are the workhorses of the CI/CD pipeline, and define explicit tasks that a CI/CD pipeline runs.  By default, all jobs from one stage run in parallel, unless specified otherwise (using the &lt;code&gt;needs&lt;/code&gt; keyword as an attribute of a job induces a temporal directed acyclic graph – jobs can be made “dependent” on the successful completion of other jobs within a stage).  In this example, I’ve defined three stages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;run-unit-tests&lt;/li&gt;
&lt;li&gt;build-package&lt;/li&gt;
&lt;li&gt;build-image&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Any job associated with the &lt;code&gt;run-unit-tests&lt;/code&gt; stage will run to completion (or failure) PRIOR TO ANY job in the stages &lt;code&gt;build-package&lt;/code&gt; and &lt;code&gt;build-image&lt;/code&gt; starting.  If all jobs in the &lt;code&gt;run-unit-tests&lt;/code&gt; stage complete successfully, then the next stage (&lt;code&gt;build-package&lt;/code&gt;) will begin.  We define individual jobs, and give them a stage attribute.  Stages define the rough ordering of jobs.  Each job runs in the context of an “image” or environment.  We can set a global &lt;code&gt;image&lt;/code&gt; and define stages as&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# global CI/CD image
image: python:3.9-slim

# stages of this example pipeline
stages:
  - run-unit-tests
  - build-package
  - build-image

 variables:
    LC_ALL: C.UTF-8
    LANG: C.UTF-8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or define the image as an attribute of a job.  Gitlab CI/CD by default uses Docker images in which to run jobs.  Setting the image is analogous to using the &lt;code&gt;FROM&lt;/code&gt; command in a Dockerfile.  We&amp;rsquo;ve also set some global variables, here the &lt;code&gt;LC_ALL&lt;/code&gt; and &lt;code&gt;LANG&lt;/code&gt; variables.&lt;/p&gt;
&lt;p&gt;To “run” Gitlab pipelines for the purpose of CI/CD, we use “runners”, which are build instances installed on a server.  Gitlab offers “shared” runners (use of these is free if you use 
&lt;a href=&#34;https://www.gitlab.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.gitlab.com&lt;/a&gt;, but you need to register a credit card to prevent abuse of Gitlab resources).  You can also register your own device(s) to act as a Gitlab runner.&lt;/p&gt;
&lt;p&gt;Below are examples of two jobs in the &lt;code&gt;run-unit-tests&lt;/code&gt; stage.  These two jobs are effectively the same code, apart from the unique unit tests that they run.  However, we&amp;rsquo;ve made the job &lt;code&gt;unit-tests-2&lt;/code&gt; dependent on the output of the job &lt;code&gt;unit-tests-1&lt;/code&gt; (see the &lt;code&gt;needs&lt;/code&gt; keyword of &lt;code&gt;unit-tests-2&lt;/code&gt;) .  Both jobs use the global &lt;code&gt;python:3.9-slim&lt;/code&gt; image.  We can run some “setup”  stuff (&lt;code&gt;before_script&lt;/code&gt;), run an actual script (&lt;code&gt;script&lt;/code&gt;), and run clean up (&lt;code&gt;after_script&lt;/code&gt;, not shown) &amp;ndash; these delineations (before, during, after) are for organizational purposes, and not due to any explicit functional differences in the delineations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;### Ignore the script called &amp;quot;setup_tokens.sh&amp;quot; for now -- we&#39;ll discuss this in another post. ###


# example job #1
unit-tests-1:
  stage: run-unit-tests
  before_script:
    - chmod +x ./setup_tokens.sh; ./setup_tokens.sh
    - python3 -m pip install pipenv
    - apt-get update
    - apt-get install --yes --no-install-recommends gcc g++ libffi-dev
    - python3 -m pipenv install --deploy --dev
  # run first set of unit tests
  script:
    - python3 -m pipenv run pytest -k &#39;test_examples1.py&#39;
 

# example job #2
unit-tests-2:
  stage: run-unit-tests
  before_script:
    - chmod +x ./setup_tokens.sh; ./setup_tokens.sh
    - python3 -m pip install pipenv
    - apt-get update
    - apt-get install --yes --no-install-recommends gcc g++ libffi-dev
    - python3 -m pipenv install --deploy --dev
  # run second set of unit tests
  script:
    - python3 -m pipenv run pytest -k &#39;test_examples2.py&#39;
  # wait for job 1 to finish
  needs: [unit-tests-1]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;conditional-pipeline-jobs&#34;&gt;Conditional pipeline jobs&lt;/h2&gt;
&lt;p&gt;The above jobs are relatively simple and will run every time you push a repository to Gitlab.  However, sometimes, we might only want to run a job if certain conditions are met.  For example, we might only want to build a package from the &lt;code&gt;main&lt;/code&gt; branch, or only after a merge request is made.  To this end, we can add “rules” to a job that restrict when it is actually run.&lt;/p&gt;
&lt;p&gt;Below is a more complicated job example.  The overarching goal of this job is to build a Docker image from a local Python project and push the image to the Gitlab Container Registry.  There’s a lot going on here, so I’ll break it up into pieces, but here is the whole job:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Conditional job
# Building a docker image and pushing this container to a container registry
# Link to main image: https://github.com/bentolor/docker-dind-awscli

# Conditions:
# --- merge request events
# --- target branch of merge request is &amp;quot;main&amp;quot;

# job name
build-image-glcr:
  
  # stage of pipeline
  stage: build-image
  # image that job is based on
  image: docker:20.10.16
  # sub-services of job
  services:
    - docker:20.10.16-dind
  # variables available to the job
  variables:
    DOCKER_TLS_CERTDIR: &amp;quot;/certs&amp;quot;
    IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG
    
  # some setup scripts -- here, just making sure docker is available
  before_script:
    - docker info

  # meat of the job -- authentication, building, run, push
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build 
      --build-arg CI_DEPLOY_USER=$CI_DEPLOY_USER 
      --build-arg CI_DEPLOY_PASSWORD=$CI_DEPLOY_PASSWORD 
      --build-arg CI_JOB_TOKEN=$CI_JOB_TOKEN
      -t $IMAGE_TAG .
    - docker run $IMAGE_TAG
    - docker push $IMAGE_TAG

  # job conditions
  rules:
    - if: $CI_PIPELINE_SOURCE == &#39;merge_request_event&#39; &amp;amp;&amp;amp; $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == &amp;quot;main&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This job is associated with a new stage called &lt;code&gt;build-image&lt;/code&gt; that will run as the last stage of this example CI/CD pipeline.  Without going into the specifics of the 
&lt;a href=&#34;/post/cicd-3-docker/&#34;&gt;Dockerfile&lt;/a&gt; just yet, this stage builds and pushes a Docker image to a remote repository.  We defined the job image as &lt;code&gt;docker:20.10.16&lt;/code&gt; and an additional “service” attribute as &lt;code&gt;docker:20.10.16-dind&lt;/code&gt; where “dind” means “Docker-in-Docker”.  The Docker-in-Docker feature allows an image to run Docker itself (pretty meta, huh).  The idea here is to instantiate a job from a specific Docker container (“image”) which itself has Docker installed (“dind”), which will allow the docker:20.10.16 image to build &lt;em&gt;another&lt;/em&gt; Docker container.  We’ve also defined some variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DOCKER_TLS_CERTDIR&lt;/code&gt;: needed to allow the larger scale image to communicate with a service (honestly, I don’t quite understand this and documentation on CI/CD &amp;ldquo;services&amp;rdquo; is sparse)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IMAGE_TAG&lt;/code&gt;: refers to the container destination and the image “name” &amp;ndash; this just makes our lives easier by turning into a variable what would otherwise be a really long string&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;#### Build Docker image and push to Gitlab Container Registry
build-image-glcr:
  stage: build-image
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  variables:
    DOCKER_TLS_CERTDIR: &amp;quot;/certs&amp;quot;
    IMAGE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up, we have all the script stuff.  You’ll probably recognize most of the &lt;code&gt;docker ${command}&lt;/code&gt; commands.  We first “authenticate” the current job with the Gitlab container registry (apparently there are a variety of ways to “authenticate” with Docker using one set of variables or another – the way below is the way I was able to get working, but there are 
&lt;a href=&#34;https://stackoverflow.com/questions/61251622/how-to-authenticate-to-gitlabs-container-registry-before-building-a-docker-imag&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other solutions&lt;/a&gt;).  We then build the Docker image, run the image, and push the image to the container registry.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;script:
  - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  - docker build 
    --build-arg CI_DEPLOY_USER=$CI_DEPLOY_USER 
    --build-arg CI_DEPLOY_PASSWORD=$CI_DEPLOY_PASSWORD 
    --build-arg CI_JOB_TOKEN=$CI_JOB_TOKEN
    -t $IMAGE_TAG .
  - docker run $IMAGE_TAG
  - docker push $IMAGE_TAG
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll notice a bunch of variables that I didn&amp;rsquo;t explicitly define anywhere:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CI_REGISTRY_USER&lt;/code&gt;: username for project (I think we can also use &lt;code&gt;CI_DEPLOY_USER&lt;/code&gt;, though it looks like there might be some things to figure out with branch / variable protections / non-protections)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CI_REGISTRY_PASSWORD&lt;/code&gt;: defaults to &lt;code&gt;CI_JOB_TOKEN&lt;/code&gt; value (this value is ephemeral e.g. valid only for one job at a time I think?  I think we can also use the &lt;code&gt;CI_DEPLOY_PASSWORD&lt;/code&gt; for a longer-lived alternative, though it looks like there might be some things to figure out with branch / variable protections / non-protections)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CI_REGISTRY&lt;/code&gt;: defaults to &lt;code&gt;https://gitlab.com/${group}/${project-name}/container_registry&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CI_DEPLOY_USER&lt;/code&gt;: generated user token&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CI_DEPLOY_PASSWORD&lt;/code&gt;: generated token password&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CI_JOB_TOKEN&lt;/code&gt;: see documentation 
&lt;a href=&#34;https://docs.gitlab.com/ee/ci/jobs/ci_job_token.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are all 
&lt;a href=&#34;https://docs.gitlab.com/ee/ci/variables/predefined_variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“predefined” variables&lt;/a&gt;, meaning they already exist in the Gitlab CI/CD context as part of having a 
&lt;a href=&#34;https://www.gitlab.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.gitlab.com&lt;/a&gt; account, without you explicitly defining them.  However, I did run into some issues using &lt;code&gt;CI_DEPLOY_USER&lt;/code&gt; and &lt;code&gt;CI_DEPLOY_PASSWORD&lt;/code&gt;.  In addition to having predefined variables provided by Gitlab CI/CD, we can also 
&lt;a href=&#34;./docs/SettingEnvVariables.md&#34;&gt;&lt;em&gt;manually&lt;/em&gt; predefine variables&lt;/a&gt; for a whole Gitlab Project or for a whole Group.  Go the page for your &lt;strong&gt;Project/Group &amp;gt; Settings &amp;gt; CI/CD &amp;gt; Variables &amp;gt; Expand&lt;/strong&gt;.  For example, I defined the &lt;code&gt;CI_DEPLOY_USER&lt;/code&gt; and &lt;code&gt;CI_DEPLOY_PASSWORD&lt;/code&gt; variables for my Group.  These variables are the Group-level authentication tokens and are now made accessible to all Gitlab CI/CD jobs running under this Group.  Additionally, although not shown here (because it&amp;rsquo;s used within the &lt;code&gt;setup_tokens.sh&lt;/code&gt; script), we&amp;rsquo;ve also defined an environment variable called &lt;code&gt;PACKAGE_REGISTRY_ID&lt;/code&gt; that tells the CI/CD pipeline where to build and push packages.&lt;/p&gt;
&lt;p&gt;And finally, what we’ve been waiting for, conditional pipeline jobs:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;rules:
  - if: $CI_PIPELINE_SOURCE == &#39;merge_request_event&#39; &amp;amp;&amp;amp; $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == &amp;quot;main&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As part of this job, I&amp;rsquo;ve defined a rule that indicates that this job should &lt;strong&gt;only&lt;/strong&gt; be run 1) upon a merge request event (i.e. if you click “Create Merge Request” in the console), and 2) if the name of the target branch of that merge request event is &lt;code&gt;main&lt;/code&gt;.  For example, if I create a new branch called &lt;code&gt;dev&lt;/code&gt; off of &lt;code&gt;main&lt;/code&gt; and create a merge request in the Gitlab console, this job will run.  However, it’s important to note that, if you push a commit to the &lt;code&gt;dev&lt;/code&gt; branch &lt;em&gt;after&lt;/em&gt; creating the merge request, this job will &lt;em&gt;still&lt;/em&gt; run, even if you havn’t created a &lt;em&gt;new&lt;/em&gt; merge request e.g. &lt;code&gt;CI_PIPELINE_SOURCE&lt;/code&gt; == &amp;ldquo;merge_request_event&amp;rdquo; will always default to &lt;code&gt;TRUE&lt;/code&gt; after the first merge request event, as long as the source branch is still actively being developed.  Additionally, the job itself for this &lt;strong&gt;will run the source branch code, not the target branch code&lt;/strong&gt;.  For Gitlab Premium users, there is an additional criteria called a “merged_result_event”, which would run the target branch (&lt;code&gt;main&lt;/code&gt;) code after merging the source branch (&lt;code&gt;dev&lt;/code&gt;) into the target branch.&lt;/p&gt;
&lt;p&gt;Some of the (many) external links I used in this process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.gitlab.com/ee/ci/variables/predefined_variables.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Predefined&lt;/a&gt; CI/CD variables&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://gitlab.com/gitlab-org/gitlab/-/issues/214014&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adding&lt;/a&gt; variables to the Gitlab CI/CD context&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://gitlab.com/gitlab-org/gitlab/-/issues/350582&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Setting&lt;/a&gt; up a ~/.netrc file with Gitlab credentials&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://stackoverflow.com/questions/72789599/gitlab-ci-cd-execute-script-file-that-exist-in-the-repository&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Executing&lt;/a&gt; a bash scripting within a Gitlab CI/CD pipeline&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://stackoverflow.com/questions/58939500/how-to-pass-gitlab-ci-file-variable-to-dockerfile-and-docker-container&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Passing&lt;/a&gt; variables to Docker image at build time&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.shellhacks.com/gitlab-ci-cd-build-docker-image-push-to-registry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building&lt;/a&gt; Docker image and pushing to registry&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.gitlab.com/ee/user/packages/container_registry/authenticate_with_container_registry.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Authenticating&lt;/a&gt; container registries&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.gitlab.com/ee/user/packages/pypi_repository/#authenticate-with-the-package-registry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Setting&lt;/a&gt; up a ~/.pypirc files with Gitlab credentials&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing SQL Schemas</title>
      <link>/post/sql-schema/</link>
      <pubDate>Tue, 22 Feb 2022 10:24:17 -0700</pubDate>
      <guid>/post/sql-schema/</guid>
      <description>&lt;p&gt;I was recently tasked with examining databases related to some computer vision tools that my company had acquired.  Basically, the framework was as follows&amp;hellip; Clients/users would sign up for some service with the goal in mind of building a model to classify a set of microscopy images.  These models could then be used by the client for downstream services.  The users interacted with this tool through a web application, with which they could upload the training and validation datasets.  They could also interact with models that they might have previously trained.  The software used AWS EC2 instances to train and test their models, and a database to store all the relevant image files and metadata associated with the users and experiments.&lt;/p&gt;
&lt;p&gt;However, I was not provided with any relevant information about the database schema, rendering interpretation of the API and results difficult.  Given that I had access to the MySQL database, I wanted to be able to visualize the interactions between all the relevant SQL tables.&lt;/p&gt;
&lt;p&gt;Assuming that &lt;code&gt;mysql&lt;/code&gt; and &lt;code&gt;MySQL Workbench&lt;/code&gt; are installed, and that you know the database name and corresponding account password, you can run the following command to export the database to an SQL file.  The &lt;code&gt;-u&lt;/code&gt;, &lt;code&gt;-p&lt;/code&gt;, and &lt;code&gt;--no-data&lt;/code&gt; options correspond to the user ID, password, and desire (or lack thereof) to export the data entries as well.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mysqldump -u root -p --no-data ${DB_NAME} &amp;gt; ${OUTPUT_NAME}.sql
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then reverse engineer the database schema by clicking &lt;code&gt;Database -&amp;gt; Reverse Engineer&lt;/code&gt; and subsequent the steps:&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/sql-schema/reverse_hucaf06a6fd22e2d570ceaac7c0a8c909b_156034_2000x2000_fit_lanczos_3.png&#34; &gt;


  &lt;img data-src=&#34;/post/sql-schema/reverse_hucaf06a6fd22e2d570ceaac7c0a8c909b_156034_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;536&#34; height=&#34;248&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We can then actually visualize the database schema and table structures in &lt;code&gt;MySQL Workbench&lt;/code&gt;.  Below, we see that all SQL tables are joined to the table called &lt;code&gt;users&lt;/code&gt; via their corresponding &lt;code&gt;user_id&lt;/code&gt; field.  At a minimum, we&amp;rsquo;ll be able to sort experiments by &lt;code&gt;user_id&lt;/code&gt; &amp;ndash; however, I&amp;rsquo;m hoping there is also a join for &lt;code&gt;model_name&lt;/code&gt; or something along those lines, so that we can more easily interrogate the specific parameterizations of each model.&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/sql-schema/schema_hu2c2607ea4c922e6e62e731c3eaf56347_263063_2000x2000_fit_lanczos_3.png&#34; &gt;


  &lt;img data-src=&#34;/post/sql-schema/schema_hu2c2607ea4c922e6e62e731c3eaf56347_263063_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;766&#34; height=&#34;756&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Entropy and Mutual Information</title>
      <link>/post/information/</link>
      <pubDate>Fri, 08 Oct 2021 15:24:17 -0700</pubDate>
      <guid>/post/information/</guid>
      <description>&lt;meta name=&#34;thumbnail&#34; content=&#34;featured.png&#34;&gt;
&lt;p&gt;I&amp;rsquo;m interested in looking at some spatial mappings between pairs of cortical regions, and believe that these mappings are mediated, to some degree, by the temporal coupling between cortical areas.  I don&amp;rsquo;t necessarily know the functional form of these mappings, but neurobiologically predict that these mappings are not random and have some inherent structure.  I want to examine the relationship between spatial location and strength of temporal coupling.  I&amp;rsquo;m going to use mutual information to measure this association.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s been a while since I&amp;rsquo;ve worked with information-based statistics, so I thought I&amp;rsquo;d review some proofs here.&lt;/p&gt;
&lt;h1 id=&#34;entropy&#34;&gt;Entropy&lt;/h1&gt;
&lt;p&gt;Given a random variable $X$, we define the entropy of $X$ as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
H(X) = - \sum_{x} p(x) \cdot log(p(x))
\end{align}$$&lt;/p&gt;
&lt;p&gt;Entropy measures the degree of uncertainty in a probability distribution.  It is independent of the values $X$ takes, and is entirely dependent on the density of $X$.  We can think of entropy as measuring how &amp;ldquo;peaked&amp;rdquo; a distribution is. Assume we are given a binary random variable $Y$&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y \sim Bernoulli(p) \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;such that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y =
\begin{cases}
1 &amp;amp; \text{with probability $p$} \\
0 &amp;amp; \text{with probability $1-p$}
\end{cases}
\end{align}$$&lt;/p&gt;
&lt;p&gt;If we compute $H(Y)$ as a function of $p$ and plot this result, we see the canonical curve:&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/information/entropy_hue80009bb191a784785003f9715471fdc_16228_2000x2000_fit_lanczos_3.png&#34; &gt;


  &lt;img data-src=&#34;/post/information/entropy_hue80009bb191a784785003f9715471fdc_16228_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;424&#34; height=&#34;280&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Immediately evident is that the entropy curve peaks when $p=0.5$.  We are entirely uncertain what value $y$ will take if we have an equal chance of sampling either 0 or 1.  However, when $p = 0$ or $p=1$, we know exactly which value $y$ will take &amp;ndash; we aren&amp;rsquo;t uncertain at all.&lt;/p&gt;
&lt;p&gt;Entropy is naturally related to the conditional entropy.  Given two variables $X$ and $Y$, conditional entropy is defined as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
H(Y|X) &amp;amp;= -\sum_{x}\sum_{y} = p(x,y)  \cdot log(\frac{p(x,y)}{p(x)}) \\
&amp;amp;= -\sum_{x}\sum_{y} p(y|x) \cdot p(x) \cdot log(p(y|x)) \\
&amp;amp;= -\sum_{x} p(x) \sum_{y} p(y|X=x) \cdot log(p(y|X=x))
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $H(Y|X=x) = -\sum_{y} p(y|X=x) \cdot log(p(y|X=x))$, the conditional of entropy of $Y$ given that $X=x$.  Here, we&amp;rsquo;ve used the fact that $p(x,y) = p(y|x) \cdot p(x) = p(x|y) \cdot p(y)$.To compute $H(Y|X)$, we take the weighted average of these conditional entropies, where weights are defined by the marginal probabilities of $X$.&lt;/p&gt;
&lt;h1 id=&#34;mutual-information&#34;&gt;Mutual Information&lt;/h1&gt;
&lt;p&gt;Related to entropy is the idea of mutual information.  Mutual information is a measure of the mutual dependence between two variables.  We can ask the following question:  does knowing something about variable $X$ tell us anything about variable $Y$?&lt;/p&gt;
&lt;p&gt;The mutual information between $X$ and $Y$ is defined as:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
I(X,Y) &amp;amp;= \sum_{x}\sum_{y} p(x,y) \cdot log\Big(\frac{p(x,y)}{p(x) \cdot p(y)}\Big) \\
&amp;amp;= \sum_{x}\sum_{y}p(x,y) \cdot \log(p(x,y)) - \sum_{x}\sum_{y}p(x,y) \cdot log(p(x)) - \sum_{x}\sum_{y}p(x,y) \cdot log(p(y)) \\
&amp;amp;= -H(X,Y) - \sum_{x}p(x) \cdot log(p(x)) - \sum_{y}p(y) \cdot log(p(y)) \\
&amp;amp;= H(X) + H(Y) - H(X,Y)
\end{align}$$&lt;/p&gt;
&lt;p&gt;$I(X,Y)$ is symmetric in $X$ and $Y$:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
I(X,Y) &amp;amp;= \sum_{x}\sum_{y} p(x,y) \cdot log\Big(\frac{p(x,y)}{p(x) \cdot p(y)}\Big) \\
&amp;amp;= \sum_{x}\sum_{y} p(x,y) \cdot log\Big(\frac{p(x|y)}{p(y)}\Big) \\
&amp;amp;= \sum_{x}\sum_{y} p(x,y) \cdot log(p(x|y)) - \sum_{x}\sum_{y}p(x,y) \cdot log(p(x)) \\
&amp;amp;= -H(X|Y) - \sum_{x}\sum_{y} p(x|y) \cdot p(y) \cdot log(p(x)) \\
&amp;amp;= -H(X|Y) - \sum_{x}log(p(x) \sum_{y}p(x|y) \cdot p(y) \\
&amp;amp;= -H(X|Y) - \sum_{x} p(x) \cdot log(p(x)) \\
&amp;amp;= H(X) - H(X|Y) \\
&amp;amp;= H(Y) - H(Y|X)
\end{align}$$&lt;/p&gt;
&lt;p&gt;We interpret the above to mean the following: if we are given any information about X (Y), can we reduce the uncertainty around what Y (X) should be?  We understand how much variability there is in each variable independently &amp;ndash; this is measured by the marginal entropy $H(Y)$.  If knowing $X$ reduces this uncertainty, then the conditional entropy $H(Y|X)$ should be small.  If knowing $X$ does not reduce this uncertainty, then $H(Y|X)$ can be at most as large as $H(Y)$, and we have learned nothing about our dependent variable $Y$.&lt;/p&gt;
&lt;p&gt;Put another way, if $I(X,Y) = H(Y) - H(Y|X)$ is large, then the mutual information between $X$ and $Y$ is large, indicating that $X$ is informative of $Y$.  However, if $I(X,Y)$ is small, then the mutual information is small, and $X$ is not informative of $Y$.&lt;/p&gt;
&lt;h1 id=&#34;application&#34;&gt;Application&lt;/h1&gt;
&lt;p&gt;For my problem, I&amp;rsquo;m given two variables, $Z$ and $C$.  I&amp;rsquo;m interested in examining how knowledge of $C$ might reduce our uncertainty about $Z$.  $C$ itself is defined by a pair of variables, $A$, and $B$, such that we have $C_{1} = (a_{1}, b_{1})$.  $Z$ is distributed over the tensor-product space of $A$ and $B$, that is:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Z = f(A \otimes B)
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $A \otimes B$ is defined as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
A \otimes B =
\begin{bmatrix}
(a_{1},b_{1}) &amp;amp; \dots &amp;amp; (a_{1},b_{k}) \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
(a_{p}, b_{1}) &amp;amp; \dots &amp;amp; (a_{p}, b_{k})
\end{bmatrix}
\end{align}$$&lt;/p&gt;
&lt;p&gt;such that $z_{i,j} = f(a_{i}, b_{j}$)&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/information/domain_hudf6752327912411f9654bba4b0b0185d_22907_2000x2000_fit_lanczos_3.png&#34; &gt;


  &lt;img data-src=&#34;/post/information/domain_hudf6752327912411f9654bba4b0b0185d_22907_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;424&#34; height=&#34;424&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We define the mutual information between $Z$ and $C$ as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
I(Z,C) &amp;amp;= \sum_{z}\sum_{c} p(z,c) \cdot log\Big(\frac{p(z,c)}{p(z)p(c)} \Big) \\
&amp;amp;= H(Z) - H(Z|C) \\
&amp;amp;= H(Z) - H(X|(A,B)) \\
&amp;amp;= H(Z) - \sum_{a,b} p(a,b) \sum_{z} p(z|(A,B)=(a,b)) \cdot log(p(z|(A,B)=(a,b)))
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the pair $(a,b)$ represents a bin or subsection of the tensor-product space.  The code for this approach can be found below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def entropy(sample):
    
    &amp;quot;&amp;quot;&amp;quot;
    Compute the entropy of a given data sample.  
    Bins are estimated using `Freedman Diaconis` Estimator.
    
    Args:
        sample: float, NDarray
        data from which to compute entropy of
    Returns:
        H: entropy
    &amp;quot;&amp;quot;&amp;quot;
    
    if np.ndim(sample) &amp;gt; 1:
        sample = np.reshape(sample, np.product(sample.shape))
    
    edges = np.histogram_bin_edges(sample, bins=&#39;fd&#39;)
    [counts,_] = np.histogram(sample, bins=edges)
    
    # compute marginal distribution of sample
    m_sample = counts/counts.sum()
    
    return (-1)*np.nansum(m_sample*np.ma.log2(m_sample))
    

def mutual_information_grid(X,Y,Z):
    
    &amp;quot;&amp;quot;&amp;quot;
    Compute the mutual information of a dependent variable over a grid 
    defined by two indepedent variables.
    
    Args:
        X,Y: float, NDarray
            coordinates over which dependent variable is distributed
        Z: float, array
            dependent variable
    Returns:
        estimates: dict
          keys:
            I: float
                mutual information I(Z; (X,Y))
            W: float, NDarray
                matrix of weighted conditional entropies
            marginal: float
                marginal entropy
            conditional: float
                conditional entropy
    &amp;quot;&amp;quot;&amp;quot;
    
    x_edges = np.histogram_bin_edges(X, bins=&#39;fd&#39;)
    [xc, _] = np.histogram(X, bins=x_edges)
    xc = xc/xc.sum()
    nx = x_edges.shape[0]-1
    
    y_edges = np.histogram_bin_edges(Y, bins=&#39;fd&#39;)
    [yc, _] = np.histogram(Y, bins=y_edges)
    yc = yc/yc.sum()
    ny = y_edges.shape[0]-1
    
    # matrix of conditional entropies for each bin
    H = np.zeros((nx, ny))
    
    # compute pairwise marginal probability of X/Y bins
    mxy = xc[:,None]*yc[None,:]
    
    for x_bin in range(nx):
        for y_bin in range(ny):
            
            x_idx = np.where((X&amp;gt;=x_edges[x_bin]) &amp;amp; (X&amp;lt;x_edges[x_bin+1]))[0]
            y_idx = np.where((Y&amp;gt;=y_edges[y_bin]) &amp;amp; (Y&amp;lt;y_edges[y_bin+1]))[0]
            
            bin_samples = Z[x_idx,:][:,y_idx]
            bin_samples = np.reshape(bin_samples, 
                                     np.product(bin_samples.shape))
            
            H[x_bin, y_bin] = entropy(bin_samples)

    W = H*mxy
    conditional = np.nansum(W)
    marginal = entropy(Z)
    
    I = marginal - conditional
    
    estimates = {&#39;mi&#39;: I,
                 &#39;weighted-conditional&#39;: W,
                 &#39;marginal&#39;: marginal,
                 &#39;conditional&#39;: conditional}
    
    return estimates
    
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Jumping-Knowledge Representation Learning With LSTMs</title>
      <link>/post/jumping-knowledge/</link>
      <pubDate>Sun, 14 Feb 2021 23:24:17 -0700</pubDate>
      <guid>/post/jumping-knowledge/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;As I mentioned in my previous post on 
&lt;a href=&#34;/post/constrained-gat/&#34;&gt;constrained graph attention networks&lt;/a&gt;, graph neural networks suffer from overfitting and oversmoothing as network depth increases.  These issues can ultimately be linked to the local topologies of the graph.&lt;/p&gt;
&lt;p&gt;If we consider a 2d image as a graph (i.e. pixels become nodes), we see that images are highly 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Regular_graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;regular&lt;/a&gt; &amp;ndash; that is, each node has the same number of neighbors, except for those at the image periphery.  When we apply convolution kernels over node signals, filters at any given layer are aggregating information from the same sized neighborhods irrespective of their location.&lt;/p&gt;
&lt;p&gt;However, if we consider a graph, there is no guarantee that the graph will be regular.  In fact, in many situations, graphs are highly &lt;em&gt;irregular&lt;/em&gt;, and are characterized by unique topological neighborhood properties such as tree-like structures or 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Expander_graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expander graphs&lt;/a&gt;, that are sparse yet highly connected.  If we compare an expander node to a node whose local topology is more regular, we would find that the number of signals that each node implicitly convolves at each network layer would vary considerably.  These topological discrepancies have important implications when we consider problems like node and graph classification, as well as edge prediction.  The problem ultimately boils down to one of flexibility: can we account for unique local topologies of a graph in order to dynamically aggregate local information on a node-by-node basis?&lt;/p&gt;















&lt;figure id=&#34;figure-node-signal-aggregation-as-a-function-of-network-depth--at-each-layer-the-neural-network-implicitly-aggregates-signals-over-an-increasingly-larger-neighborhood--in-this-example-the-network-is-highly-regular----however-not-all-graphs-are&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./InfluenceRadius.png&#34; data-caption=&#34;Node signal aggregation as a function of network depth.  At each layer, the neural network implicitly aggregates signals over an increasingly-larger neighborhood.  In this example, the network is highly regular &amp;ndash; however, not all graphs are.&#34;&gt;


  &lt;img src=&#34;./InfluenceRadius.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Node signal aggregation as a function of network depth.  At each layer, the neural network implicitly aggregates signals over an increasingly-larger neighborhood.  In this example, the network is highly regular &amp;ndash; however, not all graphs are.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In a recent paper, the authors propose one approach to address this question, which they call &amp;ldquo;jumping knowledge representation learning&amp;rdquo;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.  Instead of utilizing the output of the last convolution layer to inform the prediction, jumping-knowledge networks aggregate the embeddings from all hidden layers to inform the final prediction.  The authors develop an approach to study the &amp;ldquo;influence distribution&amp;rdquo; of nodes: for a given node $x$, the influence distribution $I_{x}$ characterizes how much the final embedding of node $x$ is influenced by the input features of every other node:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
I(x,y) &amp;amp;= \sum_{i=1}^{m} |\frac{\delta h_{x}^{k}}{\delta h_{y}^{0}}|&lt;em&gt;{i} \\
I&lt;/em&gt;{x}(y) &amp;amp;= I(x,y) \Big/\sum_{z} I(x,z)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;They show that influence distribution $I_{x}$ for a $k$-layer graph convolution network is equal, in expectation, to the $k$-step random walk distribution.  They point out that the random walk distribution of expander-like nodes converge quickly &amp;ndash; the final embeddings of these nodes are represenative of the whole graph and carry global information &amp;ndash; while the random-walk distribution of nodes with tree-like topology converge slowly &amp;ndash; these nodes carry more-local information&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.  These two conclusions are related to the spectral gap of the graph &amp;ndash; the smallest non-zero eigenvalue of the graph Laplacian.  A large spectral gap indicates high-connectivity, while a low spectral gap indicates low connectivity.  From a graph theory perspective, this local connectivity is related to the idea of centrality.  Nodes with high centrality will easily saturate their random walk distribution, but will also aggregate information from large neighborhoods quickly.  For graph neural networks with fixed aggregation kernels, this has important implications for representation learning, because the feature distributions of nodes with different topologies will not correspond to the same degree of locality, which may not lead to the best learned representations for all nodes.  A radius that is too large may result in over-smoothing of node features, while a radius that is too small may not be robust enough to learn optimal node embeddings.&lt;/p&gt;
&lt;p&gt;The jumping knowledge network architecture is conceptually similar to other graph neural networks, and we can, in fact, simply incorporate the jumping knowledge mechanism as an additional layer.  The goal is to adaptively learn the effective neighborhood size on a node-by-node basis, rather than enforcing the same aggregation radius for every node (remember, we want to account for local topological and feature variations).  The authors suggest three possible aggregation functions: concatentation, max-pooling, and an LSTM-attention mechanism &lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.  Each aggregator learns an optimal combination of the hidden embeddings, which is then pushed through a linear layer to generate the final network output.  Concatenation determines the optimal linear combination of hidden embeddings for the entire dataset simultaneously, so it is not a node-specific aggregator.  Max-pooling selects the most important hidden layer for each feature element on a node-by-node basis &amp;ndash; however, empirically, I found that max-pooling was highly unstable in terms of model learning.  The LSTM-attention aggregator treats the hidden embeddings as a sequence of elements, and assigns a unique attention score to each hidden embedding &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;















&lt;figure id=&#34;figure-schematic-of-a-jumping-knowledge-network--the-neural-network-generates-an-embedding-for-each-hidden-layer--the-aggregator-function-then-optimally-combines-these-hidden-embeddinggs-to-learn-the-optimal-abstraction-of-input-information--some-alternative-aggregation-functions-include-max-pooling-concatenation-and-an-lstm-layer--in-the-case-of-an-lstm-layer-coupled-with-an-attention-mechanism-the-aggregator-computes-a-convex-combination-of-hidden-embeddings&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./JKGAT_LSTM.png&#34; data-caption=&#34;Schematic of a jumping-knowledge network.  The neural network generates an embedding for each hidden layer.  The aggregator function then optimally combines these hidden embeddinggs to learn the optimal abstraction of input information.  Some alternative aggregation functions include max-pooling, concatenation, and an LSTM layer.  In the case of an LSTM layer coupled with an attention mechanism, the aggregator computes a convex combination of hidden embeddings.&#34;&gt;


  &lt;img src=&#34;./JKGAT_LSTM.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Schematic of a jumping-knowledge network.  The neural network generates an embedding for each hidden layer.  The aggregator function then optimally combines these hidden embeddinggs to learn the optimal abstraction of input information.  Some alternative aggregation functions include max-pooling, concatenation, and an LSTM layer.  In the case of an LSTM layer coupled with an attention mechanism, the aggregator computes a convex combination of hidden embeddings.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;long-short-term-memory&#34;&gt;Long-Short Term Memory&lt;/h4&gt;
&lt;p&gt;Briefly, given a sequence of samples $X_{1}, X_{2}, \dots X_{t}$, the LSTM cell learns temporal dependencies between elements of a sequence by maintaining a memory of previously observed elements &amp;ndash; in our case, the sequence elements are the embeddings learned by each consecutive hidden layer.  An LSTM cell is characterized by three gates controlling information flow between elements in the sequence: input, forget, and output, as well as a cell state vector, which captures the memory and temporal dependencies between sequence elements&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;















&lt;figure id=&#34;figure-schematic-of-an-lstm-cell--the-cell-controls-what-information-is-remembered-from-previous-elements-in-a-sequence-and-what-information-is-incorporated-into-memory-given-a-new-element-in-the-sequence&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./LSTM_cell.png&#34; data-caption=&#34;Schematic of an LSTM cell.  The cell controls what information is remembered from previous elements in a sequence, and what information is incorporated into memory given a new element in the sequence.&#34;&gt;


  &lt;img src=&#34;./LSTM_cell.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Schematic of an LSTM cell.  The cell controls what information is remembered from previous elements in a sequence, and what information is incorporated into memory given a new element in the sequence.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;$$
\begin{align}
f_{t} &amp;amp;= \sigma(W_{f}X_{t} + U_{f}h_{t-1} + b_{f}) \\
i_{t} &amp;amp;= \sigma(W_{i}X_{t} + U_{i}h_{t-1} + b_{i}) \\
o_{t} &amp;amp;= \sigma(W_{o}X_{t} + U_{o}h_{t-1} + b_{o}) \\
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where $W$, $U$, and $b$ are learnable parameters of the gates.  Here, $X_{t}$ is the $t$-th sequence element, $h_{t-1}$ represents the learned LSTM cell embedding for element $t-1$, and $C_{t-1}$ represents the current memory state, given the previous $1, 2 \dots t-1$ elements.  The input and forget gates determine which aspects of a sequence element are informative / uninformative, and decide what information to keep / forget, while the output gate combines the previous memory state with our new knowledge.  We can roughly think of this process as updating our prior beliefs, in the Bayesian sense, with new incoming data.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\tilde{c_{t}} &amp;amp;= \sigma(W_{c}X_{t} + U_{c}h_{t-1} + b_{c}) \\
c_{t} &amp;amp;= f_{t}\circ c_{t-1} + i_{t} \tilde{c}&lt;em&gt;{t} \\
h&lt;/em&gt;{t} &amp;amp;= o_{t} \circ tanh(c_{t})
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The embeddings for each element learned by the LSTM cell are represented by $h_{t}$.  In the original paper&lt;sup id=&#34;fnref2:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, the authors propose to apply a bi-directional LSTM to simultaneously learn forwards and backwards embeddings, which are concatenated and pushed through a single-layer perceptron to compute layer-specific attention weights for each node:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\alpha_{i}^{t} &amp;amp;= \sigma(\vec{w}&lt;em&gt;{t}^{T}(h^{F}&lt;/em&gt;{i, t} || h^{B}&lt;em&gt;{i, t})) \\
\alpha&lt;/em&gt;{i}^{t} &amp;amp;= \frac{\exp{(\alpha_{i}^{t})}}{\sum_{t=1}^{L} \exp{(\alpha_{i}^{t})}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The softmax-normalized attention weights represent a probability distribution over attention weights&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\sum_{t=1}^{L} \alpha_{i}^{t} &amp;amp;= 1 \\
\\
\alpha_{i}^{t} &amp;amp;&amp;gt;= 0
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where $\alpha_{i}^{t}$ represents how much node $i$ attends to the embedding of hidden layer $t$.  The optimal embedding is then computed as the attention-weighted convex combination of hidden embeddings:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
X_{i, \mu} = \sum_{t=1}^{L} \alpha_{i}^{t}X_{i, t}
\end{align}
$$&lt;/p&gt;
&lt;h2 id=&#34;an-application-of-jumping-knowledge-networks-to-cortical-segmentation&#34;&gt;An Application of Jumping Knowledge Networks to Cortical Segmentation&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ve implemented the jumping knowledge network using DGL 
&lt;a href=&#34;https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/jkgat/jkgat.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.  Below, I&amp;rsquo;ll demonstrate the application of jumping knowledge representation learning to a cortical segmentation task.  Neuroscientifically, we have reason to believe that the influence radius will vary along the cortical manifold, even if the mesh structure is highly regular.  As such, I am specifically interested in examining the importance that each node assigns to the embeddings of each hidden layer.  To that end, I utilize the LSTM-attention aggregator.  Similarly, as the jumping-knowledge mechanism can be incorporated as an additional layer to any general graph neural network, I will use graph attention networks (GAT) as the base network architecture, and compare vanilla GAT performance to GATs with a jumping knowledge mechanism (JKGAT).&lt;/p&gt;
&lt;p&gt;Below, I show the prediction generated by a 9-layer JKGAT model, with 4 attention heads and 32 hidden channels per layer, with respect to the &amp;ldquo;known&amp;rdquo; or &amp;ldquo;true&amp;rdquo; cortical map.  We find slight differences in the performance of our JKGAT model with respect to the ground truth map, notably in the lateral occipital cortex and the medial prefrontal cortex.&lt;/p&gt;















&lt;figure id=&#34;figure-comparison-of-the-group-average-predicted-cortical-segmentation-produced-by-the-jkgat-model-to-the-ground-truth-cortical-segmentation--the-ground-truth-was-previously-generated-herehttpswwwncbinlmnihgovpmcarticlespmc4990127--the-consensus-cortical-map-corresponds-very-well-to-the-true-map&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./prediction_G.png&#34; data-caption=&#34;Comparison of the group-average predicted cortical segmentation produced by the JKGAT model, to the ground truth cortical segmentation.  The ground truth was previously generated 
here.  The consensus cortical map corresponds very well to the true map.&#34;&gt;


  &lt;img src=&#34;./prediction_G.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Comparison of the group-average predicted cortical segmentation produced by the JKGAT model, to the ground truth cortical segmentation.  The ground truth was previously generated 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4990127/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.  The consensus cortical map corresponds very well to the true map.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;When we consider the accuracies for various parameterizations of our models, we see that the JKGAT performs quite well.  Notably, it performs better than the GAT model in most cases.  Likewise, as hypothesized, the JKGAT performs better than the GAT model as network depth increases, specifically because we are able to dynamically learn the optimal influence radii for each node, rather than constraining the same radius size for the entire graph.  This allows us to learn more abstract representations of the input features by mitigating oversmoothing and by accounting for node topological variability, which is important for additional use-cases like graph classification.&lt;/p&gt;















&lt;figure id=&#34;figure-model-accuracy-comparison-between-gat-and-jkgat-models-on-a-node-classification-problem-for-cortical-segmentation--accuracy-is-represented-as-the-fraction-of-correctly-labeled-nodes-in-a-graph-across-150-validation-subjects--each-node-in-the-graph-has-80-features-and-each-graph-has-30k-nodes&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./validation.accuracy.png&#34; data-caption=&#34;Model accuracy comparison between GAT and JKGAT models on a node classification problem for cortical segmentation.  Accuracy is represented as the fraction of correctly-labeled nodes in a graph, across 150 validation subjects.  Each node in the graph has 80 features, and each graph has 30K nodes.&#34;&gt;


  &lt;img src=&#34;./validation.accuracy.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model accuracy comparison between GAT and JKGAT models on a node classification problem for cortical segmentation.  Accuracy is represented as the fraction of correctly-labeled nodes in a graph, across 150 validation subjects.  Each node in the graph has 80 features, and each graph has 30K nodes.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Similarly, we find that JKGAT networks generate segmentation predictions that are more reproducible and consistent across resampled datasets.  This is important, especially in the case where we might acquire data on an individual multiple times, and want to generate a cortical map for each acquisition instance.  Unless an individual suffers from an accelerating neurological disorder, experiences a traumatic neurological injury, or the time between consecutive scans is very long (on the order of years), we expect the cortical map of any given individual to remain quite static (though examining how the &amp;ldquo;map&amp;rdquo; of an individual changes over time is still an open-ended topic).&lt;/p&gt;















&lt;figure id=&#34;figure-model-reproducibility-comparison-between-gat-and-jkgat-models-on-a-node-classification-problem-for-cortical-segmentation-using-150-validation-subjects--each-subject-has-four-repeated-datasets--within-a-given-subject-the-topology-of-each-graph-is-the-same-but-the-node-features-are-re-sampled-for-each-graph--reproducibility-is-computed-using-the-f1-score-between-all-pairs-of-predicted-node-classifications-such-that-we-compute-6-f1-scores-for-each-subject&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./validation.reproducibility.png&#34; data-caption=&#34;Model reproducibility comparison between GAT and JKGAT models on a node classification problem for cortical segmentation, using 150 validation subjects.  Each subject has four repeated datasets.  Within a given subject, the topology of each graph is the same, but the node features are re-sampled for each graph.  Reproducibility is computed using the F1-score between all pairs of predicted node classifications, such that we compute 6 F1 scores for each subject.&#34;&gt;


  &lt;img src=&#34;./validation.reproducibility.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model reproducibility comparison between GAT and JKGAT models on a node classification problem for cortical segmentation, using 150 validation subjects.  Each subject has four repeated datasets.  Within a given subject, the topology of each graph is the same, but the node features are re-sampled for each graph.  Reproducibility is computed using the F1-score between all pairs of predicted node classifications, such that we compute 6 F1 scores for each subject.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Finally, when we consider the importance that each cortical node assigns to the unique embedding at the $k$-th layer via the LSTM-attention aggregation function, we see very interesting results.  Notably, we see high spatial auto-correlation of the attention weights.  Even more striking, is that this spatial correlation seems to correspond to well-studied patterns of resting-state networks identified using functional MRI. Apart from the adjacency structure of our graphs, we do not encode any &lt;em&gt;a priori&lt;/em&gt; information of brain connectivity.  That the LSTM-attention aggregator of the jumping-knowledge layer idenfities maps corresponding reasonably well to known functional networks of the human brain is indicative, to some extent, of how the model is learning, and more importantly, of which features are useful in distinguishing cortical areas from one another.&lt;/p&gt;
&lt;p&gt;Let us consider the attention map for layer 4.  We can interpret the maps as follows: for a given network architecture (in this case, a network with 9 layers), we find that areas in the primary motor (i.e. Brodmann areas 3a and banks of area 4) and primary auditory cortex (Broddmann areas A1 and R1) preferentially attend to the embedding of hidden layer 4, relative to the rest of the cortex &amp;ndash; this indicates that the implicit aggregation over an influence radius of 4 layers is deemed more informative for the classification of nodes in the primary motor and auditory regions than for orther cortical areas.  However, whether this says anything about the implicit complexitiy of the cortical signals of these areas remains to be studied.&lt;/p&gt;















&lt;figure id=&#34;figure-maps-of-learned-lstm-attention-aggregator-weights--each-inset-corresponds-to-the-weights-learned-by-every-cortical-node-for-the-k-th-layer-hidden-embedding-black-low-red-high--we-see-that-most-of-the-attention-mass-is-distributed-over-layers-4-7-indicating-that-most-nodes-assign-maximal-importance-to-intermediate-levels-of-abstraction--however-we-do-see-spatially-varying-attention--notably-within-a-given-attention-map-we-find-that-nodes-of-the-lateral-default-mode-network-preferentially-attend-to-the-embeddings-of-layers-1-3-while-layer-4-is-preferentially-attended-to-by-the-primary-motor-and-auditory-areas&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./attentions.png&#34; data-caption=&#34;Maps of learned LSTM-attention aggregator weights.  Each inset corresponds to the weights learned by every cortical node for the $k$-th layer hidden embedding (black: low, red: high).  We see that most of the attention mass is distributed over layers 4-7, indicating that most nodes assign maximal importance to intermediate levels of abstraction.  However, we do see spatially varying attention.  Notably, within a given attention map, we find that nodes of the lateral Default Mode Network preferentially attend to the embeddings of layers 1-3, while layer 4 is preferentially attended to by the primary motor and auditory areas.&#34;&gt;


  &lt;img src=&#34;./attentions.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Maps of learned LSTM-attention aggregator weights.  Each inset corresponds to the weights learned by every cortical node for the $k$-th layer hidden embedding (black: low, red: high).  We see that most of the attention mass is distributed over layers 4-7, indicating that most nodes assign maximal importance to intermediate levels of abstraction.  However, we do see spatially varying attention.  Notably, within a given attention map, we find that nodes of the lateral Default Mode Network preferentially attend to the embeddings of layers 1-3, while layer 4 is preferentially attended to by the primary motor and auditory areas.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Xu et al. 
&lt;a href=&#34;https://arxiv.org/pdf/1806.03536.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Representation Learning on Graphs with Jumping Knowledge Networks&lt;/a&gt;. 2018.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Dinitz et a. 
&lt;a href=&#34;https://arxiv.org/pdf/1611.01755.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large Low-Diameter Graphs are Good Expanders&lt;/a&gt;. 2017.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Lutzeyer et al. 
&lt;a href=&#34;https://arxiv.org/pdf/1712.03769.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Comparing Graph Spectra of Adjacency and Laplacian Matrices&lt;/a&gt;. 2017.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Gers, Felix.  
&lt;a href=&#34;http://www.felixgers.de/papers/phd.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Long Short-Term Memory in Recurrent Neural Networks&lt;/a&gt;. 2001.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Fan et al. 
&lt;a href=&#34;https://www.mdpi.com/2073-4441/12/1/175/htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Comparison of Long Short Term Memory Networks and the Hydrological Model in Runoff Simulation&lt;/a&gt;.  2020.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Constrained Graph Attention Networks</title>
      <link>/post/constrained-gat/</link>
      <pubDate>Fri, 25 Dec 2020 23:24:17 -0700</pubDate>
      <guid>/post/constrained-gat/</guid>
      <description>&lt;p&gt;In their recent 
&lt;a href=&#34;https://arxiv.org/abs/1910.11945&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click 
&lt;a href=&#34;#Implementation&#34;&gt;here&lt;/a&gt;).  Briefly, GATs are a 
&lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recently-developed&lt;/a&gt; neural network architecture applied to data distributed over a graph domain.  We can think of graph convolutional networks as progressively transforming and aggregating signals from within a local neighborhood of a node.  At each iteration of this process, we implicitly merge signals from larger and larger neighborhoods of the node of interest, and thereby learn unique representations of nodes that are dependent on their surroundings.&lt;/p&gt;
&lt;p&gt;GATs incorporate the seminal idea of &amp;ldquo;attention&amp;rdquo; into this learning process.  In each message-passing step, rather than updating the features of a source-node via equally-weighted contributions of neighborhood nodes, GAT models learn an attention function &amp;ndash; i.e. they learn how to differentially pay attention to various signals in the neighborhood.  In this way, the algorithm can learn to focus on imporant signals and disregard superfluous signals.  If we consider neural networks as universal funtion approximators, the attention mechanism improves the approximating ability by incorporating multiplicative weight factors into the learning.&lt;/p&gt;















&lt;figure id=&#34;figure-figure-from-velickovic-et-alhttpsarxivorgpdf171010903pdf--for-a-source-node-i-and-destination-node-j-vectors-vech_i-and-vech_j-are-the-input-feature-vectors-of-nodes-i-and-j-in-layer-l--mathbfw-is-a-learned-affine-projection-matrix--mathbfveca-is-the-learned-attention-function--the-source-and-destination-node-input-features-are-pushed-through-the-attention-layer-as-alpha_ij--sigmabigvecatmathbfwbigvech_i--vech_jbigbig-where-sigma-is-an-activation-function-and-alpha_ij-the-unnormalized-attention-that-node-i-pays-to-node-j-in-layer-l--attention-weights-are-then-passed-through-a-softmax-layer-mapping-the-attentions-between-01&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./attention_mechanism.png&#34; data-caption=&#34;Figure from 
Velickovic et al.  For a source node $i$ and destination node $j$, vectors $\vec{h_{i}}$ and $\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$.  $\mathbf{W}$ is a learned affine projection matrix.  $\mathbf{\vec{a}}$ is the learned attention function.  The source and destination node input features are pushed through the attention layer as $\alpha_{i,j} = \sigma\Big(\vec{a}^{T}\mathbf{W}\Big(\vec{h_{i}} || \vec{h_{j}}\Big)\Big)$ where $\sigma$ is an activation function, and $\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$.  Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].&#34;&gt;


  &lt;img src=&#34;./attention_mechanism.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure from 
&lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Velickovic et al&lt;/a&gt;.  For a source node $i$ and destination node $j$, vectors $\vec{h_{i}}$ and $\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$.  $\mathbf{W}$ is a learned affine projection matrix.  $\mathbf{\vec{a}}$ is the learned attention function.  The source and destination node input features are pushed through the attention layer as $\alpha_{i,j} = \sigma\Big(\vec{a}^{T}\mathbf{W}\Big(\vec{h_{i}} || \vec{h_{j}}\Big)\Big)$ where $\sigma$ is an activation function, and $\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$.  Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;However, GATs are not without their pitfals, as noted by Wang et al.  Notably, the authors point to two important issues that GATs suffer from: overfitting of attention values and oversmoothing of signals across class boundaries.  The authors propose that GATs overfit the attention function because the learning process is driven only by classification error, with complexity $O(|V|)$ i.e. the number of nodes in the graph.  With regards to oversmoothing, the authors note that a single attention layer can be viewed as a form of Laplacian smoothing:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y = AX^{l}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $A_{n \times n}$ is the attention weight matrix with $A_{i,j} = \alpha_{i,j}$ if $j \in \mathcal{N_{i}}$ and $0$ otherwise.  Because $\sum_{j\in \mathcal{N_{i}}} \alpha_{i,j} = 1$, we can view $A$ as a random walk transition probability matrix.  If we assume that graph $G=(V,E)$ has $K$ connected components, repeated application of $A$ to $X$ distributed over $G$ will result in a stationary distribution of node features within each connected component &amp;ndash; that is, the features vectors of the nodes within each connected component will converge on the component mean.  However, as the authors point out, we typically have multiple layers $l_{1}\dots l_{j}$, each with their own attention matrix $A_{1} \dots A_{j}$, each representing a unique transition probability matrix.  Because we generally do not have disconnected components, nodes from different classes will be connected &amp;ndash; consequentially, deep GAT networks will mix and smooth signals from different adjacent components, resulting in classification performance that worsens with network depth.  Importantly, multi-head attention networks do not alleviate this convergence issue &amp;ndash; each head can be viewed as a unique probability transition matrix, which all suffer from the same oversmoothing issue as $l \rightarrow \infty$.&lt;/p&gt;
&lt;p&gt;Wang et al. propose to incorporate two margin-based constraints into the learning process.  The first constraint, $\mathcal{L_{g}}$, addresses the overfitting issue, by enforcing that learned attentions between adjacent pairs of nodes be higher than attentions between distant pairs of nodes.&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\mathcal{L_{g}} &amp;amp;= \sum_{i \in V} \sum_{j \in \mathcal{N_{i}} \setminus \mathcal{N_{i}^{-}}} \sum_{k \in V\setminus \mathcal{N_{i}}} max(0, \phi(v_{i},v_{k}) + \zeta_{g} - \phi(v_{i},v_{j}))
\end{align}$$&lt;/p&gt;
&lt;p&gt;The second constraint, $\mathcal{L_{b}}$, address the oversmoothing issue, by enforcing that learned attentions between pairs of adjacent nodes with the same label be higher than attention between pairs of adjacent nodes with different labels:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\mathcal{L_{b}} &amp;amp;= \sum_{i \in V}\sum_{j \in \mathcal{N_{i}^{+}}} \sum_{k \in \mathcal{N_{i}^{-}}} max(0, \phi(v_{i},v_{k}) + \zeta_{b} - \phi(v_{i},v_{j}))
\end{align}$$&lt;/p&gt;
&lt;p&gt;In both cases, $\phi(,)$ is the attention function between a pair of nodes, $\mathcal{N_{i}^{+}}$ and $\mathcal{N_{i}^{-}}$ are the nodes adjacent to node $i$ with the same (+) and different (-) labels as $i$, and $\zeta_{g}$ and $\zeta_{b}$ are slack variables controlling the margin between attention values.  The first loss function, $\mathcal{L_{g}}$, can be implemented via negative sampling of nodes (the authors actually perform importance-based negative sampling based on attention-weighted node degrees, but showed that this only marginally improved classification accuracy in benchmark datasets).&lt;/p&gt;
&lt;p&gt;The authors propose one final addition to alleviate the oversmoothing issue posed by vanilla GATs.  Rather than aggregating over all adjacent nodes in a neighborhood, the authors propose to aggregate over the nodes with the $K$ greatest attention values.  Because the class boundary loss $\mathcal{L_{b}}$ enforces large attentions on nodes of the same label and small attention on nodes of different labels, aggregating over the top $K$ nodes will tend to exclude nodes of different labels than the source node in the message passing step, thereby preventing oversmoothing.  The authors show that this constrained aggregation approach is preferable to attention dropout proposed in the original 
&lt;a href=&#34;https://arxiv.org/pdf/1710.10903.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GAT paper&lt;/a&gt;.  &lt;a name=&#34;Implementation&#34;&gt;
Taken together, the authors deem these margin-based loss and constrained aggregation &amp;ldquo;Constrained Graph Attention Network&amp;rdquo; (C-GAT).&lt;/p&gt;
&lt;/a&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;I wasn&amp;rsquo;t able to find an implementation of the Constrained Graph Attention Network for my own purposes, so I&amp;rsquo;ve implemented the algorithm myself in 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt; (DGL) &amp;ndash; the source code for this convolutional layer can be found 
&lt;a href=&#34;https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/conv/cgatconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.  This implementation makes use of the original DGL &lt;code&gt;GATConv&lt;/code&gt; layer structure, with modifications made for the constraints and aggregations.  Specifically, the API for &lt;code&gt;CGATConv&lt;/code&gt; has the following modifications:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
CGATCONV(in_feats, 
         out_feats, 
         num_heads, 
         feat_drop=0., 
         graph_margin=0.1, # graph structure loss slack variable
         class_margin=0.1, # class boundary loss slack variable
         top_k=3, # number of messages to aggregate over
         negative_slope=0.2,
         residual=False,
         activation=None,
         allow_zero_in_degree=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of note is the fact that the &lt;code&gt;attn_drop&lt;/code&gt; parameter has been substituted by the &lt;code&gt;top_k&lt;/code&gt; parameter in order to mitigate oversmoothing, and the two slack variables $\zeta_{g}$ and $\zeta_{b}$ are provided as &lt;code&gt;graph_margin&lt;/code&gt; and &lt;code&gt;class_margin&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With regards to the loss functions, the authors compute all-pairs differences between all edges incident on a source node, instead of summing over the positive / negative sample attentions ($\mathcal{L_{g}}$) and same / different label attentions ($\mathcal{L_{b}}$) and then differencing these summations.  In this way, the C-GAT model anchors the loss values to specific nodes, promoting learning of more generalizable attention weights.  The graph structure loss function $\mathcal{L_{g}}$ is implemented with the &lt;code&gt;graph_loss&lt;/code&gt; reduction function below:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def graph_loss(nodes):
            
    &amp;quot;&amp;quot;&amp;quot;
    Loss function on graph structure.
    
    Enforces high attention to adjacent nodes and 
    lower attention to distant nodes via negative sampling.
    &amp;quot;&amp;quot;&amp;quot;

    msg = nodes.mailbox[&#39;m&#39;]

    pw = msg[:, :, :, 0, :].unsqueeze(1)
    nw = msg[:, :, :, 1, :].unsqueeze(2)

    loss = (nw + self._graph_margin - pw).clamp(0)
    loss = loss.sum(1).sum(1).squeeze()

    return {&#39;graph_loss&#39;: loss}
.
.
.
graph.srcdata.update({&#39;ft&#39;: feat_src, &#39;el&#39;: el})
graph.dstdata.update({&#39;er&#39;: er})
graph.apply_edges(fn.u_add_v(&#39;el&#39;, &#39;er&#39;, &#39;e&#39;))
e = self.leaky_relu(graph.edata.pop(&#39;e&#39;))

# construct the negative graph by shuffling edges
# does not assume a single graph or blocked graphs
# see cgatconv.py for ```construct_negative_graph``` function
neg_graph = [construct_negative_graph(i, k=1) for i in dgl.unbatch(graph)]
neg_graph = dgl.batch(neg_graph)

neg_graph.srcdata.update({&#39;ft&#39;: feat_src, &#39;el&#39;: el})
neg_graph.dstdata.update({&#39;er&#39;: er})
neg_graph.apply_edges(fn.u_add_v(&#39;el&#39;, &#39;er&#39;, &#39;e&#39;))
ne = self.leaky_relu(neg_graph.edata.pop(&#39;e&#39;))

combined = th.stack([e, ne]).transpose(0, 1).transpose(1, 2)
graph.edata[&#39;combined&#39;] = combined
graph.update_all(fn.copy_e(&#39;combined&#39;, &#39;m&#39;), graph_loss)

# compute graph structured loss
Lg = graph.ndata[&#39;graph_loss&#39;].sum() / (graph.num_nodes() * self._num_heads)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, the class boundary loss function $\mathcal{L_{b}}$ is implemented with the following message and reduce functions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def adjacency_message(edges):
            
    &amp;quot;&amp;quot;&amp;quot;
    Compute binary message on edges.

    Compares whether source and destination nodes
    have the same or different labels.
    &amp;quot;&amp;quot;&amp;quot;

    l_src = edges.src[&#39;l&#39;]
    l_dst = edges.dst[&#39;l&#39;]

    if l_src.ndim &amp;gt; 1:
        adj = th.all(l_src == l_dst, dim=1)
    else:
        adj = (l_src == l_dst)

    return {&#39;adj&#39;: adj.detach()}

def class_loss(nodes):
    
    &amp;quot;&amp;quot;&amp;quot;
    Loss function on class boundaries.
    
    Enforces high attention to adjacent nodes with the same label
    and lower attention to adjacent nodes with different labels.
    &amp;quot;&amp;quot;&amp;quot;

    m = nodes.mailbox[&#39;m&#39;]

    w = m[:, :, :-1]
    adj = m[:, :, -1].unsqueeze(-1).bool()

    same_class = w.masked_fill(adj == 0, np.nan).unsqueeze(2)
    diff_class = w.masked_fill(adj == 1, np.nan).unsqueeze(1)

    difference = (diff_class + self._class_margin - same_class).clamp(0)
    loss = th.nansum(th.nansum(difference, 1), 1)

    return {&#39;boundary_loss&#39;: loss}
.
.
.
graph.ndata[&#39;l&#39;] = label
graph.apply_edges(adjacency_message)
adj = graph.edata.pop(&#39;adj&#39;).float()

combined = th.cat([e.squeeze(), adj.unsqueeze(-1)], dim=1)
graph.edata[&#39;combined&#39;] = combined
graph.update_all(fn.copy_e(&#39;combined&#39;, &#39;m&#39;), class_loss)
Lb = graph.ndata[&#39;boundary_loss&#39;].sum() / (graph.num_nodes() * self._num_heads)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And finally, the constrained message aggregation is implemented using the following reduction function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def topk_reduce_func(nodes):
    
    `&amp;quot;&amp;quot;&amp;quot;
    Aggregate attention-weighted messages over the top-K 
    attention-valued destination nodes
    &amp;quot;&amp;quot;&amp;quot;

    K = self._top_k

    m = nodes.mailbox[&#39;m&#39;]
    [m,_] = th.sort(m, dim=1, descending=True)
    m = m[:,:K,:,:].sum(1)

    return {&#39;ft&#39;: m}
.
.
.
# message passing
if self._top_k is not None:
    graph.update_all(fn.u_mul_e(&#39;ft&#39;, &#39;a&#39;, &#39;m&#39;), 
                    topk_reduce_func)
else:
    graph.update_all(fn.u_mul_e(&#39;ft&#39;, &#39;a&#39;, &#39;m&#39;),
                    fn.sum(&#39;m&#39;, &#39;ft&#39;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cross-Entropy With Structure</title>
      <link>/post/structured-cross-entropy/</link>
      <pubDate>Wed, 09 Dec 2020 01:12:32 -0700</pubDate>
      <guid>/post/structured-cross-entropy/</guid>
      <description>&lt;p&gt;As I mentioned in my previous 
&lt;a href=&#34;/post/gaussian-kernel-convolution/&#34;&gt;post&lt;/a&gt;, I work with cortical surface segmentation data.  Due to the biology of the human brain, there is considerable reproducible structure and function across individuals (thankfully!).  One manifestion of this reproducibility is exemplified by the neocortex a.k.a. the thin (~2.5mm) gray matter layer of cell-bodies at the periphery of the brain.  The neocortex is well known to have local homogeneity in terms of types of neuronal cells, protein and gene expression, and large-scale function, for example.  Naturally, researchers have been trying to identify discrete delineations of the cortex for nearly 100 years, by looking for regions of local homogeneity of various features along the cortical manifold.&lt;/p&gt;
&lt;p&gt;As in my previous post, I&amp;rsquo;m working on this problem using graph convolution networks (GCN).  Given the logits output by a forward pass of a GCN, I want to classify a cortical node as belonging to some previously identified cortical area.  Using categorical cross-entropy, we can calculate the loss of a given foward pass of the model $h(X; \Theta)$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
L = -\sum_{k=1}^{K} \sum_{l \in \mathcal{L}} x_{l}^{k} \cdot log(\sigma(x^{k})_{l})
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where $x^{k}$ is the output of the model for a single node, $x_{l}^{k}$ is the one-hot-encoding value of the true labels, and $\sigma$ is the softmax function.  Importantly, the cross-entropy cost is high when the probability assigned to the true label of a node is small i.e. $log(0) = \infty$, while $log(1) = 0$ &amp;ndash; as such, the cross-entropy tries to minimize the rate of false negatives.&lt;/p&gt;
&lt;p&gt;However, we can incorporate more &lt;em&gt;structure&lt;/em&gt; into this loss function.  As I mentioned previously, we know that the brain is highly reproducible across individuals.  In our case, we have years of biological evidence pointing to the fact that functional brain areas i.e. like the primary visual area (V1), will always be in the same anatomical location i.e. posterior occipital cortex &amp;ndash; and will always be adjacent to a small subjset of other functionally-defined areas, like the secondary visual area (V2), for example.&lt;/p&gt;















&lt;figure id=&#34;figure-various-maps-of-the-primate-visual-cortex--tootell-et-al-2003httpswwwjneurosciorgcontent23103981&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.jneurosci.org/content/jneuro/23/10/3981/F1.large.jpg?width=800&amp;amp;height=600&amp;amp;carousel=1&#34; data-caption=&#34;Various maps of the primate visual cortex.  
Tootell et al, 2003.&#34;&gt;


  &lt;img src=&#34;https://www.jneurosci.org/content/jneuro/23/10/3981/F1.large.jpg?width=800&amp;amp;height=600&amp;amp;carousel=1&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Various maps of the primate visual cortex.  
&lt;a href=&#34;https://www.jneurosci.org/content/23/10/3981&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tootell et al, 2003&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This leads us to the idea of assigning a high cost when nodes which should be in V1, for example, are assigned labels of regions that are not adjacent to V1.  We do so by by defining another cost function:&lt;/p&gt;
&lt;p&gt;\begin{align}
G = -\sum_{k=1}^{k}\sum_{l \in \mathcal{L}} \sum_{h \in \mathcal{L} \setminus \mathcal{N_{l}}} w_{l}^{k} \cdot log(1-\sigma(x^{k})_{l})
\end{align}&lt;/p&gt;
&lt;p&gt;where $w_{l}^{k}$ is the probability weight assigned to label $h \in \mathcal{L}\setminus \mathcal{N_{l}}$ i.e. the set of labels not adjacent to label $l$.  In order to follow the idea of a cross-entropy, we enforce the following constraints on weights $\mathbf{w}$:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
w_{l}^{k} &amp;gt;&amp;amp;= 0 \\
\sum_{l \in \mathcal{L}} w_{l}^{k} &amp;amp;= 1
\end{align}
$$&lt;/p&gt;
&lt;p&gt;such that the vector $\mathbf{w}$ is a probability distribution over labels.  Importantly, if we  consider more closely what this loss-function is doing, we are encouraging the predicted label of $x^{k}$ to &lt;strong&gt;not&lt;/strong&gt; be in the set $\mathcal{L} \setminus \mathcal{N_{l}}$.  Assume, for example, that the true label of $x^{k}$ is $t$, and that label $j$ is not adjacent to label $t$ on the cortical surface.  If the softmax function assigns a probability $p(x^{k}&lt;em&gt;{l} = j) = 0.05$, then $log(1-p(x^{k}&lt;/em&gt;{l} = j))$ will be small.  However, if $p(x^{k}&lt;em&gt;{l} = j) = 0.95$, then $log(1-p(x^{k}&lt;/em&gt;{l} = j))$ will be large.  Consequentially, we penalize higher probabilities assigned to labels not adjacent to our true label &amp;ndash; i.e. ones that are not even biologically plausible.  If a candidate label of $x^{k}&lt;em&gt;{l} \in \mathcal{N&lt;/em&gt;{t}}$, we simply set $w_{l}^{k} = 0$ &amp;ndash; that is, we do not penalize the true label (obviously), or labels adjacent to the true label, since these are the regions we really want to consider.&lt;/p&gt;
&lt;p&gt;Below, I&amp;rsquo;ve implemented this loss function using 
&lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch&lt;/a&gt; and 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt;.  Assume that we are given the adjacency matrix of our mesh, the logits of our model, and the true label of our training data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

import dgl
import dgl.function as fn
import torch.nn.functional as F

import torch as th

def structured_cross_entropy(graph, logits, target):
    
    &amp;quot;&amp;quot;&amp;quot;
    Compute a structured cross-entropy loss.
    
    Loss penalizes high logit probabilities assigned to labels
    that are not directly adjacent to the true label.
    
    Parameters:
    - - - - -
    graph: DGL graph
        input graph structure
    input: torch tensor
        logits from model
    target: torch tensor
        true node labeling
    Returns:
    - - - -
    loss: torch tensor
        structured cross-entropy loss
    &amp;quot;&amp;quot;&amp;quot;
    
    # compute one-hot encoding of true labels
    hot_encoding = F.one_hot(target).float()
    
    # identify adjacent labels
    weight = th.matmul(hot_encoding.t(), 
                               th.matmul(graph.adjacency_matrix(), hot_encoding))
    weight = (1-(weight&amp;gt;0).float())

    # compute inverted encoding (non-adjacent labels receive value of 1)
    inv_encoding = weight[target]
    # weight by 1/(# non adjacent)
    # all non-adjacent labels receive the same probability
    # adjacent labels and self-label receive probability of 0
    inv_encoding = inv_encoding / inv_encoding.sum(1).unsqueeze(1)
    loss = th.sum(inv_encoding*th.log(1-F.softmax(logits)), 1)

    return -loss.mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we wanted to use this loss function in conjunction with another loss, like the usual cross-entropy, we could perform something like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# define a regularizing parameter
gamma = 0.1
# define the usual cross-entropy loss function
loss_fcn = torch.nn.CrossEntropyLoss()
loss = loss_function(logits, target) + gamma*structured_cross_entropy(graph, logits, target)

# because our new loss functions performs computations using Pytorch
# the computation history is stored, and we can compute the gradient 
# with respect to this combined loss as

optimizer.zero_grad() # zero the gradients (no history)
loss.backward() # compute new gradients
optimizer.step() # update weights and parameters w.r.t new gradient

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we&amp;rsquo;re optimizing two loss functions now i.e. the global accuracy of the model as defined using the conventional cross-entropy, &lt;strong&gt;and&lt;/strong&gt; the desire for predicted labels to &lt;em&gt;not&lt;/em&gt; be far away from the true label using the structured cross-entropy, this combination of loss functions will likely have the effect of slightly reducing global accuracy &amp;ndash; however, it will have the effect of generating predictions showing fewer anatomically spurious labels i.e. we are less likely to see vertices in the frontal lobe labeled as V1, or vertices in the lateral parietal cortex labeled as Anterior Cingulate.  Global predictions will be more biologically plausible.  While GCNs as a whole are alreadly better-able to incorporate local spatial information than other models due to the fact that they convolve signals based on the adjacency structure of the network in question, I have found empirically that these anatomically spurious predictions are still possible &amp;ndash; hence the need for this more-structured regularization.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Graph Convolutional Networks</title>
      <link>/post/gaussian-kernel-convolution/</link>
      <pubDate>Mon, 07 Dec 2020 23:24:17 -0700</pubDate>
      <guid>/post/gaussian-kernel-convolution/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m using 
&lt;a href=&#34;https://tkipf.github.io/graph-convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;graph convolutional networks&lt;/a&gt; as a tool to segment the cortical surface of the brain.  This research resides in the domain of &lt;em&gt;node classification&lt;/em&gt; using &lt;em&gt;inductive learning&lt;/em&gt;.  By node classification, I mean that we wish to assign a discrete label to cortical surface locations (nodes / vertices in a graph) on the basis of some feature data and brain network topology.  By inductive learning, I mean that we will train, validate, and test on datasets with possibly different graph topologies &amp;ndash; this is in contrast to &lt;em&gt;transductive learning&lt;/em&gt; that learns models that do not generalize to arbitrary network topology.&lt;/p&gt;
&lt;p&gt;In conventional convolutions over regular grid domains, such as images, using approaches like
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convolutional_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ConvNet&lt;/a&gt;, we learn the parameters of a sliding filter that convolves the signal around a pixel of interest $p_{i,j}$, such that we aggregate the information from pixels $p_{\Delta i, \Delta j}$ for some fixed distance $\Delta$ away from $p$. Oftentimes, however, we encounter data that is distributed over a graphical domain, such as social networks, journal citations, brain connectivity, or the electrical power grid.  In such cases, concepts like &amp;ldquo;up&amp;rdquo;, &amp;ldquo;down&amp;rdquo;, &amp;ldquo;left&amp;rdquo;, and &amp;ldquo;right&amp;rdquo; do not make sense &amp;ndash; what does it mean to be &amp;ldquo;up&amp;rdquo; from something in a network? &amp;ndash; so we need some other notion of neighborhood.&lt;/p&gt;
&lt;p&gt;In come graph convolutional networks (GCNs).  GCNs generalize the idea of neighborhood aggregation to the graph domain by utilizing the adjacency structure of a network &amp;ndash; we can now aggregate signals near a node by using some neighborhood around it.  While vanilla GCNs learn rotationally-invariant filters, recent developments in the world of 
&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer networks&lt;/a&gt; have opened up the door for much more flexible and inductive models (see: 
&lt;a href=&#34;https://arxiv.org/abs/1710.10903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Attention Networks&lt;/a&gt;, 
&lt;a href=&#34;https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GraphSAGE&lt;/a&gt;).&lt;/p&gt;















&lt;figure id=&#34;figure-demonstration-of-graph-convolution-network-from-thomas-kipfhttpstkipfgithubiograph-convolutional-networks&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png&#34; data-caption=&#34;Demonstration of graph convolution network from 
Thomas Kipf.&#34;&gt;


  &lt;img src=&#34;https://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Demonstration of graph convolution network from 
&lt;a href=&#34;https://tkipf.github.io/graph-convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas Kipf&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;I was specifically interested in applying the methodology described 
&lt;a href=&#34;http://arxiv.org/abs/1803.10336&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, where the authors utilitize Gaussian kernels as filters over the neighborhood of nodes.  However, the authors did not open-source their code &amp;ndash; as such, I needed to implement this method myself.  Assume our input data to layer $l$ is $Y^{(l)} \in \mathbb{R}^{N \times q}$ for $N$ nodes in the graph.  We can define the Gaussian kernel-weighted convolution as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
z_{i,p}^{(l)} = \sum_{j \in \mathcal{N}&lt;em&gt;{i}} \sum&lt;/em&gt;{q=1}^{M_{(l)}} \sum_{k=1}^{K_{(l)}} w_{p,q,k}^{(l)} \cdot y_{j,q}^{(l)} \cdot \phi(\hat{\mu}&lt;em&gt;{i}, \hat{\mu}&lt;/em&gt;{j}; \Theta_{k}^{(l)}) + b_{p}^{(l)}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Above, $y_{j,q}^{(l)}$ is the $q$-th input feature of neighboring node $j$, $w_{p,q,k}^{(l)}$ is the linear weight assigned to this feature for the $k$-th kernel, and $\phi(\hat{\mu}&lt;em&gt;{i}, \hat{\mu}&lt;/em&gt;{j}; \Theta_{k}^{(l)})$ is the $k$-th kernel weight between node $i$ and node $j$, defined as:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\phi(\hat{\mu_{i}}, \hat{\mu_{j}}; \sigma_{k}^{(l)}, \mu_{k}^{(l)}  ) = \exp^{-\sigma_{k}^{(l)} \left\Vert (\hat{\mu_{i}} - \hat{\mu_{j}}) - \mu_{k}^{(l)} \right\Vert^{2}}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Extrinsically, the kernel weights are represented by edges in a sparse affinity matrix, such that index $(i,j)$ is the Gaussian kernel weight between node $i$ and node $j$ for the $k$-th kernel in the $l$-th layer, where nodes $j$ are restricted to be within a certain neighborhood or distance of node $i$.  This can be seen more clearly here:&lt;/p&gt;















&lt;figure id=&#34;figure-figure-from-wu-et-alhttpswwwncbinlmnihgovpmcarticlespmc7052684--v_i-is-our-voxel-of-interest-and-v_ki-for-demonstration-purposes-is-an-adjacent-node--both-v_i-and-v_ki-are-characterized-by-embedding-vectors-e_i-e_ki-in-mathbbrq-from-which-we-compute-the-kernel-weight-phi_ik-characterizing-how-similar-the-two-vertices-embedding-vectors-are&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./gaussian_radius.png&#34; data-caption=&#34;Figure from 
Wu et al..  $v_{i}$ is our voxel of interest, and $v_{k}^{i}$, for demonstration purposes, is an adjacent node.  Both $v_{i}$ and $v_{k}^{i}$ are characterized by embedding vectors $e_{i}, e_{k}^{i} \in \mathbb{R}^{q}$, from which we compute the kernel weight $\phi_{i,k}$ characterizing how similar the two vertices&amp;rsquo; embedding vectors are.&#34;&gt;


  &lt;img src=&#34;./gaussian_radius.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure from 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7052684/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wu et al.&lt;/a&gt;.  $v_{i}$ is our voxel of interest, and $v_{k}^{i}$, for demonstration purposes, is an adjacent node.  Both $v_{i}$ and $v_{k}^{i}$ are characterized by embedding vectors $e_{i}, e_{k}^{i} \in \mathbb{R}^{q}$, from which we compute the kernel weight $\phi_{i,k}$ characterizing how similar the two vertices&amp;rsquo; embedding vectors are.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;I implemented a new convolutional layer called &lt;code&gt;GAUSConv&lt;/code&gt; (available 
&lt;a href=&#34;https://github.com/kristianeschenburg/parcellearning/blob/master/parcellearning/layers/gausconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).  To implement this algorithm, I utilized the 
&lt;a href=&#34;https://www.dgl.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Graph Library&lt;/a&gt; (DGL), which offers a stellar single unifed API based on message passing (I&amp;rsquo;m using 
&lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pytorch&lt;/a&gt; as the backend).  I noticed that I could formulate this problem using attention mechanisms described in the 
&lt;a href=&#34;https://arxiv.org/abs/1710.10903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Attention Network&lt;/a&gt; paper &amp;ndash; however, instead of computing attention weights using a fully connected layer as described in that work, I would compute kernel weights using Gaussian filters.  Similarly, just as the GAT paper describes &lt;em&gt;multi-head attention&lt;/em&gt; for multiple attention channels, I could analogize my fomulation to &lt;em&gt;multi-head kernels&lt;/em&gt; for multiple kernel channels.  To this end, I could make use of the 
&lt;a href=&#34;https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/gatconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;GATConv&lt;/code&gt;&lt;/a&gt; API quite easily by replacing the attention computations with the Gaussian kernel filtrations.  Likewise, I utilized the 
&lt;a href=&#34;https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/graphconv.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;GraphConv&lt;/code&gt;&lt;/a&gt; API to incorporate linear weights from the 
&lt;a href=&#34;https://arxiv.org/pdf/1609.02907.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Convolution Network&lt;/a&gt; paper.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;GAUSConv&lt;/code&gt; layer is similar to both the &lt;code&gt;GraphConv&lt;/code&gt; and &lt;code&gt;GATConv&lt;/code&gt; layers but differs in a few places.  Rather than initializing the layer with attention heads, we initialize it with the number of kernels and a kernel dropout probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
GAUSConv(in_feats, # number of input dimensions
         out_feats, # number of output features
         num_kernels, # number of kernels for current layer
         feat_drop=0., # dropout probability of features
         kernel_drop=0., # dropout probability of kernels
         negative_slope=0.2, # leakly relu slope
         activation=None, # activation function to apply after forward pass
         random_seed=None, # for example / reproducibility purposes
         allow_zero_in_degree=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Importantly, in the layer instantiation, we define &lt;strong&gt;linear weights&lt;/strong&gt; &lt;em&gt;and&lt;/em&gt; &lt;strong&gt;kernel mean and sigma parameters&lt;/strong&gt;, &lt;code&gt;mu&lt;/code&gt; and &lt;code&gt;sigma&lt;/code&gt;.  We initialize both kernel parameters with the flag &lt;code&gt;require_grad=True&lt;/code&gt;, which enables us to update these kernel parameters during the backward pass of the layer.  Both parameters are initialized with values in the &lt;code&gt;reset_parameters&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# initialize feature weights and bias vector
self.weights = nn.Parameter(
    th.Tensor(num_kernels, in_feats, out_feats), requires_grad=True)
self.bias = nn.Parameter(
    th.Tensor(num_kernels, out_feats), requires_grad=True)

# initialize kernel perameters
self.mu = nn.Parameter(
    th.Tensor(1, num_kernels, in_feats), requires_grad=True)
self.sigma = nn.Parameter(
    th.Tensor(num_kernels, 1), requires_grad=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now here is the clever part, and where the 
&lt;a href=&#34;https://docs.dgl.ai/en/0.4.x/api/python/function.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DGL message passing interface&lt;/a&gt; really shines through.  DGL fuses the &lt;code&gt;send&lt;/code&gt; and &lt;code&gt;receive&lt;/code&gt; messages so that no messages between nodes are ever explicitly stored, using built-in &lt;strong&gt;message&lt;/strong&gt; and &lt;strong&gt;reduce&lt;/strong&gt; functions.  To compute the kernel weights between all pairs of source and destrination nodes, we use these built-in functions.  The important steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;compute node feature differences between all source / destination node pairs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;aggregate and reduce incoming messages from destination nodes scaled by the kernel weights, to update the source node features&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the forward pass of our layer, we perform the following steps:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;### forward pass of GAUSConv layer ###

# compute all pairwise differences between adjacent node features
graph.ndata[&#39;h&#39;] = feat
graph.apply_edges(fn.u_sub_v(&#39;h&#39;, &#39;h&#39;, &#39;diff&#39;))

# compute kernel weights for each source / desintation pair
e = graph.edata[&#39;diff&#39;].unsqueeze(1) - mu
e = -1*sigma*th.norm(e, dim=2).unsqueeze(2)
e = e.exp()
graph.edata[&#39;e&#39;] = e

# apply kernel weights to destination node features
graph.apply_edges(fn.v_mul_e(&#39;h&#39;, &#39;e&#39;, &#39;kw&#39;))

# apply linear projection to kernel-weighted destination node features
a = th.sum(th.matmul(graph.edata[&#39;kw&#39;].transpose(1, 0), weights), dim=0)

# apply kernel dropout
a = self.kernel_drop(a)
graph.edata[&#39;a&#39;] = a

# final message-passing and reduction step
# aggregate weighted destination node features to update source node features
graph.update_all(fn.copy_e(&#39;a&#39;, &#39;m&#39;), fn.sum(&#39;m&#39;, &#39;h&#39;))
rst = graph.ndata[&#39;h&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, given a graph and features, we instantiate a &lt;code&gt;GAUSConv&lt;/code&gt; layer and propogate our features through the network via:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# set random seed
random_seed=1

# define arbitrary input/output feature shape
n_samples = 4
in_feats=4
out_feats=2
features = th.ones(n_samples, in_feats)

# define number of kernels
num_kernels=2

# create graph structure
u, v = th.tensor([0, 0, 0, 1]), th.tensor([1, 2, 3, 3])
g = dgl.graph((u, v))
g = dgl.to_bidirected(g)
g = dgl.add_self_loop(g)

# instantiate layer
GausConv = GAUSConv(in_feats=in_feats,
                    out_feats=out_feats,
                    random_seed=random_seed,
                    num_kernels=num_kernels,
                    feat_drop=0,
                    kernel_drop=0)
                    
# forward pass of layer
logits = GausConv(g, features)

print(logits)
tensor([[0.1873, 0.7217],
        [0.1405, 0.5413],
        [0.0936, 0.3608],
        [0.1405, 0.5413]], grad_fn=&amp;lt;AddBackward0&amp;gt;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Distances Between Subspaces</title>
      <link>/post/comparing-subspaces/</link>
      <pubDate>Wed, 24 Jun 2020 15:27:11 -0700</pubDate>
      <guid>/post/comparing-subspaces/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m working with some multi-dimensional float-valued data &amp;ndash; I&amp;rsquo;ll call a single instance of this data $X \in \mathbb{R}^{n \times k}$.  I have multiple samples $X_{1}, X_{2}&amp;hellip;X_{t}$, and want to compare these subspaces &amp;ndash; namely, I want to compute the distance between pairs of subspaces.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s assume that our subspaces are not rank-deficient &amp;ndash; i.e. for a given subspace sample, all of our dimensions are linearly independent.  Thus, the $k$ vectors form a basis set that spans some $k$-d subspace in $\mathbb{R}^{n}$.  We can think of each $k$-d subspace as a hyperplane in $(k+1)$-d space, just as we can think of a 2-d plane in 3-d space.  One way to compare these subspaces is by using the &amp;ldquo;principle angles between subspaces&amp;rdquo; (or 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Angles_between_flats&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;angles between flats&lt;/a&gt;).  We can compare the &amp;ldquo;angles&amp;rdquo; between these hyperplanes, which will tell us how &amp;ldquo;far apart&amp;rdquo; the two subspaces are.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-intersecting-2d-linear-subspaceshttpswwwresearchgatenetpublication327930102_optimal_exploitation_of_subspace_prior_information_in_matrix_sensing&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/comparing-subspaces/subspaces_hub081a28bf258255fb45bfa4cda66f295_17202_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Intersecting 2D linear 
subspaces.&#34;&gt;


  &lt;img data-src=&#34;/post/comparing-subspaces/subspaces_hub081a28bf258255fb45bfa4cda66f295_17202_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;640&#34; height=&#34;483&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Intersecting 2D linear 
&lt;a href=&#34;https://www.researchgate.net/publication/327930102_Optimal_Exploitation_of_Subspace_Prior_Information_in_Matrix_Sensing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;subspaces&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This comparison is effectively based on the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/QR_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QR decomposition&lt;/a&gt; and the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Singular Value Decomposition&lt;/a&gt;.  For two subspaces $[U, W]$, we compute the QR decomposition of both:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
U &amp;amp;= Q_{u}R_{u}\\
W &amp;amp;= Q_{w}R_{w}\\
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $Q_{u}$ and $Q_{w} \in \mathbb{R}^{n \times k}$ are orthonormal bases such that $Q_{u}^{T}Q_{u} = Q_{w}^{T}Q_{w} = I_{k}$ that span the same subspace as the original columns of $U$ and $W$, and $R_{u}$ and $R_{w} \in \mathbb{R}^{k \times k}$ are lower triangular matrices.  Next, we compute the matrix $D = \langle Q_{u}, Q_{w} \rangle = Q_{u}^{T} Q_{w} \in \mathbb{R}^{k \times k}$, and then apply the singular value decomposition:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
D = USV^{T}
\end{align}$$&lt;/p&gt;
&lt;p&gt;We can sort of think of $D$ as the cross-covariance matrix.  As such, the singular vectors represent the main orthogonal axes of cross-covariation between our two subspaces, while the singular values represent angles.  In order to compute the principle angles of our subspaces, we simply take&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\theta &amp;amp;= cos^{-1}(S) \\
&amp;amp;=cos^{-1}[\sigma_{1}, \sigma_{2}&amp;hellip;\sigma_{k}]
\end{align}$$&lt;/p&gt;
&lt;p&gt;which gives us the principle angles (in radians).  Because the SVD is invariant to sign (+/-), the principle angles range between $\Big[0, \frac{\pi}{2}\Big]$.  This means that subspaces that span the same space have a principle angle of 0, and subspaces that are orthogonal (maximally far apart) to one another have a principle angle of $\frac{\pi}{2}$.&lt;/p&gt;
&lt;p&gt;In order to compute the &amp;ldquo;distance&amp;rdquo; between our subspaces, we can apply 
&lt;a href=&#34;https://galton.uchicago.edu/~lekheng/work/schubert.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;various metrics&lt;/a&gt; to our vector of principle angles.  The simplest approach is to apply the $L2$ norm to our vector of principle angles, $\theta$, as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
d(X_{i}, X_{j}) = \sqrt{\sum_{n=1}^{k} cos^{-1}(\sigma_{n})^{2}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;This metric is called the 
&lt;a href=&#34;http://www.eeci-institute.eu/GSC2011/Photos-EECI/EECI-GSC-2011-M5/book_AMS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grassmann Distance&lt;/a&gt; and is formally related to the geodesic distance between subspaces distributed on the Grassmannian manifold.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-grassmann-manifold-and-its-tangent-spacehttpsdeepaiorgpublicationautomatic-recognition-of-space-time-constellations-by-learning-on-the-grassmann-manifold-extended-version&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/comparing-subspaces/grassmann_hu2cd3f58cc8fbe46f24a9a6bd61a80bd2_41873_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Grassmann manifold and its 
tangent space.&#34;&gt;


  &lt;img data-src=&#34;/post/comparing-subspaces/grassmann_hu2cd3f58cc8fbe46f24a9a6bd61a80bd2_41873_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;398&#34; height=&#34;249&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Grassmann manifold and its 
&lt;a href=&#34;https://deepai.org/publication/automatic-recognition-of-space-time-constellations-by-learning-on-the-grassmann-manifold-extended-version&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tangent space&lt;/a&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This, however, is a topic for another future blog post.  There are a variety of metrics we can use to compute the pairwise distance between subspaces, some of which are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Asimov: $\; max(\theta)$&lt;/li&gt;
&lt;li&gt;Fubini-Study:  $\; cos^{-1}(\prod sin(\theta))$&lt;/li&gt;
&lt;li&gt;Spectral:  $\; 2 sin(\frac{max(\theta)}{2})$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;but all are fundamentally based on some function of our vector of principle angles, $\theta$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lab Meeting: pip and the Python Packaging Index</title>
      <link>/post/pyni-packages/</link>
      <pubDate>Sun, 07 Jun 2020 10:20:09 -0700</pubDate>
      <guid>/post/pyni-packages/</guid>
      <description>&lt;p&gt;What follows are the contents of part of a lab meeting presentation I gave recently.  The topic of the meeting was &amp;ldquo;Python for Neuroimaging&amp;rdquo;, where I covered basic software development tools that brain imaging scientists might be interested in.&lt;/p&gt;
&lt;h1 id=&#34;creating-python-packages&#34;&gt;Creating Python Packages&lt;/h1&gt;
&lt;p&gt;In this lesson, I&amp;rsquo;ll show you how to build your own Python package that you can then install locally or upload to the 
&lt;a href=&#34;https://pip.pypa.io/en/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python Packaging Index&lt;/a&gt; (for those of you familar with 
&lt;a href=&#34;https://www.r-project.org/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R&lt;/a&gt;, think 
&lt;a href=&#34;https://cran.r-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRAN&lt;/a&gt;, but for Python).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m going to be basing a lot of the material off of this 
&lt;a href=&#34;https://packaging.python.org/tutorials/packaging-projects/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;, but will also show a real example using some of my own personal code.&lt;/p&gt;
&lt;h2 id=&#34;what-are-packages&#34;&gt;What are packages?&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m sure most of you are familiar with packages and libraries already, either from Matlab, R, or Python.  Packages are basically bundles of various snippets of code, i.e. &lt;strong&gt;methods&lt;/strong&gt;, &lt;strong&gt;classes&lt;/strong&gt;, &lt;strong&gt;scripts&lt;/strong&gt;, &lt;strong&gt;tests&lt;/strong&gt; etc. that are bundled together to perform some function.  Generally (hopefully), there is coherence to what these snippets of code do &amp;ndash; they should interact together in some way or relate to some overarching computational goal.&lt;/p&gt;
&lt;p&gt;Within a package, you can have different groupings of code, where each grouping does some unique or discrete computing.  These groupings are called &lt;strong&gt;submodules&lt;/strong&gt;.  A common submodule in many packages is an &lt;strong&gt;Input / Output (io)&lt;/strong&gt; module that will read and write data that this package interacts with or produces.  Another common submodule is often related to &lt;strong&gt;plotting&lt;/strong&gt; the outputs of your code.  And then almost always, there are submodules that perform the brunt of the algorithmic work.  So inside modules, you&amp;rsquo;ll find snippets of code that relate to the goal or concept of the module.&lt;/p&gt;
&lt;p&gt;Think of a package as a &lt;em&gt;toolbox&lt;/em&gt; with a bunch of drawers, each with a label: &lt;em&gt;wood-working&lt;/em&gt;, &lt;em&gt;welding&lt;/em&gt;, &lt;em&gt;gardening&lt;/em&gt;, &lt;em&gt;flooring&lt;/em&gt;, etc.  These drawers are submodules.  You can tell by their names that they each cover certain topics.  Each drawer contains a set of tools:  &lt;em&gt;woord-working&lt;/em&gt; might contain &lt;em&gt;saw&lt;/em&gt;, &lt;em&gt;nail&lt;/em&gt;, &lt;em&gt;sandpaper&lt;/em&gt;, &lt;em&gt;wood glue&lt;/em&gt;, while &lt;em&gt;welding&lt;/em&gt; might containg &lt;em&gt;solder&lt;/em&gt;, &lt;em&gt;flux&lt;/em&gt;, &lt;em&gt;oxygen&lt;/em&gt;, &lt;em&gt;glove&lt;/em&gt;.  These tools are the functions, classes, and scripts that relate to that submodule.&lt;/p&gt;
&lt;p&gt;Overall, this toolbox performs some stuff related to construction, homebuilding, repair, and has discrete bundles of code useful for a variety of those tasks.&lt;/p&gt;
&lt;h3 id=&#34;directory-structure-for-a-python-package&#34;&gt;Directory structure for a Python package&lt;/h3&gt;
&lt;p&gt;Here we examine the skeleton of a package.  All packages follow this basic structure.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pkg_name
|-- __init__.py
|-- LICENSE
|-- pkg_name/
|    |-- submodule_a/
|    |    |-- __init__.py
|    |    |-- a_1.py
|    |-- submodule_b/
|    |    |-- __init__.py
|    |    |-- b_1.py
|    |    |-- b_2.py
|-- README.md
|-- setup.py
|-- test/
|    |-- __init__.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;__init__.py&lt;/code&gt; is a required file that allows your package to be imported.  The only &lt;code&gt;__init__.py&lt;/code&gt; file that needs to contain anything is the highest-level file. The others can be empty, but they must exist.  Here are the contents of the highest-level &lt;code&gt;__init__.py&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__all__ = [
    &#39;a_1&#39;, &#39;b_1&#39;, &#39;b_2&#39;
]

from .submodule_a import (a_1)
from .submodule_b import (b_1, b_2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;LICENSE&lt;/code&gt; tells other users / individuals in what capacity they are allowed to use your code.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;README.md&lt;/code&gt; describes how to use your code, and often contains examples.  This is a &lt;strong&gt;markdown&lt;/strong&gt; file, but can generally be any type of &lt;strong&gt;markup&lt;/strong&gt; language.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;test/&lt;/code&gt; is a directory in which you would want to write 
&lt;a href=&#34;http://softwaretestingfundamentals.com/unit-testing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unit tests&lt;/a&gt; for your code.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;setup.py&lt;/code&gt; is what allows your to install your package.  It&amp;rsquo;s a set of instructions that get supplied to 
&lt;a href=&#34;https://setuptools.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;setuptools&lt;/a&gt; package.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from setuptools import setup, find_packages

with open(&amp;quot;README.md&amp;quot;, &amp;quot;r&amp;quot;) as fh:
    long_description = fh.read()

 setup(
   name=&#39;pkg_name&#39;,
   version=&#39;0.1.0&#39;,
   author=&#39;Kristian M. Eschenburg&#39;,
   author_email=&#39;keschenb@uw.edu&#39;,
   packages=find_packages(),
   scripts=[],
   url=&amp;quot;https://github.com/kristianeschenburg/pkg_name&amp;quot;,
   license=&#39;LICENSE.txt&#39;,
   description=&#39;An awesome package that does something&#39;,
   long_description=long_description,
   install_requires=[
       &amp;quot;numpy&amp;quot;
       &amp;quot;pytest&amp;quot;,
       &amp;quot;matplotlib&amp;quot;,
   ],
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;compiling-installing-and-uploading-your-package&#34;&gt;Compiling, installing, and uploading your package&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Register on PyPi&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once we&amp;rsquo;ve done all this, we&amp;rsquo;re just about ready to create our Python package and upload it to 
&lt;a href=&#34;https://pypi.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pypi.org&lt;/a&gt;.  But first, we need to create an account.  For testing purposes, we&amp;rsquo;ll create a test account 
&lt;a href=&#34;https://test.pypi.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, but the process is the same.&lt;/p&gt;
&lt;p&gt;After you create your account, we need to create an API token, that will allow us to upload files to either 
&lt;a href=&#34;https://test.pypi.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Test PyPi&lt;/a&gt; or 
&lt;a href=&#34;https://pypi.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyPi&lt;/a&gt; (depending on what we&amp;rsquo;re doing) &amp;ndash; the following steps are the same, regardless.&lt;/p&gt;
&lt;p&gt;Under your Test PyPi account, click your username in the top right, go to &lt;code&gt;Account Settings&lt;/code&gt;:&lt;/p&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;notebook_figures/packages/PyPi_Account.png&#34; &gt;


  &lt;img src=&#34;notebook_figures/packages/PyPi_Account.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Scroll down and click &lt;code&gt;Add API Token&lt;/code&gt;:&lt;/p&gt;

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;figure &gt;
 
 
   &lt;a data-fancybox=&#34;&#34; href=&#34;notebook_figures/packages/PyPi_Token.png&#34; &gt;
 
 
   &lt;img src=&#34;notebook_figures/packages/PyPi_Token.png&#34; alt=&#34;&#34;  &gt;
 &lt;/a&gt;
 
 
 
 &lt;/figure&gt;

&lt;p&gt;Follow the instructions there, making sure to select &amp;ldquo;Entire Account&amp;rdquo; option under the &lt;code&gt;Scope&lt;/code&gt; tab.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DO NOT CLOSE THIS WINDOW WHEN THIS IS COMPLETE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Next, type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd $HOME
touch .pypirc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and using your favorite text editor, enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;[testpypi]
  repository: https://test.pypi.org/legacy/
  username = __token__
  password = pypi-***
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we were creating a token for PyPi, we&amp;rsquo;d type:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;[pypi]
  repository: https://pypi.org/
  username = __token__
  password = pypi-***
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Close your current terminal, and open a new window to refresh your settings.  Now, when we go to upload our package to PyPi, we&amp;rsquo;ll be able to type the commands without needed to supply a username and password directly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Compile your package&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we need to make sure that a few Python packages are installed.  Namely, we need to install 
&lt;a href=&#34;https://pip.pypa.io/en/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pip&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
python get-pip.py
pip install -U pip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can install the following packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade pip setuptools wheel # for installing Python packages
pip install tqdm # progress bar package
pip install --user --upgrade twine # for publishing to PyPi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we can compile our package:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python setup.py bdist_wheel
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which creates the directories &lt;code&gt;dist&lt;/code&gt;, &lt;code&gt;build&lt;/code&gt;, and &lt;code&gt;pkg_name.egg-info&lt;/code&gt;.  The &lt;code&gt;*.egg-info&lt;/code&gt; file is basically some zipped meta-data about your package, but we&amp;rsquo;re really only interested in the &lt;code&gt;*.whl&lt;/code&gt; file in &lt;code&gt;dist&lt;/code&gt; &amp;ndash; &amp;ldquo;wheels&amp;rdquo; are a &amp;ldquo;distribution&amp;rdquo; format, newly designed to replace &amp;ldquo;eggs&amp;rdquo;.  I won&amp;rsquo;t go into it here, but &lt;strong&gt;eggs&lt;/strong&gt; were sort an &lt;em&gt;ad hoc&lt;/em&gt; solution to packaging Python code &amp;ndash; &lt;strong&gt;wheels&lt;/strong&gt; were part of 
&lt;a href=&#34;https://www.python.org/dev/peps/pep-0427/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PEP427&lt;/a&gt; i.e. is actually an &amp;ldquo;enhancement&amp;rdquo; to the Python language, and the formal way of packaging Python code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Installing your code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can install our code locally with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install dist/pkg_name-0.0.0-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to install an &amp;ldquo;editable&amp;rdquo; version of your package, do this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will allow you to change your &lt;code&gt;*.py&lt;/code&gt; files and have these changes take effect immediately when importing your package, without needing to rebuild each time &amp;ndash; but this method installs from the &lt;strong&gt;egg&lt;/strong&gt; distribution, and generally produces larger build files, since the build needs to keep track of your actual source code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Upload your code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can upload our code to PyPi now using the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m twine upload --repository testpypi dist/*
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, if you click &amp;ldquo;Your Projects&amp;rdquo; under your account name on PyPi, you&amp;rsquo;ll see that you project has been uploaded.&lt;/p&gt;
&lt;p&gt;** I should note that, any time you want to upgrade your code and upload it to PyPi again, you need to remove all files from the &lt;code&gt;dist&lt;/code&gt; directory, increment the &lt;code&gt;version&lt;/code&gt; number in the &lt;code&gt;setup.py&lt;/code&gt; file &amp;ndash; i.e. 0.0.0 &amp;ndash;&amp;gt; 0.0.1 &amp;ndash; rebuild your package with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bash python setup.py bdist_wheel
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;example-with-personal-package&#34;&gt;Example with personal package&lt;/h3&gt;
&lt;p&gt;I don&amp;rsquo;t generally upload my code to PyPi (probably scared bugs in the code, and people finding them, and then thinking I&amp;rsquo;m terrible at software development, and going down a long spiral of self-deprecation, but I digress&amp;hellip;) but I do upload it all to GitHub.  In either case, here is a walk-through of packaging some software called &lt;code&gt;pysurface&lt;/code&gt; that I use for processing mesh-based data &amp;ndash; I use it for adjacency matrices, performing Laplacian smoothing on surfaces, sampling points from mesh triangle simplices, plotting on surfaces&amp;hellip;  Just some stuff that I find myself doing a lot.&lt;/p&gt;
&lt;p&gt;Here is the directory containing all my code:&lt;/p&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;notebook_figures/packages/pysurface_code.png&#34; &gt;


  &lt;img src=&#34;notebook_figures/packages/pysurface_code.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;You&amp;rsquo;ll see 5 different modules: &lt;code&gt;graphs&lt;/code&gt;, &lt;code&gt;operations&lt;/code&gt;, &lt;code&gt;plotting&lt;/code&gt;, &lt;code&gt;spectra&lt;/code&gt;, and &lt;code&gt;utilities&lt;/code&gt;, and you&amp;rsquo;ll note that each module directory has a &lt;code&gt;__init__.py&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Here is my &lt;code&gt;setup.py&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from os import path
from setuptools import setup, find_packages
import sys

here = path.abspath(path.dirname(__file__))

with open(path.join(here, &#39;README.rst&#39;), encoding=&#39;utf-8&#39;) as readme_file:
    readme = readme_file.read()

with open(path.join(here, &#39;requirements.txt&#39;)) as requirements_file:
    # Parse requirements.txt, ignoring any commented-out lines.
    requirements = [line for line in requirements_file.read().splitlines()
                    if not line.startswith(&#39;#&#39;)]


setup(
    name=&#39;pysurface&#39;,
    version=&amp;quot;0.0.4&amp;quot;,
    description=&amp;quot;Python package for quickly processing surface meshes.&amp;quot;,
    long_description=readme,
    author=&amp;quot;Kristian Eschenburg&amp;quot;,
    author_email=&#39;keschenb@uw.edu&#39;,
    url=&#39;https://github.com/kristianeschenburg/pysurface&#39;,
    packages=find_packages(),
    entry_points={
        &#39;console_scripts&#39;: [
            # &#39;some.module:some_function&#39;,
            ],
        },
    include_package_data=True,
    package_data={
        &#39;pysurface&#39;: [
            # When adding files here, remember to update MANIFEST.in as well,
            # or else they will not be included in the distribution on PyPI!
            # &#39;path/to/data_file&#39;,
            ]
        },
    install_requires=requirements,
    license=&amp;quot;BSD (3-clause)&amp;quot;,
    classifiers=[
        &#39;Development Status :: 2 - Pre-Alpha&#39;,
        &#39;Natural Language :: English&#39;,
        &#39;Programming Language :: Python :: 3&#39;,
    ],
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that I&amp;rsquo;ve run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python setup.py bdist_wheel
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;based off the &lt;code&gt;dist&lt;/code&gt;, &lt;code&gt;build&lt;/code&gt;, and &lt;code&gt;pysurface.egg-info&lt;/code&gt; directories.  &lt;code&gt;dist&lt;/code&gt; contains a file called &lt;code&gt;pysurface-0.0.4-py3-none.any.whl&lt;/code&gt;, which is the actual distribution that can be used for installation.  I&amp;rsquo;ve uploaded the code to Test PyPi, and this is what we see:&lt;/p&gt;















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;notebook_figures/packages/PyPi_Project.png&#34; &gt;


  &lt;img src=&#34;notebook_figures/packages/PyPi_Project.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We can then install the package and all of it&amp;rsquo;s dependencies from TestPypi via&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --index-url https://test.pypi.org/simple/ pysurface
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can do something like the following in a Python script:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pysurface
# or
from pysurface import graphs, spectra
# or
from pysurface.spectra import eigenspectrum
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Submitting Batch Jobs with qsub</title>
      <link>/post/submitting-batch-jobs-with-qsub/</link>
      <pubDate>Tue, 05 May 2020 14:24:17 -0700</pubDate>
      <guid>/post/submitting-batch-jobs-with-qsub/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m fortunate enought to work in a lab with some high-level computing infrastructure.  We have a cluster of machines using the 
&lt;a href=&#34;http://bioinformatics.mdc-berlin.de/intro2UnixandSGE/sun_grid_engine_for_beginners/README.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sun Grid Engine&lt;/a&gt; (SGE) software system for distributed resource management.  The other day, I was searching for how to wrap my Python scripts with &lt;code&gt;qsub&lt;/code&gt; so that I could submit a batch of jobs to our cluster.  Eventually, I want to be able to submit jobs with dependencies between them, but we&amp;rsquo;ll start here.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s create an example script that computes the mean of an MRI image.  Let&amp;rsquo;s call the script &lt;code&gt;compute_mean.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#!/usr/bin/env python

import argparse
import nibabel as nb
import numpy as np
import pandas as pd


parser = ArgumentParser(&#39;Compute the mean of MRI, and save to CSV file.&#39;)
parser.add_argument(&#39;-i&#39;, &#39;--input_image&#39;, help=&#39;Path to MRI image.&#39;,
    required=True, type=str)
parser.add_argument(&#39;-o&#39;, &#39;--output_csv&#39;, help=&#39;Output CSV file.&#39;,
    required=True, type=str)

args = parser.parse_args()

# read in image
img = nb.load(args.input_image)
# get voxel-wise data
data = img.get_data()

# compute mean
mu = np.mean(data)

# save to csv
df = pd.DataFrame({&#39;mean&#39;: [mu]})
df.to_csv(args.output_csv)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line of this script, &lt;code&gt;#!/usr/bin/env python&lt;/code&gt; tells the script to use the local &lt;code&gt;python&lt;/code&gt; environment.  In my case, I have a customized installation of Python, along with a bunch packages and libraries that I&amp;rsquo;ve written and installed that are not available for the rest of my lab (since they&amp;rsquo;re still in the testing phase or just something I&amp;rsquo;m experimenting with).  This line tells the script to use &lt;em&gt;my&lt;/em&gt; Python environment, rather than the default version on our servers.&lt;/p&gt;
&lt;p&gt;We can then create a bash wrapper, let&amp;rsquo;s call in &lt;code&gt;mean_wrapper.sh&lt;/code&gt; for a single subject&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
#$ -M keschenb@uw.edu
#$ -m abe
#$ -r y
#$ -o tmp.out
#$ -e tmp.err

# Compute mean of image
image=$1
output=$2

python compute_mean.py ${image} ${output}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second and third line here, with the &lt;code&gt;M&lt;/code&gt; and &lt;code&gt;m&lt;/code&gt; parameters, tell the script to email me once it completes the processing (or if there are any errors).  And finally, we can create a wrapper that takes in a list of subjects to process, and the input and output directories, and submits each individual job to the queue using &lt;code&gt;qsub&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

subjects=$1
image_dir=$2
output_dir=$3

# we create a variable, as our cluster has 2 different queues to use
# this could be hardcoded though
queue_name=$4 

while read subj
do

    image_file=${image_dir}${subj}.nii.gz
    output_file=${output_dir}${subj}.csv

    qsub -q ${queue_name}.q mean_wrapper.sh ${input_image} ${output_file}

done &amp;lt;${subjects}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here&amp;rsquo;s an example output from running &lt;code&gt;qstat&lt;/code&gt; after submitting a batch of jobs to the cluster:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-example-of-qstat-command-after-submitting-jobs-view-qsub&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/submitting-batch-jobs-with-qsub/qsub_huaabaffbba137fe798b5631086ab7846a_122046_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Example of qstat command, after submitting jobs view qsub.&#34;&gt;


  &lt;img data-src=&#34;/post/submitting-batch-jobs-with-qsub/qsub_huaabaffbba137fe798b5631086ab7846a_122046_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;799&#34; height=&#34;323&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Example of qstat command, after submitting jobs view qsub.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;One thing you&amp;rsquo;ll notice is the column &lt;code&gt;priority&lt;/code&gt; &amp;ndash; this is literally a &lt;code&gt;priority queue&lt;/code&gt; data structure that I mentioned in my last post on the Watershed by Flooding algorithm.  Each job is submitted to the queue with a priority value assigned to it by the SGE software, and the jobs are processed in that order &amp;ndash; highest priority first, lowest priority last.  Your IT manager can personalize the priority values for specific users or types of jobs, such that they are given preference or moved back in line.  This represents an equitable way of distributing compute resources across users in a lab, generally using a first-come, first-serve basis, or restricting users to a certain number of nodes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Watershed by Flooding: Applied Data Structures</title>
      <link>/post/watershed-by-flooding-applied-data-structures/</link>
      <pubDate>Fri, 01 May 2020 11:12:32 -0700</pubDate>
      <guid>/post/watershed-by-flooding-applied-data-structures/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m applying some methods developed in 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4677978/pdf/bhu239.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper&lt;/a&gt; for testing purposes in my own thesis research.  Specifically, I have some float-valued data, $F$, that varies along the cortical surface of the brain.  Visually, I can see that there are areas where these scalar maps change abruplty.  I want to identify this boundary(s) &amp;ndash; eventually, I&amp;rsquo;ll segment out the regions I&amp;rsquo;m interested in.&lt;/p&gt;
&lt;h3 id=&#34;computing-the-gradient-map&#34;&gt;Computing the Gradient Map&lt;/h3&gt;
&lt;p&gt;The authors use some conventional brain imaging software to compute the gradient of their data.  The domain of this data is a triangulated mesh, described by the graph $G = (V, E)$, where $V$ are vertices in Euclidean space and $E$ the edges between these vertices.  In short, for a vertex, $v_{i}$, we first need to compute the gradient vector of the scalar field at $v_{i}$.  We &amp;ldquo;unfold&amp;rdquo; the 3D positions of adjacent vertices onto the tangent plane of $v_{i}$ &amp;ndash; which we can do by orthogonally projecting the adjacent vertices onto the affine subspace at $v_{i}$ and then weighting appropriately.  We then regress the graph signal (our scalar field) onto these unfolded positions.  The $L_{2}$ norm of this vector is the gradient at $v_{i}$.&lt;/p&gt;
&lt;p&gt;For each vertex, we have a normal vector to the surface $N$, its spatial 3D coordinates $v_{i} = (x, y, z)$, and a list of its adjacent vertices.  We can compute the orthogonal projector onto affine subspace spanned by $N$, $P_{N}$, and it&amp;rsquo;s orthogonal complement, $Q_{N}$, as:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
P_{N} &amp;amp;= N(N^{T}N)^{-1}N^{T} \\
Q_{N} &amp;amp;= I - P_{N}
\end{align}$$&lt;/p&gt;
&lt;p&gt;For any vertex, $v_{j}$, we can compute the orthogonal projection onto the affine subspace spanned by $Q_{N}$ as:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
q(v_{j}) = Q_{N}(v_{j} - v_{i}) + v_{i}
\end{align}$$&lt;/p&gt;
&lt;p&gt;We generate the vectors&lt;/p&gt;
&lt;p&gt;$$
S_{i} = f(i) -
\begin{bmatrix}
f(v_{1}) \\
f(v_{2}) \\
\vdots \\
f(v_{j})
\end{bmatrix}
\in \mathbb{R}^{j} \;\;\;
R_{i} =
\begin{bmatrix}
q(v_{1}) \\
q(v_{2}) \\
\vdots \\
q(v_{j})
\end{bmatrix} \in \mathbb{R}^{j \times 3}
$$&lt;/p&gt;
&lt;p&gt;where $S_{i}$ is the difference between the scalar value at our vertex $f(v_{i})$ and the vector of adjacent $j$ scalar field values, and $R_{i}$ is the matrix of $j$ orthogonally projected adjacent vertex coordinates.  Then we perform least squares regression to solve for $\beta$:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
S_{i} = R_{i}\beta
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $\beta \in \mathbb{R}^{3}$, which indicates how much each coordinate axis contributes to variation in the scalar field at $v_{i}$.  The gradient value at vertex $v_{i} = \left || \beta \right||_{2}$.&lt;/p&gt;
&lt;h3 id=&#34;watershed-by-flooding-algorithm&#34;&gt;Watershed By Flooding Algorithm&lt;/h3&gt;
&lt;p&gt;The new scalar field of $L_{2}$ norms is our gradient field, which describes how &amp;ldquo;quickly&amp;rdquo; our original data changes at each vertex.  We can now apply the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Watershed_%28image_processing%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Watershed Algorithm&lt;/a&gt; to segment our mesh data.  In brief, the watershed algorithm treats the gradient field as a &lt;em&gt;topographic map&lt;/em&gt;: low-elevation areas (areas with a small gradient) are &amp;ldquo;water basins&amp;rdquo;.  If we imagine water flooding this map from the bottom up, basins at low elevation will flood first, while areas at higher elevations will fill last.  When water basins meet, the water has reached a &amp;ldquo;boundary&amp;rdquo; (or ridgeline, if we&amp;rsquo;re using the topographic map idea).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve implemented an algorithm variant called 
&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0098300418307957&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Priority Flooding&amp;rdquo;&lt;/a&gt;, using Python&amp;rsquo;s 
&lt;a href=&#34;https://docs.python.org/2/library/heapq.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;heapq&lt;/a&gt; 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Priority_queue&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;priority queue&lt;/a&gt; data type class.  The priority queue is an application of the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Binary_heap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;binary heap&lt;/a&gt; data structure &amp;ndash; as nodes are added to the heap, the branching process determines where to put nodes (left or right of a current node), based on some value &amp;ndash; in the case of the priority queue, this value is the &amp;ldquo;priority&amp;rdquo;.  We utilize the priority queue because it gives us a principled way to iterate over unlabeled vertices, and, with some auxilliary data structures, is guaranteed to converge.  The algorithm proceeds as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Identify local minima, and assign each minima a unique label&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add directly adjacent vertices of local minima to priority queue (lower gradient -&amp;gt; higher priority)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;While the queue is not empty, get the highest-priority item&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If vertices adjacent to this vertex have only one label, assign this vertex to that label&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Else assign this vertex as a boundary vertex&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add unlabed adjacent vertices to the queue&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from queue import PriorityQueue

class PriorityFlood(object):

    &amp;quot;&amp;quot;&amp;quot;
    Class to segment a scalar field using the Priority Flooding
    watershed algorithm.
    &amp;quot;&amp;quot;&amp;quot;

    def __init__(self):

        self.pq = PriorityQueue()

    def fit(self, gradient, A, M):

        &amp;quot;&amp;quot;&amp;quot;
        Fitting procedure for watershed algorithm.

        Parameters:
        - - - - -
        gradient: float, array
            map of gradient values of scalar field
        A: dict of lists
            adjacency list of data domain
        M: list
            local minima in scalar field used to seed algorithm
        &amp;quot;&amp;quot;&amp;quot;

        # initialize empty label vector
        labels = np.zeros((gradient.shape))
        labels[:] = np.nan

        # keep track of items in queue
        in_queue = np.zeros((gradient.shape)).astype(np.bool)

        # assign local minima unique labels
        # add their adjacent vertices to heap
        for i, loc_min in enumerate(M):

            # label local minima
            labels[loc_min] = i+1

            # add neighbors of local minima to p-queue
            for nidx in A[loc_min]:
                in_queue[nidx] = True
                self.pq.put((gradient[nidx], nidx))

        # iterate over p-queue items
        # items assigned to a label or
        # assigned as a boundary
        while not self.pq.empty():

            # get highest priority item
            # mark as not in p-queue
            [gr, idx] = self.pq.get()
            in_queue[idx] = False

            # get item neighbors and their labels
            i_neighbors = np.asarray(A[idx])
            i_nlabels = labels[i_neighbors]

            # get labels of adjacent vertices that are not nan
            nans = np.isnan(i_nlabels)
            unique_labels = np.unique(i_nlabels[~nans])

            # if more than one unique label
            # assign current vertex as border vertex
            if len(unique_labels) &amp;gt; 1:
                continue

            # otherwise assign to water basin
            else:

                # basin assignment
                labels[idx] = unique_labels

                # identify neighbors without labels that arent in the p-queue
                gidx = np.where((nans) &amp;amp; (~in_queue[i_neighbors]))[0]

                # add these to p-queue
                for nidx in i_neighbors[gidx]:
                    if not in_queue[nidx]:
                        in_queue[nidx] = True
                        self.pq.put((gradient[nidx], nidx))

        self.labels_ = labels

&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-example-of-priorityflooding-algorithm-applied-to-gradient-of-inferiorparietal-region--shown-on-inflated-and-flattened-cortical-surfaces&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/watershed-by-flooding-applied-data-structures/WSA_hu0337061d81dd53d2945002c9b9cfc50e_191463_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Example of PriorityFlooding algorithm, applied to gradient of inferiorparietal region.  Shown on inflated and flattened cortical surfaces.&#34;&gt;


  &lt;img data-src=&#34;/post/watershed-by-flooding-applied-data-structures/WSA_hu0337061d81dd53d2945002c9b9cfc50e_191463_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1280&#34; height=&#34;803&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Example of PriorityFlooding algorithm, applied to gradient of inferiorparietal region.  Shown on inflated and flattened cortical surfaces.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;One caveat that came up is that the &lt;strong&gt;PriorityQueue&lt;/strong&gt; class does not check for duplicates &amp;ndash; that is, two vertices might share an adjacent vertex, and this vertex might have already been added to the heap.  In this case, we would unnecessarily view the same vertices many times.  To alleviate this, we create a boolean Numpy array, &lt;code&gt;in_queue&lt;/code&gt;, that stores whether an item is already in the queue.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dose-Response Curves and Biomarker Diagnostic Power</title>
      <link>/post/dose-response-curves-and-biomarker-diagnostic-power/</link>
      <pubDate>Tue, 27 Aug 2019 11:12:32 -0700</pubDate>
      <guid>/post/dose-response-curves-and-biomarker-diagnostic-power/</guid>
      <description>&lt;p&gt;The other day, one of my friends and colleagues (I&amp;rsquo;ll refer to him as &amp;ldquo;Dr. A&amp;rdquo;) asked me if I knew anything about assessing biomarker diagnostic power.  He went on to describe his clinical problem, which I&amp;rsquo;ll try to recant here (but will likely mess up some of the relevant detail &amp;ndash; his research pertains to generating induced pluripotent cardiac stem cells, which I have little to no experience with):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;So chemotherapy is meant to combat cancer.  But some clinicians, and their patients, have found that some forms of chemotherapy and anti-cancer drugs later result in problems with the heart and vasculature &amp;ndash; these problems are collectively referred to as &amp;lsquo;cardiotoxicity&amp;rsquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;We&amp;rsquo;re interested in developing biomarkers that will help us identify which patients might be susceptible to cardiotoxicity, and in assessing the predictive power of these biomarkers.  Can you help me?&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What follows will be my exploration into what Dr. A called 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Dose%E2%80%93response_relationship&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dose-response curves&lt;/a&gt;, and my approach on how to use these curves to assess biomarker diagnostic power.  I&amp;rsquo;ll do a walk through of some Python code that I&amp;rsquo;ve written up, where I&amp;rsquo;ll examine dose-response curves and their diagnostic power using the &lt;strong&gt;receiver operating characteristic&lt;/strong&gt; (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROC&lt;/a&gt;) and the closely related &lt;strong&gt;area under the curve&lt;/strong&gt; (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AUC&lt;/a&gt;) metrics.&lt;/p&gt;
&lt;p&gt;If you want to see the actual dose-response curve analysis, skip to the &lt;strong&gt;Analyzing Synthetic Dose-Response Curves&lt;/strong&gt; section down below.&lt;/p&gt;
&lt;h3 id=&#34;brief-introduction-to-the-roc-and-auc&#34;&gt;Brief Introduction to the ROC and AUC&lt;/h3&gt;
&lt;p&gt;I first wanted to describe to Dr. A how to use the ROC for biomarkers, so I made a brief Python tutorial for him to look at.  Below begins a more in-depth description of what I sent him.&lt;/p&gt;
&lt;p&gt;We start by importing some necessary libraries:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

import numpy as np
from sklearn.metrics import auc
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we define a function to compute the ROC curve.  While 
&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit-learn&lt;/a&gt; has a function to compute the ROC, computing it yourself makes it easier to understand what the ROC curve actually represents.&lt;/p&gt;
&lt;p&gt;The ROC curve is essentially a curve that plots the true positive and false positive classification rates against one another, for various user-defined thresholds of the data.  We pick a biomarker level threshold &amp;ndash; let&amp;rsquo;s say $T$ &amp;ndash; assign each sample a value of 0 or 1 depending on whether its measured biomarker level is greater than or less than $T$, and then compare these assignments to the &lt;em&gt;true&lt;/em&gt; Case/Control classifications to compute the true and false positive rates.  We plots these rates for many $T$ to generate the ROC curve.&lt;/p&gt;
&lt;p&gt;The ROC let&amp;rsquo;s you look at how sensitive your model is to various parameterizations &amp;ndash; are you able to accurately identify Cases from Controls?  How acceptable are misclassified results?  In this situation, we don&amp;rsquo;t want to classify someone as healthy when in fact they might develop cardiotoxicity, so we want to allow some flexibility in terms of the number of false positives generated by our model &amp;ndash; it&amp;rsquo;s a classic case of &amp;ldquo;better safe than sorry&amp;rdquo;, since the &amp;ldquo;sorry&amp;rdquo; outcome might be an accidental patient death.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def roc(case, control, npoints, gte=True):
    
    &amp;quot;&amp;quot;&amp;quot;
    Compute ROC curve for given set of Case/Control samples.
    
    Parameters:
    - - - - -
    case: float, array
        Samples from case patients
    control: float, array
        Samples from control patients
    npoints: int
        Number of TP/FP pairs to generate
    gte: boolean
        Whether control mean expected to be greater
        than case mean.

    Returns:
    - - - -
    specificity: float, array
        false positive rates
    sensitivity: float, array
        true positive rates
    &amp;quot;&amp;quot;&amp;quot;

    # make sure generating more
    # than 1 TP/FP pair
    # so we can plot an actual ROC curve
    assert npoints &amp;gt; 1
    

    # made sure case and control samples
    # are numpy arrays
    case = np.asarray(case)
    control = np.asarray(control)
    
    # check for NaN values
    # keep only indices without NaN
    case_nans = np.isnan(case)
    cont_nans = np.isnan(control)
    nans = (case_nans + cont_nans)
    
    specificity = []
    sensitivity = []
    
    # we&#39;ll define the min and max thresholds 
    # based on the min and max of our data
    conc = np.concatenate([case[~nans],control[~nans]])

    # function comparison map
    # use ```gte``` parameter
    # if we expect controls to be less than
    # cases, gte = False
    # otherwise gte = True
    comp_map = {&#39;False&#39;: np.less,
                &#39;True&#39;: np.greater}
    
    # generate npoints equally spaced threshold values
    # compute the false positive / true positive rates 
    # at each threshold
    for thresh in np.linspace(conc.min(), conc.max(), npoints):

        fp = (comp_map[gte](case[~nans], thresh)).mean()
        tn = 1-fp
        
        tp = (comp_map[gte](control[~nans], thresh)).mean()
        fn = 1-tp
        
        specificity.append(tn)
        sensitivity.append(tp)
    
    return [np.asarray(specificity), np.asarray(sensitivity)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up, I generate 5 different datasets.  Each dataset corresponds to fake samples from a &amp;ldquo;Control&amp;rdquo; distribution, and a &amp;ldquo;Case&amp;rdquo; distribution &amp;ndash; each is distributed according to a univariate Normal distribution.  The Control distribution remains the same in each scenario: $N(\mu = 10, \sigma = 1)$, but I change the $\mu_{Case}$ parameter of the Case distribution in each instance, such that $\mu_{Case} \in [5,6,7,8,9,10]$.&lt;/p&gt;
&lt;p&gt;We plot the example datasets as follows, and then compute the ROC curves for each dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# define unique means
m1 = np.arange(5,11)
samples = {}.fromkeys(m1)

# define control distribution (this stays the same across 
n2 = np.random.normal(loc=10, scale=1, size=2000)

fig, (ax) = plt.subplots(2, 3, figsize=(15, 6))

for i, ax in enumerate(fig.axes):

    n1 = np.random.normal(loc=m1[i], scale=1, size=2000)
    samples[m1[i]] = n1

    ax.hist(n1, 25, alpha=0.5, label=&#39;Case&#39;, density=True,)
    ax.hist(n2, 25, alpha=0.5, label=&#39;Control&#39;, density=True,)
    ax.set_title(&#39;Mean: {:}, Sigma: 1&#39;.format(m1[i]), fontsize=15)
    ax.set_xlabel(&#39;Biomarker Measures Variable&#39;, fontsize=15)
    ax.set_ylabel(&#39;Density&#39;, fontsize=15)
    ax.legend()

plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-example-casecontrol-datasets&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/Example.Histograms_hu3fd4354cc4807317c36f9db23aa54a3a_131982_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Example Case/Control datasets.&#34;&gt;


  &lt;img data-src=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/Example.Histograms_hu3fd4354cc4807317c36f9db23aa54a3a_131982_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1080&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Example Case/Control datasets.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, (ax1) = plt.subplots(1, 1)
for mean, case_data in samples.items():

    [spec, sens] = roc(case_data, n2, 100, gte=True)
    A = auc(spec, sens)

    L = &#39;Mean: %i, AUC: %.3f&#39; % (mean, A)
    plt.plot(1-np.asarray(spec), np.asarray(sens), label=L)
    plt.legend(bbox_to_anchor=(1.04,1), fontsize=15)
    plt.xlabel(&#39;1-Specificity&#39;, fontsize=15)
    plt.ylabel(&#39;Sensitivity&#39;, fontsize=15)

    plt.title(&#39;ROC for Varying Case Distributions&#39;, fontsize=15)
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-roc-curves-for-example-datasets&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/Example.ROC.Curves_hub481d6f71bf7ebea89899b0a7048eba2_73909_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;ROC curves for example datasets.&#34;&gt;


  &lt;img data-src=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/Example.ROC.Curves_hub481d6f71bf7ebea89899b0a7048eba2_73909_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ROC curves for example datasets.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We see that, as the distributions become less separated, the ability to distinguish points from either distribution is diminished.  This is shown by 1) a flattening of the ROC curve towards the diagonal, along with 2) the integration of the ROC curve, which generates the AUC metric.  When the distributions are far apart (as when the $\mu_{Case} = 5$), it is quite easy for a simple model to distinguish points sampled from either distribution, meaning this hypothetical model has good diagnostic power.&lt;/p&gt;
&lt;h3 id=&#34;analyzing-synthetic-dose-response-curves&#34;&gt;Analyzing Synthetic Dose-Response Curves&lt;/h3&gt;
&lt;p&gt;We now analyze some synthetic data, generated to look like dose-response curves.  As a refresher, dose-response curves measure the behavior of some tissue cells in response to increasing levels of (generally) drugs.  The $x$-axis is the drug dose, measured in some concentration or volume, and the $y$-axis is generally some measure of cell death or survival, generally ranging from 0% to 100%.  The curves, however, look sigmoidal.  So let&amp;rsquo;s first generate a function to create sigmoid curves.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sigmoid(beta, intercept, x):
    
    &amp;quot;&amp;quot;&amp;quot;
    Fake sigmoid function, takes in coefficient, shift, and dose values.
    
    Parameters:
    - - - - -
    beta: float
        slope
    intercept: float
        negative exponential intercept
    x: float, array
        data samples
        
    Returns:
    - - - -
    dose_response: float, array
        single-subject dose-response vector
        Between 0 and 1.
    &amp;quot;&amp;quot;&amp;quot;
    
    dose_resonse =  1 / (1 + np.exp(-beta*x + intercept))
    
    return dose_resonse
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let&amp;rsquo;s actually generate some synethetic data for a dataset of fake subjects.  I want to incorporate some variability into the Cases and Controls, so I&amp;rsquo;ll sample the subject parameters from distributions.  In this case, for each subject, I&amp;rsquo;ll sample the logistic curve slope coefficient from $Beta$ distributions, and the intercept from $Normal$ distributions.  We&amp;rsquo;ll sample 1000 Cases, and 1000 Controls.&lt;/p&gt;
&lt;p&gt;For the slopes, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\beta_{Control} &amp;amp;\sim Beta(a=5, b=3) \
\beta_{Case} &amp;amp;\sim Beta(a=10, b=2)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;and for the intercepts, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
I_{Control} &amp;amp;\sim Normal(\mu=0, \sigma=1) \
I_{Case} &amp;amp;\sim Normal(\mu=4, \sigma=1)
\end{align}
$$&lt;/p&gt;
&lt;p&gt;As such, the dose-response curve for individual, $k$, is generated as follows:&lt;/p&gt;
&lt;p&gt;$$
dr_{k} = \frac{1}{1 + e^{-(\beta_{k}X + I_{k})}}
$$&lt;/p&gt;
&lt;p&gt;where $\beta_{k}$ and $I_{k}$ are the slope and intercept values for the given subject.  These distributional parameterizations are arbitrary &amp;ndash; I just wanted to be able to incorporate variability across subjects and groups.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s generate some random Case/Control dose-response data and plot the coefficient histograms:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# n cases and controls
S = 1000

# dictionary of slopes and intercept values for each subject
controls = {k: {&#39;beta&#39;: None,
                &#39;intercept&#39;: None} for k in np.arange(S)}
cases = {k: {&#39;beta&#39;: None,
             &#39;intercept&#39;: None} for k in np.arange(S)}

# get lists of betas and intercepts
beta_control = []
beta_case = []

intercept_control = []
intercept_case = []

for i in np.arange(S):

    controls[i][&#39;beta&#39;] = np.random.beta(a=5, b=3)
    controls[i][&#39;intercept&#39;] = np.random.normal(loc=0, scale=1)

    intercept_control.append(controls[i][&#39;intercept&#39;])
    beta_control.append(controls[i][&#39;beta&#39;])

    cases[i][&#39;beta&#39;] = np.random.beta(a=10, b=2)
    cases[i][&#39;intercept&#39;] = np.random.normal(loc=4, scale=1)

    intercept_case.append(cases[i][&#39;intercept&#39;])
    beta_case.append(cases[i][&#39;beta&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Intercept histograms look like two different $$Normal$$ distributions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure(figsize=(12, 8))
plt.hist(intercept_control, 50, color=&#39;r&#39;, alpha=0.5, density=True, label=&#39;Control&#39;);
plt.hist(intercept_case, 50, color=&#39;b&#39;, alpha=0.5, density=True, label=&#39;Case&#39;);
plt.ylabel(&#39;Density&#39;, fontsize=15);
plt.xlabel(&#39;Intercepts&#39;, fontsize=15);
plt.title(&#39;Intercepts Coefficients By Group&#39;, fontsize=15);
plt.legend(fontsize=15);
plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-intercept-distributions-for-synthetic-dose-response-curves&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.Intercepts_hua52c747286aa8e02ed6e6f3f9284fd3e_50701_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Intercept distributions for synthetic dose-response curves.&#34;&gt;


  &lt;img data-src=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.Intercepts_hua52c747286aa8e02ed6e6f3f9284fd3e_50701_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;576&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Intercept distributions for synthetic dose-response curves.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Slope histograms look like two different $Beta$ distributions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure(figsize=(12, 8))
plt.hist(beta_control, 50, color=&#39;r&#39;, alpha=0.5, label=&#39;Control&#39;, density=True);
plt.hist(beta_case, 50, color=&#39;b&#39;, alpha=0.5, label=&#39;Case&#39;, density=True);
plt.ylabel(&#39;Density&#39;, fontsize=15);
plt.xlabel(&#39;Betas&#39;, fontsize=15);
plt.title(&#39;Slope Coefficients By Group&#39;, fontsize=15);
plt.legend(fontsize=15);
plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-slope-distributions-for-synthetic-dose-response-curves&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.Slopes_hu1357c4832cc06dad7fe19891c932d38e_54436_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Slope distributions for synthetic dose-response curves.&#34;&gt;


  &lt;img data-src=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.Slopes_hu1357c4832cc06dad7fe19891c932d38e_54436_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;576&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Slope distributions for synthetic dose-response curves.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Now we&amp;rsquo;ll generate some fake dose-response curves for each of the 1000 Controls, and 1000 Cases.  We&amp;rsquo;ll plot a subset of these curves to visualize our cross-group curve variability.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# define synthetic dose range
doses = np.linspace(-15,15,500)
dose_min = doses.min()
shifted_dose = doses + np.abs(dose_min)

fig, (ax1) = plt.subplots(figsize=(12, 8))

ec50_control = []
ec50_case = []

for c in np.arange(S):

    control_sample = sigmoid(controls[c][&#39;beta&#39;], controls[c][&#39;intercept&#39;], doses)
    case_sample = sigmoid(cases[c][&#39;beta&#39;], cases[c][&#39;intercept&#39;], doses)

    ec50_control.append(shifted_dose[control_sample &amp;lt; 0.5].max())
    ec50_case.append(shifted_dose[case_sample &amp;lt; 0.5].max())

    if (c % 15) == 0:

        ax1.plot(shifted_dose, control_sample, c=&#39;r&#39;, linewidth=3, alpha=0.3)
        ax1.plot(shifted_dose, case_sample, c=&#39;b&#39;, linewidth=3, alpha=0.3)

plt.legend({})
plt.title(&#39;Dose Response Curve&#39;, fontsize=20);
plt.xlabel(&#39;Dose&#39;, fontsize=20);
plt.xticks(fontsize=15)
plt.ylabel(&#39;Response&#39;, fontsize=20);
plt.yticks(fontsize=15)

custom_lines = [Line2D([0], [0], color=&#39;r&#39;, lw=4),
                Line2D([0], [0], color=&#39;b&#39;, lw=4)]
plt.legend(custom_lines, [&#39;Control&#39;, &#39;Case&#39;], fontsize=20);
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-casecontrol-dose-response-curves&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.Curves_hube552fddd3db2e1c8078160f744ce767_111292_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Case/Control dose-response curves.&#34;&gt;


  &lt;img data-src=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.Curves_hube552fddd3db2e1c8078160f744ce767_111292_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;576&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Case/Control dose-response curves.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;For our preliminary biomarker of interest, let&amp;rsquo;s look at the &lt;strong&gt;ec50&lt;/strong&gt;, which is the dose at which &lt;em&gt;50%&lt;/em&gt; of the cells show some response (i.e. where our $y$-axis = 0.5), for each sample in our dataset.  We&amp;rsquo;ll plot these doses as a function of Cases and Controls.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure(figsize=(12, 8))
plt.hist(ec50_control, 50, color=&#39;r&#39;, alpha=0.5, label=&#39;Control&#39;, density=True);
plt.hist(ec50_case, 50, color=&#39;b&#39;, alpha=0.5, label=&#39;Case&#39;, density=True);
plt.legend(fontsize=20);
plt.xlabel(&#39;ec50&#39;, fontsize=20);
plt.xticks(fontsize=15);
plt.ylabel(&#39;Density&#39;, fontsize=20);
plt.yticks(fontsize=15);
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-biomarker-distributions-ec50-for-cases-and-controls&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.ec50_hu708c2993441f485c03fd9143c770b8b4_45795_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Biomarker distributions: ec50 for Cases and Controls.&#34;&gt;


  &lt;img data-src=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.ec50_hu708c2993441f485c03fd9143c770b8b4_45795_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;576&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Biomarker distributions: ec50 for Cases and Controls.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;If we select a different threshold &amp;ndash; i.e. instead of 0.5, we can iterate over the range of 0.1 - 0.9, for example, in increments of 0.1 &amp;ndash; we generate different biomarkers (ec10, ec20 &amp;hellip; ec90).  We can treat each biomarker as a different classification model, and assess how powerful that model is at assessing whether someone will develop cardiotoxicity or not.  To do so, we&amp;rsquo;ll create distributions for each biomarker (not shown), and then generate ROC curves and AUC values for each curve.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-roc-curves-and-auc-for-each-ec-x-biomarker-level&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.ecCurves.All_hu55950f119f4e6eab959a42f36f20836c_76398_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;ROC curves and AUC for each ec-X biomarker level.&#34;&gt;


  &lt;img data-src=&#34;/post/dose-response-curves-and-biomarker-diagnostic-power/DR.ecCurves.All_hu55950f119f4e6eab959a42f36f20836c_76398_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;576&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ROC curves and AUC for each ec-X biomarker level.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This is where my limited domain knowledge comes at a cost &amp;ndash; I&amp;rsquo;m not sure if the biomarkers I&amp;rsquo;ve chosen (i.e. incremental &lt;strong&gt;ec&lt;/strong&gt; values) are actually biologically relevant.  The point, however, is that each biomarker yields a different AUC, which theoretically shows that the Cases and Controls can be differentially distinguished, depending on which biomarker we choose to examine.  In this case, &lt;strong&gt;ec10&lt;/strong&gt; has the most discriminative diagnostic power.&lt;/p&gt;
&lt;p&gt;Something I did wonder about while exploring this data was how dependent the ROC curves and AUC statistics are on sample size.  Previously, I&amp;rsquo;d looked at rates of convergence of various estimators &amp;ndash; the AUC should also theoretically show some convergence to a &amp;ldquo;true&amp;rdquo; value as $n$ increases &amp;ndash; but I&amp;rsquo;m not sure if it follows any sort of relevant distribution.  I imagine the AUC is domain-dependent, in that it depends on the distribution of the biomarker of interest?  Might be a good idea for another post&amp;hellip;&lt;/p&gt;
&lt;p&gt;Cheers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Initializing Lists in Python With Prespecified Size</title>
      <link>/post/initializing-lists-in-python-with-prespecified-size/</link>
      <pubDate>Wed, 21 Aug 2019 01:12:32 -0700</pubDate>
      <guid>/post/initializing-lists-in-python-with-prespecified-size/</guid>
      <description>&lt;p&gt;I wanted to make a quick note about something I found incredibly helpful the other day.&lt;/p&gt;
&lt;p&gt;Lists (or ArrayLists, as new Computer Science students are often taught in their CS 101 courses), as a data strucure are fundamentally based on arrays, but with additional methods associated with them.  Lists are generally filled with an &lt;code&gt;append&lt;/code&gt; method, that fills indices in this array sequentially.  Lists are often useful in the case where the number of intial spots that will be filled is unknown, or if you&amp;rsquo;re working with many objects of different types.&lt;/p&gt;
&lt;p&gt;The base arrays are generally associated with a &lt;code&gt;size&lt;/code&gt; or &lt;code&gt;length&lt;/code&gt; parameter, that initializes the array to a certain length.  Under the hood (and generally hidden from the user), however, the &lt;code&gt;List&lt;/code&gt; class also has a &lt;code&gt;resize&lt;/code&gt; method that adds available space to the array when a certain percentage of available indices are occupied, technically allowing the size of the list to grow when more space is needed.&lt;/p&gt;
&lt;p&gt;Perpetually applying &lt;code&gt;resize&lt;/code&gt; becomes slow in the case when you&amp;rsquo;re appending a lot of items.  All of the data currently in the &lt;code&gt;List&lt;/code&gt; object will need to be moved into the new, resized array.&lt;/p&gt;
&lt;p&gt;I needed to aggregate a large number (couple thousand) of Pandas DataFrame objects, each saved as a single file, into a single DataFrame.  My first thought was to simply incrementally load and append all incoming DataFrames to a list, and then use &lt;code&gt;pandas.concat&lt;/code&gt; to aggregate them all together.  Appending all of these DataFrames together became pretty time consuming (at this point, I remembered the &lt;code&gt;resize&lt;/code&gt; issue).&lt;/p&gt;
&lt;p&gt;A quick Google search led me to the following solution, allowing me to predefine how large I wanted my list to be:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# For simplicity assume we have 10 items
known_size = 10
initialized_list = [None]*known_size

print(len(initialized_list))
10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Neat, huh?  And ridiculously simple.  Now, rather than append, we can do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for j, temp_file in enumerate(list_of_files):
    loaded_file = load_file(temp_file)
    initialized_list[j] = loaded_file
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because the &lt;strong&gt;memory has already been pre-allocated&lt;/strong&gt;, the &lt;code&gt;resize&lt;/code&gt; method is never accessed, and we save time.  I also found 
&lt;a href=&#34;http://zwmiller.com/blogs/python_data_structure_speed.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this blog post&lt;/a&gt; with some information about timing with regards to Numpy arrays, lists, and tuples &amp;ndash; the author shows that indexing into a Numpy array is actually slower than indexing into a list.  Numpy arrays are primarilly useful in the case where operations can be vectorized &amp;ndash; then they&amp;rsquo;re the clear winners in terms of speed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Delta Method</title>
      <link>/post/the-delta-method/</link>
      <pubDate>Tue, 19 Mar 2019 12:43:32 -0700</pubDate>
      <guid>/post/the-delta-method/</guid>
      <description>&lt;p&gt;Here, we&amp;rsquo;ll look at various applications of the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Delta Method&lt;/a&gt;, especially in the context of variance stabilizing transformations, along with looking at the confidence intervals of estimates.&lt;/p&gt;
&lt;p&gt;The Delta Method is used as a way to approximate the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Standard_error&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Standard Error&lt;/a&gt; of transformations of random variables, and is based on a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Taylor_series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Taylor Series&lt;/a&gt; approximation.&lt;/p&gt;
&lt;p&gt;In the univariate case, if we have a random variable, $X_{n}$, that converges in distribution to a $N(0, \sigma^{2})$ distribution, we can apply a function to this random variable as:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\sqrt{n}(X_{n} - \theta) \xrightarrow{d} N(0,\sigma^{2}) \\
\sqrt{n}(g(X_{n}) - g(\theta)) \xrightarrow{d} ; ?
\end{align}$$&lt;/p&gt;
&lt;p&gt;However, we don&amp;rsquo;t know the asymptotic variance of this transformed variable just yet.  In this case, we can approximate our function $g(x)$ using a Taylor Series approximation, evaluated at $\theta$:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
g(x) = g(\theta) + g\prime(\theta)(x-\theta) + O()
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $O()$ is the remainder of higher-order Taylor Series terms that converges to 0.&lt;/p&gt;
&lt;p&gt;By 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Slutsky%27s_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slutsky&amp;rsquo;s Theorem&lt;/a&gt; and the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Continuous_mapping_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Continious Mapping Theorem&lt;/a&gt;, we know that since $\bar{\theta} \xrightarrow{p} \theta$, we know that $g\prime(\bar{\theta}) \xrightarrow{p} g\prime(\theta)$&lt;/p&gt;
&lt;p&gt;Plugging this back in to our original equation and applying Slutsky&amp;rsquo;s Perturbation Theorem, we have:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;= \sqrt{n}(\Big[g(\theta) + g\prime(\theta)(x-\theta)\Big] - g(\theta)) \\
&amp;amp;= \sqrt{n}(g\prime(\theta)(x-\theta)) \\
&amp;amp;= g\prime(\theta)\sqrt{n}(X_{n} - \theta)
\end{align}$$&lt;/p&gt;
&lt;p&gt;and since we know that $\sqrt{n}(\bar{X_{n}} - \theta)  \xrightarrow{d} N(0,\sigma^{2})$, we now know that $g\prime(\theta) \sqrt{n}(\bar{X_{n}} - \theta) \xrightarrow{d} N(0,g\prime(\theta)^{2} \sigma^{2})$.  As such, we have that:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\sqrt{n}(g(X_{n}) - g(\theta)) \xrightarrow{d} N(0, g\prime(\theta)^{2}\sigma^{2})
\end{align}$$&lt;/p&gt;
&lt;p&gt;The Delta Method can be generalized to the multivariate case, where, instead of the derivative, we use the gradient vector of our function:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\sqrt{n}(g(\bar{X_{n}} - g(\theta)) \xrightarrow{d} N(0, \nabla(g)^{T} \Sigma \nabla(g))
\end{align}$$&lt;/p&gt;
&lt;p&gt;Below, I&amp;rsquo;m going to look at a few examples applying the Delta Method to simple functions of random variables.  Then I&amp;rsquo;ll go into more involved examples applying the Delta Method via 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Variance-stabilizing_transformation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Variance Stabilizing Transformations&lt;/a&gt;.  Oftentimes, the variance of an estimate depends on its mean, which can vary with the sample size.  In this case, we&amp;rsquo;d like to find a function $g(\theta)$, such that, when applied via the Delta Method, the variance is constant as a function of the sample size.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll start by importing the necessary libraries and defining two functions:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import matplotlib.pyplot as plt

from matplotlib import rc
rc(&#39;text&#39;, usetex=True)

from scipy.stats import norm, poisson, expon
import numpy as np
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we define two simple functions &amp;ndash; one to compute the difference between our estimate and its population paramter, and the other to compute the function of our random variable as described by the Central Limit Theorem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def conv_prob(n, est, pop):
    
    &amp;quot;&amp;quot;&amp;quot;
    Method to compute the estimate for convergence in probability.
    &amp;quot;&amp;quot;&amp;quot;
    
    return (est-pop)

def clt(n, est, pop):
    
    &amp;quot;&amp;quot;&amp;quot;
    Method to examine the Central Limit Theorem.
    &amp;quot;&amp;quot;&amp;quot;
    
    return np.sqrt(n)*(est-pop)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at an easy example with the Normal Distribution.  We&amp;rsquo;ll set $\mu = 0$ and $\sigma^{2} = 5$.  Remember that when using the &lt;code&gt;Scipy&lt;/code&gt; Normal distribution, the &lt;code&gt;norm&lt;/code&gt; class accepts the &lt;strong&gt;standard deviation&lt;/strong&gt;, not the variance.  We&amp;rsquo;ll show via the Central Limit Theorem that the function $\sqrt{n}(\bar{X_{n}} - \mu) \xrightarrow{d} N(0,\sigma^{2})$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# set sample sample sizes, and number of sampling iterations
N = [5,10,50,100,500,1000]
iters = 500

mu = 0; sigma = np.sqrt(5)

# store estimates
norm_clt = {n: [] for n in N}

samples = norm(mu,sigma).rvs(size=(iters,1000))

for n in N:
    for i in np.arange(iters):
        
        est_norm = np.mean(samples[i,0:n])
        norm_clt[n].append(clt(n, est_norm, mu))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let&amp;rsquo;s plot the results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot results using violin plots
fig = plt.subplots(figsize=(8,5))

for i,n in enumerate(N):
    temp = norm_clt[n]
    m = np.mean(temp)
    v = np.var(temp)
    print(&#39;Sample Size: %i has empirical variance: %.2f&#39; % (n, v.mean()))
        
    plt.violinplot(norm_clt[n], positions=[i],)
&lt;/code&gt;&lt;/pre&gt;















&lt;figure id=&#34;figure-central-limit-theorem-applied-to-normal-distribution&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;Normal_CLT.jpg&#34; data-caption=&#34;Central Limit Theorem applied to Normal Distribution.&#34;&gt;


  &lt;img src=&#34;Normal_CLT.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Central Limit Theorem applied to Normal Distribution.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As expected, we see that the Normal distribution mean and variance estimates are independent of the sample size.  In this case, we don&amp;rsquo;t need to apply a variance stabiliing transformation.  We also see that the variance fluctuates around $5$.  Now, let&amp;rsquo;s apply a simple function $g(\theta) = \theta^{2}$ to our data.  So $g\prime(\theta) = 2\theta$, and the variance of our function becomes $g\prime(\mu)^{2}\sigma^{2} = (2\mu)^{2} \sigma^{2} = 4\mu^{2}\sigma^{2}$.  Let&amp;rsquo;s look at a few plots, as a function of changing $\mu$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# set sample sample sizes, and number of sampling iterations
mus = [1,2,3,4]

N = [5,10,50,100,500,1000]
iters = 2000
sigma = np.sqrt(5)


fig, ([ax1,ax2], [ax3,ax4]) = plt.subplots(2,2, figsize=(14,9))
for j ,m in enumerate(mus):
    
    # store estimates
    norm_clt = {n: [] for n in N}
    samples = norm(m, sigma).rvs(size=(iters, 1000))
    
    
    plt.subplot(2,2,j+1)
    for k, n in enumerate(N):
        np.random.shuffle(samples)
        for i in np.arange(iters):

            est_norm = np.mean(samples[i, 0:n])
            norm_clt[n].append(clt(n, est_norm**2, m**2))

        plt.violinplot(norm_clt[n], positions=[k],)
&lt;/code&gt;&lt;/pre&gt;















&lt;figure id=&#34;figure-central-limit-theorem-applied-to-function-of-normal-distribution&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;Normal_Squared.jpg&#34; data-caption=&#34;Central Limit Theorem applied to function of Normal Distribution.&#34;&gt;


  &lt;img src=&#34;Normal_Squared.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Central Limit Theorem applied to function of Normal Distribution.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We see that the variance increases as the mean increases, and that, as the sample sizes increase, the distributions converge to the $$N(0, 4\mu^{2}\sigma^{2})$$ asymptotic distribution.&lt;/p&gt;
&lt;h4 id=&#34;variance-stabilization-for-the-poisson-distribution&#34;&gt;Variance Stabilization for the Poisson Distribution&lt;/h4&gt;
&lt;p&gt;Now let&amp;rsquo;s look at an example where the variance depends on the sample size.  We&amp;rsquo;ll use the Poisson distribution in this case.  We know that for the Poisson distribution, the variance is dependent on the mean, so let&amp;rsquo;s define a random variable, $X_{\lambda}$, where $\lambda = n*\theta$.  $n$ is the sample size, and $\theta$ is a fixed constant.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll define $ X_{\lambda } = \sum_{i=1}^{n} X_{\theta}$, the sum of $n$ independent Poisson random variables, so that the expected value and variance of $X_{\lambda } = n\theta$&lt;/p&gt;
&lt;p&gt;If we wanted to apply the Central Limit Theorem to $X_{\lambda }$, our convergence would be as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\sqrt{n}(X_{\lambda} - \lambda) \xrightarrow{d} N(0,\sigma^{2}(\lambda))
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the variance $\sigma^{2}(\lambda)$ depends on the mean, $\lambda$.  In order to stabilize the variance of this variable, we can apply the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Delta Method&lt;/a&gt;, in order to generate a variable that converges to a standard Normal distribution asymptotically.&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\sqrt{n}(g(X_{\lambda}) - g(\lambda)) \xrightarrow{d} N(0,g\prime(\theta)^{2}\sigma^{2}) \
\end{align}$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;g\prime(\theta)^{2} \theta = 1 \\
&amp;amp;g\prime(\theta)^{2} = \frac{1}{\theta} \\
&amp;amp;g\prime(\theta) = \frac{1}{\sqrt{\theta}} \\
&amp;amp;g(\theta) = \int \frac{\partial{\theta}}{\sqrt{\theta}} \\
&amp;amp;g(\theta) = 2\sqrt{\theta}
\end{align}$$&lt;/p&gt;
&lt;p&gt;is our variance stabilizing function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def p_lambda(n, theta=0.5):
    
    &amp;quot;&amp;quot;&amp;quot;
    Function to compute lambda parameter for Poisson distribution.
    Theta is constant.
    &amp;quot;&amp;quot;&amp;quot;
    return n*theta
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;theta = 0.5

N = [5,10,50,100,250,500,750,1000]
iters = 500

clt_pois = {n: [] for n in N}
pois_novar= {n: [] for n in N}
pois_var = {n: [] for n in N}

for n in N:
    for i in np.arange(iters):
        est_mu = np.mean(poisson(mu=(n*theta)).rvs(n))

        pois_novar[n].append(clt(n, est_mu, p_lambda(n)))
        pois_var[n].append(clt(n, 2*np.sqrt(est_mu), 2*np.sqrt(p_lambda(n))))
        
        clt_pois[n].append(conv_prob(n, est_mu, n*theta))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig,([ax1, ax2]) = plt.subplots(2,1, figsize=(15,6))

plt.subplot(1,2,1)
for i,n in enumerate(N):
    plt.violinplot(pois_novar[n], positions=[i])

plt.subplot(1,2,2)
for i,n in enumerate(N):
    plt.violinplot(pois_var[n], positions=[i])
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-variance-stabilization-of-poisson-distribution&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/the-delta-method/Poisson_hu5e3c49a7388a40f7d11c771dd1270a61_74713_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Variance stabilization of Poisson distribution.&#34;&gt;


  &lt;img data-src=&#34;/post/the-delta-method/Poisson_hu5e3c49a7388a40f7d11c771dd1270a61_74713_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1080&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Variance stabilization of Poisson distribution.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;variance-stabilization-for-the-exponential-distribution&#34;&gt;Variance Stabilization for the Exponential Distribution&lt;/h4&gt;
&lt;p&gt;Applying the same method to the Exponential distribtuion, we&amp;rsquo;ll find that the variance stabilizing transformation is $g(\theta) = log(\theta)$.  We&amp;rsquo;ll apply that here:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;theta = 0.5

N = [5,10,50,100,250,500,750,1000]
iters = 500

clt_exp = {n: [] for n in N}
exp_novar= {n: [] for n in N}
exp_var = {n: [] for n in N}

for n in N:
    for i in np.arange(iters):
        samps = expon(scale=n*theta).rvs(n)
        
        est_mu = np.mean(samps)
        est_var = np.var(samps)

        exp_novar[n].append(clt(n, est_mu, (n*theta)))
        exp_var[n].append(clt(n, np.log(est_mu), np.log(n*theta)))
        
        clt_exp[n].append(conv_prob(n, est_mu, n*theta))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig,([ax1, ax2]) = plt.subplots(2,1, figsize=(15,6))

plt.subplot(1,2,1)
for i,n in enumerate(N):
    plt.violinplot(exp_novar[n], positions=[i])

plt.subplot(1,2,2)
for i,n in enumerate(N):
    plt.violinplot(exp_var[n], positions=[i])
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-variance-stabilization-of-exponential-distribution&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/the-delta-method/Exponential_huea2034d5b1cd7cc9503f2c97f5f9221b_72157_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Variance stabilization of Exponential distribution.&#34;&gt;


  &lt;img data-src=&#34;/post/the-delta-method/Exponential_huea2034d5b1cd7cc9503f2c97f5f9221b_72157_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1080&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Variance stabilization of Exponential distribution.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;example-of-standard-error-computation-using-delta-method-for-polynomial-regression&#34;&gt;Example of Standard Error Computation Using Delta Method for Polynomial Regression&lt;/h4&gt;
&lt;p&gt;As an example of applying the Delta Method to a real-world dataset,  I&amp;rsquo;ve downloaded the 
&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/banknote&amp;#43;authentication&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;banknote&lt;/strong&gt;&lt;/a&gt; dataset from the 
&lt;a href=&#34;https://archive.ics.uci.edu/ml/index.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UCI Machine Learning Repository&lt;/a&gt;.  In this exercise, I&amp;rsquo;ll apply the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;logistic function&lt;/a&gt; via logistic regression to assess whether or not a banknote is real or fake, using a set of features.   I&amp;rsquo;ll compute confidence intervals of our prediction probabilities using the Delta Method.  There are four unique predictors in this case: the &lt;strong&gt;variance&lt;/strong&gt;, &lt;strong&gt;skew&lt;/strong&gt;, &lt;strong&gt;kurtosis&lt;/strong&gt;, and &lt;strong&gt;entropy&lt;/strong&gt; of the Wavelet-transformed banknote image.  I&amp;rsquo;ll treat each of these predictors independently, using polynomial basis function of degree 3.&lt;/p&gt;
&lt;p&gt;In this example, we&amp;rsquo;re interested in the standard error of our probability estimate.  Our function is the Logistic Function, as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
g(\beta) &amp;amp;= \frac{1}{1+e^{-x^{T}\beta}} \\
&amp;amp;= \frac{e^{x^{T}\beta}}{1+e^{x^{T}\beta}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the gradient of this multivariate function is:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\nabla g(\beta) &amp;amp;= \frac{\partial g}{\partial \beta} e^{x^{T}\beta}(1+e^{x^{T}\beta})^{-1} \\
&amp;amp;= x^{T}e^{x^{T}\beta}(1+e^{x^{T}\beta})^{-1} - x^{T}e^{x^{T}\beta}e^{x^{T}\beta} \\
&amp;amp;= x^{T}\Big(e^{x^{T}\beta}(1+e^{x^{T}\beta})^{-1} - e^{x^{T}\beta}e^{x^{T}\beta}\Big)(1+e^{x^{T}\beta})^{-2} \\
&amp;amp;= x^{T} \frac{e^{x^{T}\beta}}{(1+e^{x^{T}\beta})^{2}} \\
\nabla g(\beta) &amp;amp;= x^{T} g(\beta)(1-g(\beta))
\end{align}$$&lt;/p&gt;
&lt;p&gt;so that the final estimate of our confidence interval becomes&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp; \sim N(0,x^{T} g(\beta)(1-g(\beta)) \Sigma g(\beta)(1-g(\beta))x) \\
&amp;amp; \sim N(0, \nabla g(\beta)^{T} \Sigma \nabla g(\beta))
\end{align}$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import statsmodels.api as sm
from sklearn.preprocessing import PolynomialFeatures
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import statsmodels.api as sm
from sklearn.preprocessing import PolynomialFeatures

bank = pd.read_csv(&#39;/Users/kristianeschenburg/Documents/Statistics/BankNote.txt&#39;,
                   sep=&#39;,&#39;,header=None, names=[&#39;variance&#39;, &#39;skew&#39;, &#39;kurtosis&#39;, &#39;entropy&#39;,&#39;class&#39;])
bank.head()

fig = plt.subplots(2,2, figsize=(12,8))
for j, measure in enumerate([&#39;variance&#39;, &#39;kurtosis&#39;, &#39;skew&#39;, &#39;entropy&#39;]):

    predictor = np.asarray(bank[measure])
    response = np.asarray(bank[&#39;class&#39;])
    
    idx = (response == 1)

    # plot test set
    plt.subplot(2,2,j+1)
    plt.violinplot(predictor[idx], positions=[1]);
    plt.violinplot(predictor[~idx], positions=[0])
    plt.title(&#39;{:} By Classification&#39;.format(measure), fontsize=18)
    plt.ylabel(&#39;Measure: {:}&#39;.format(measure),fontsize=15)
    plt.yticks(fontsize=13)
    plt.xticks([0,1],[&#39;Fake&#39;,&#39;Real&#39;], fontsize=15)

plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-bank-note-feature-distributions-based-on-note-class&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/the-delta-method/bank_notes_hu2e6b6012dbc427ee46e44a8dade0f2f1_73271_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Bank note feature distributions, based on note class.&#34;&gt;


  &lt;img data-src=&#34;/post/the-delta-method/bank_notes_hu2e6b6012dbc427ee46e44a8dade0f2f1_73271_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;576&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Bank note feature distributions, based on note class.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Based on the above plot, we can see that &lt;strong&gt;variance&lt;/strong&gt;, &lt;strong&gt;skew&lt;/strong&gt;, and &lt;strong&gt;kurtosis&lt;/strong&gt; seem to be the most informative, while the &lt;strong&gt;entropy&lt;/strong&gt; distributions do not seem to be that different based on bank note class.&lt;/p&gt;
&lt;p&gt;Next, we fit a logistic regression model of note classification on note feature, with polynomial order of degree 3.  We then compute the standard errors of the transformed variance.  It was transformed using the &lt;strong&gt;logistic function&lt;/strong&gt;, so we&amp;rsquo;ll need to compute the gradient of this function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.subplots(2,2, figsize=(12,8))
for j, measure in enumerate([&#39;variance&#39;, &#39;kurtosis&#39;, &#39;skew&#39;, &#39;entropy&#39;]):

    # Generate polynomial object to degree 
    # transform age to 4-degree basis function
    poly = PolynomialFeatures(degree=2)
    idx_order = np.argsort(bank[measure])

    predictor = bank[measure][idx_order]
    response = bank[&#39;class&#39;][idx_order]

    features = poly.fit_transform(predictor.values.reshape(-1,1));

    # fit logit curve to curve
    logit = sm.Logit(response, features).fit();
    
    test_features = np.linspace(np.min(predictor), np.max(predictor), 100)
    test_features = poly.fit_transform(test_features.reshape(-1,1))
    # predict on test set
    class_prob = logit.predict(test_features)

    cov = logit.cov_params()
    yx = (class_prob*(1-class_prob))[:,None] * test_features
    se = np.sqrt(np.diag(np.dot(np.dot(yx, cov), yx.T)))

    # probability can&#39;t exceed 1, or be less than 0
    upper = np.maximum(0, np.minimum(1, class_prob+1.96*se))
    lower = np.maximum(0, np.minimum(1, class_prob-1.96*se))

    # plot test set
    plt.subplot(2,2,j+1)
    plt.plot(test_features[:, 1], class_prob);
    plt.plot(test_features[:, 1], upper, color=&#39;red&#39;, linestyle=&#39;--&#39;, alpha=0.5);
    plt.plot(test_features[:, 1], lower, color=&#39;red&#39;, linestyle=&#39;--&#39;, alpha=0.5);
    plt.title(r&#39;P(isReal \Big| X)&#39;, fontsize=18)
    plt.xlabel(&#39;{:}&#39;.format(measure),fontsize=15)
    plt.ylabel(&#39;Probability&#39;,fontsize=15)
    plt.grid(True)

plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-confidence-intervals-for-each-feature-computed-using-delta-method&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/the-delta-method/bank_notes_CI_hu581948ebf8121f540d3ba9752b606efb_109427_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Confidence intervals for each feature, computed using Delta Method.&#34;&gt;


  &lt;img data-src=&#34;/post/the-delta-method/bank_notes_CI_hu581948ebf8121f540d3ba9752b606efb_109427_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;576&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Confidence intervals for each feature, computed using Delta Method.
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Mahalanobis Distances of Brain Connectivity</title>
      <link>/post/mahalanobis-distances-of-brain-connectivity/</link>
      <pubDate>Fri, 07 Dec 2018 05:12:32 -0700</pubDate>
      <guid>/post/mahalanobis-distances-of-brain-connectivity/</guid>
      <description>&lt;p&gt;For one of the projects I&amp;rsquo;m working on, I have an array of multivariate data relating to brain connectivity patterns.  Briefly, each brain is represented as a surface mesh, which we represent as a graph $G = (V,E)$, where $V$ is a set of $n$ vertices, and $E$ are the set of edges between vertices.&lt;/p&gt;
&lt;p&gt;Additionally, for each vertex $v \in V$, we also have an associated scalar &lt;em&gt;label&lt;/em&gt;, which we&amp;rsquo;ll denote $l(v)$, that identifies what region of the cortex each vertex belongs to, the set of regions which we define as $L = {1, 2, &amp;hellip; k}$.  And finally, for each vertex $v \in V$, we also have a multivariate feature vector $r(v) \in \mathbb{R}^{1 \times k}$, that describes the strength of connectivity between it, and every region $l \in L$.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-example-of-cortical-map-and-array-of-connectivity-features&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/mahalanobis-distances-of-brain-connectivity/parcellation_huf0b3bb1bc41eebdcb9d4479e215063d3_232126_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Example of cortical map, and array of connectivity features.&#34;&gt;


  &lt;img data-src=&#34;/post/mahalanobis-distances-of-brain-connectivity/parcellation_huf0b3bb1bc41eebdcb9d4479e215063d3_232126_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1296&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Example of cortical map, and array of connectivity features.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;I&amp;rsquo;m interested in examining how &amp;ldquo;close&amp;rdquo; the connectivity samples of one region, $l_{j}$, are to another region, $l_{k}$.  In the univariate case, one way to compare a scalar sample to a distribution is to use the $t$-statistic, which measures how many standard deviations away from the mean a given sample is:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
t_{s} = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $\mu$ is the population mean, and $s$ is the sample standard deviation.  If we square this, we get:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
t^{2} = \frac{(\bar{x} - \mu)^{2}}{\frac{s^{2}}{n}} =  \frac{n (\bar{x} - \mu)^{2}}{S^{2}} \sim F(1,n)
\end{align}$$&lt;/p&gt;
&lt;p&gt;We know the last part is true, because the numerator and denominator are independent $\chi^{2}$ distributed random variables.  However, I&amp;rsquo;m not working with univariate data &amp;ndash; I have multivariate data.  The multivariate generalization of the $t$-statistic is the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Mahalanobis_distance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mahalanobis Distance&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
d &amp;amp;= \sqrt{(\bar{x} - \mu)\Sigma^{-1}(\bar{x}-\mu)^{T}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the squared Mahalanobis Distance is:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
d^{2} &amp;amp;= (\bar{x} - \mu)\Sigma^{-1}(\bar{x}-\mu)^{T}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $\Sigma^{-1}$ is the inverse covariance matrix.  If our $X$&amp;rsquo;s were initially distributed with a multivariate normal distribution, $N_{p}(\mu,\Sigma)$ (assuming $\Sigma$ is non-degenerate i.e. positive definite), the squared Mahalanobis distance, $d^{2}$ has a $\chi^{2}_{p}$ distribution.  We show this below.&lt;/p&gt;
&lt;p&gt;We know that $(X-\mu)$ is distributed $N_{p}(0,\Sigma)$.  We also know that, since $\Sigma$ is symmetric and real, that we can compute the eigendecomposition of $\Sigma$ as:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\Sigma = U \Lambda U^{T}
\end{align}$$&lt;/p&gt;
&lt;p&gt;and consequentially, because $U$ is an orthogonal matrix, and because $\Lambda$ is diagonal, we know that $\Sigma^{-1}$ is:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\Sigma^{-1} &amp;amp;= (U \Lambda U^{T})^{-1} \\
&amp;amp;= U \Lambda^{-1} U^{T} \\
&amp;amp;= (U \Lambda^{\frac{-1}{2}}) (U \Lambda^{\frac{-1}{2}})^{T} \\
&amp;amp;= R R^{T}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Therefore, we know that $R^{T}(X-\mu) \sim N_{p}(0,I_{p})$:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
X &amp;amp;\sim N_{p}(\mu,\Sigma) \\
(X-\mu) = Y &amp;amp;\sim N_{p}(0,\Sigma)\\
R^{T}Y = Z &amp;amp;\sim N_{p}(0, R^{T} \Sigma R) \\
&amp;amp;\sim N_{p}(0, \Lambda^{\frac{-1}{2}} U^{T} (U \Lambda U^{T}) U \Lambda^{\frac{-1}{2}}) \\
&amp;amp;\sim N_{p}(0, \Lambda^{\frac{-1}{2}} I_{p} \Lambda I_{p} \Lambda^{\frac{-1}{2}}) \\
&amp;amp;\sim N_{p}(0,I_{p})
\end{align}$$&lt;/p&gt;
&lt;p&gt;so that we have&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;= (X-\mu)\Sigma^{-1}(X-\mu)^{T} \\
&amp;amp;= (X-\mu)RR^{T}(X-\mu)^{T} \\
&amp;amp;= Z^{T}Z
\end{align}$$&lt;/p&gt;
&lt;p&gt;the sum of $p$ standard Normal random variables, which is the definition of a $\chi_{p}^{2}$ distribution with $p$ degrees of freedom.  So, given that we start with a $MVN$ random variable, the squared Mahalanobis distance is $\chi^{2}_{p}$ distributed.  Because the sample mean and sample covariance are consistent estimators of the population mean and population covariance parameters, we can use these estimates in our computation of the Mahalanobis distance.&lt;/p&gt;
&lt;p&gt;Also, of particular importance is the fact that the Mahalanobis distance is &lt;strong&gt;not symmetric&lt;/strong&gt;.  That is to say, if we define the Mahalanobis distance as:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
M(A, B) = \sqrt{(A - \mu(B))\Sigma(B)^{-1}(A-\mu(B))^{T}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;then $M(A,B) \neq M(B,A)$, clearly.  Because the parameter estimates are not guaranteed to be the same, it&amp;rsquo;s straightforward to see why this is the case.&lt;/p&gt;
&lt;p&gt;Now, back to the task at hand.  For a specified target region, $l_{T}$, with a set of vertices, $V_{T} = {v \; : \; l(v) \; = \; l_{T}, \; \forall \; v \in V}$, each with their own distinct connectivity fingerprints, I want to explore which areas of the cortex have connectivity fingerprints that are different from or similar to $l_{T}$&amp;rsquo;s features, in distribution.  I can do this by using the Mahalanobis Distance.  And based on the analysis I showed above, we know that the data-generating process of these distances is related to the $\chi_{p}^{2}$ distribution.&lt;/p&gt;
&lt;p&gt;First, I&amp;rsquo;ll estimate the covariance matrix, $\Sigma_{T}$, of our target region, $l_{T}$, using the 
&lt;a href=&#34;http://perso.ens-lyon.fr/patrick.flandrin/LedoitWolf_JMA2004.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ledoit-Wolf estimator&lt;/a&gt; (the shrunken covariance estimate has been shown to be a more reliable estimate of the population covariance), and mean connectivity fingerprint, $\mu_{T}$.  Then, I&amp;rsquo;ll compute $d^{2} = M^{2}(A,A)$ for every $\{v: v \in V_{T}\}$.  The empirical distribution of these distances should follow a $\chi_{p}^{2}$ distribution.  If we wanted to do hypothesis testing, we would use this distribution as our null distribution.&lt;/p&gt;
&lt;p&gt;Next, in order to assess whether this intra-regional similarity is actually informative, I&amp;rsquo;ll also compute the similarity of $l_{T}$ to every other region, $\{ l_{k} \; : \; \forall \; k \in L \setminus \{T\} \}$ &amp;ndash; that is, I&amp;rsquo;ll compute $M^{2}(A, B) \; \forall \; B \in L \setminus T$.  If the connectivity samples of our region of interest are as similar to one another as they are to other regions, then $d^{2}$ doesn&amp;rsquo;t really offer us any discriminating information &amp;ndash; I don&amp;rsquo;t expect this to be the case, but we need to verify this.&lt;/p&gt;
&lt;p&gt;Then, as a confirmation step to ensure that our empirical data actually follows the theoretical $\chi_{p}^{2}$ distribution, I&amp;rsquo;ll compute the location and scale 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximumim Likelihood&lt;/a&gt;(MLE) parameter estimates of our $d^{2}$ distribution, keeping the &lt;em&gt;d.o.f.&lt;/em&gt; (e.g. $p$) fixed.&lt;/p&gt;
&lt;p&gt;See below for Python code and figures&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;step-1-compute-parameter-estimates&#34;&gt;Step 1: Compute Parameter Estimates&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import matplotlib.pyplot as plt

from matplotlib import rc
rc(&#39;text&#39;, usetex=True)

import numpy as np
from scipy.spatial.distance import cdist
from scipy.stats import chi2, probplot

from sklearn import covariance
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# lab_map is a dictionary, mapping label values to sample indices
# our region of interest has a label of 8
LT = 8

# get indices for region LT, and rest of brain
lt_indices = lab_map[LT]
rb_indices = np.concatenate([lab_map[k] for k in lab_map.keys() if k != LT])

data_lt = conn[lt_indices, :]
data_rb = conn[rb_indices, :]

# fit covariance and precision matrices
# Shrinkage factor = 0.2
cov_lt = covariance.ShrunkCovariance(assume_centered=False, shrinkage=0.2)
cov_lt.fit(data_lt)
P = cov_lt.precision_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, compute the Mahalanobis Distances:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# LT to LT Mahalanobis Distance
dist_lt = cdist(data_lt, data_lt.mean(0)[None,:], metric=&#39;mahalanobis&#39;, VI=P)
dist_lt2 = dist_lt**2

# fit covariance estimate for every region in cortical map
EVs = {l: covariance.ShrunkCovariance(assume_centered=False, 
        shrinkage=0.2) for l in labels}

for l in lab_map.keys():
    EVs[l].fit(conn[lab_map[l],:])

# compute d^2 from LT to every cortical region
# save distances in dictionary
lt_to_brain = {}.fromkeys(labels)
for l in lab_map.keys():

    temp_data = conn[label_map[l], :]
    temp_mu = temp_data.mean(0)[None, :]

    temp_mh = cdist(data_lt, temp_mu, metric=&#39;mahalanobis&#39;, VI=EVs[l].precision_)
    temp_mh2 = temp_mh**2

    lt_to_brain[l] = temp_mh2

# plot distributions seperate (scales differ)
fig = plt.subplots(2,1, figsize=(12,12))
plt.subplot(2,1,1)
plt.hist(lt_to_brain[LT], 50, density=True, color=&#39;blue&#39;, 
    label=&#39;Region-to-Self&#39;, alpha=0.7)

plt.subplot(2,1,2)
for l in labels:
    if l != LT:
        plt.hist(lt_to_brain[l], 50, density=True, linewidth=2, 
            alpha=0.4, histtype=&#39;step&#39;)
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-empirical-distributions-of-within-region-top-and-between-region-bottom-d2-values--each-line-is-the-distribution-of-the-distance-of-samples-in-our-roi-to-a-whole-region&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/mahalanobis-distances-of-brain-connectivity/IntraInterMahal_hu396a8faeb9757ca8b446c8e1d4b3975d_82931_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Empirical distributions of within-region (top) and between-region (bottom) $d^{2}$ values.  Each line is the distribution of the distance of samples in our ROI to a whole region.&#34;&gt;


  &lt;img data-src=&#34;/post/mahalanobis-distances-of-brain-connectivity/IntraInterMahal_hu396a8faeb9757ca8b446c8e1d4b3975d_82931_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;864&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Empirical distributions of within-region (top) and between-region (bottom) $d^{2}$ values.  Each line is the distribution of the distance of samples in our ROI to a whole region.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As expected, the distribution of $d^{2}$ the distance of samples in our region of interest, $l_{T}$, to distributions computed from other regions are (considerably) larger and much more variable, while the profile of points within $l_{T}$ looks to have much smaller variance &amp;ndash; this is good!  This means that we have high intra-regional similarity when compared to inter-regional similarities.  This fits what&amp;rsquo;s known in neuroscience as the 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/9651489&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;cortical field hypothesis&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;step-2-distributional-qc-check&#34;&gt;Step 2: Distributional QC-Check&lt;/h3&gt;
&lt;p&gt;Because we know that our data should follow a $\chi^{2}_{p}$ distribution, we can fit the MLE estimate of our location and scale parameters, while keeping  the $df$ parameter fixed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p = data_lt.shape[1]
mle_chi2_theory = chi2.fit(dist_lt2, fdf=p)

xr = np.linspace(data_lt.min(), data_lt.max(), 1000)
pdf_chi2_theory(xr, *mle_chi2_theory)

fig = plt.subplot(1,2,2,figsize=(18, 6))

# plot theoretical vs empirical null distributon
plt.subplot(1,2,1)
plt.hist(data_lt, density=True, color=&#39;blue&#39;, alpha=0.6,
    label = &#39;Empirical&#39;)
plt.plot(xr, pdf_chi2_theory, color=&#39;red&#39;,
    label = &#39;$\chi^{2}_{p}&#39;)

# plot QQ plot of empirical distribution
plt.subplot(1,2,2)
probplot(D2.squeeze(), sparams=mle_chi2_theory, dist=chi2, plot=plt);
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure id=&#34;figure-density-and-qq-plot-of-null-distribution&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/mahalanobis-distances-of-brain-connectivity/Density.QQPlot_hu1d3559aae9fc2b76dfaee6ede7f1500d_47405_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Density and QQ plot of null distribution.&#34;&gt;


  &lt;img data-src=&#34;/post/mahalanobis-distances-of-brain-connectivity/Density.QQPlot_hu1d3559aae9fc2b76dfaee6ede7f1500d_47405_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;864&#34; height=&#34;864&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Density and QQ plot of null distribution.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;From looking at the QQ plot, we see that the empirical density fits the theoretical density pretty well, but there is some evidence that the empirical density has heavier tails.  The heavier tail of the upper quantile could probability be explained by acknowledging that our starting cortical map is not perfect (in fact there is no &amp;ldquo;gold-standard&amp;rdquo; cortical map).  Cortical regions do not have discrete cutoffs, although there are reasonably steep 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/25316338&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gradients in connectivity&lt;/a&gt;.  If we were to include samples that were considerably far away from the the rest of the samples, this would result in inflated densities of higher $d^{2}$ values.&lt;/p&gt;
&lt;p&gt;Likewise, we also made the distributional assumption that our connectivity vectors were multivariate normal &amp;ndash; this might not be true &amp;ndash; in which case our assumption that $d^{2}$ follows a $\chi^{2}_{p}$ would also not hold.&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s have a look at some brains!  Below, is the region we used as our target &amp;ndash; the connectivity profiles from vertices in this region were used to compute our mean vector and covariance matrix &amp;ndash; we compared the rest of the brain to this region.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-region-of-interest&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/mahalanobis-distances-of-brain-connectivity/Region_LT_hu99f982ee794454d59a2b0bf97fc60dbf_189336_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Region of interest.&#34;&gt;


  &lt;img data-src=&#34;/post/mahalanobis-distances-of-brain-connectivity/Region_LT_hu99f982ee794454d59a2b0bf97fc60dbf_189336_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1440&#34; height=&#34;821&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Region of interest.
  &lt;/figcaption&gt;


&lt;/figure&gt;






  
  











&lt;figure id=&#34;figure-estimated-squared-mahalanobis-distances-overlaid-on-cortical-surface&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/mahalanobis-distances-of-brain-connectivity/MahalanobisDistance_hu9c9596cf29d1146f5fc3d3b7dde66282_764298_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Estimated squared Mahalanobis distances, overlaid on cortical surface.&#34;&gt;


  &lt;img data-src=&#34;/post/mahalanobis-distances-of-brain-connectivity/MahalanobisDistance_hu9c9596cf29d1146f5fc3d3b7dde66282_764298_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1440&#34; height=&#34;821&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Estimated squared Mahalanobis distances, overlaid on cortical surface.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Here, larger $d^{2}$ values are in red, and smaller $d^{2}$ are in black.  Interestingly, we do see pretty large variance of $d^{2}$ spread across the cortex &amp;ndash; however the values are smoothly varying, but there do exists sharp boundaries.  We kind of expected this &amp;ndash; some regions, though geodesically far away, should have similar connectivity profiles if they&amp;rsquo;re connected to the same regions of the cortex.  However, the regions with connectivity profiles most different than our target region are not only contiguous (they&amp;rsquo;re not noisy), but follow known anatomical boundaries, as shown by the overlaid boundary map.&lt;/p&gt;
&lt;p&gt;This is interesting stuff &amp;ndash; I&amp;rsquo;d originally intended on just learning more about the Mahalanobis Distance as a measure, and exploring its distributional properties &amp;ndash; but now that I see these results, I think it&amp;rsquo;s definitely worth exploring further!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convergence In Probability</title>
      <link>/post/convergence-in-probability/</link>
      <pubDate>Wed, 28 Nov 2018 13:12:32 -0700</pubDate>
      <guid>/post/convergence-in-probability/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m going over &lt;strong&gt;Chapter 5&lt;/strong&gt; in Casella and Berger&amp;rsquo;s (CB) &amp;ldquo;Statistical Inference&amp;rdquo;, specifically &lt;strong&gt;Section 5.5: Convergence Concepts&lt;/strong&gt;, and wanted to document the topic of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_probability&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;convergence in probability&lt;/a&gt; with some plots demonstrating the concept.&lt;/p&gt;
&lt;p&gt;From CB, we have the definition of &lt;em&gt;convergence in probability&lt;/em&gt;: a sequence of random variables $X_{1}, X_{2}, &amp;hellip; X_{n}$ converges in probability to a random variable $X$, if for every $\epsilon &amp;gt; 0$,&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\lim_{n \to \infty} P(| X_{n} - X | \geq \epsilon) = 0 \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;Intuitively, this means that, if we have some random variable $X_{k}$ and another random variable $X$, the absolute difference between $X_{k}$ and $X$ gets smaller and smaller as $k$ increases.  The probability that this difference exceeds some value, $\epsilon$, shrinks to zero as $k$ tends towards infinity.  Using &lt;em&gt;convergence in probability&lt;/em&gt;, we can derive the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_large_numbers#Weak_law&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weak Law of Large Numbers&lt;/a&gt; (WLLN):&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\lim_{n \to \infty} P(|\bar{X}_{n} - \mu | \geq \epsilon) = 0
\end{align}$$&lt;/p&gt;
&lt;p&gt;which we can take to mean that the sample mean converges in probability to the population mean as the sample size goes to infinity.  If we have finite variance (that is $Var(X) &amp;lt; \infty$), we can prove this using Chebyshev&amp;rsquo;s Law&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;= P(|\bar{X}_{n} - \mu | \geq \epsilon) \\
&amp;amp;= P((\bar{X}_{n} - \mu)^{2} \geq \epsilon^{2}) \leq \frac{E\Big[(\bar{X}_{n} - \mu)^{2}\Big]}{\epsilon^{2}} \\
&amp;amp;= P((\bar{X}_{n} - \mu)^{2} \geq \epsilon^{2}) \leq \frac{Var(\bar{X_{n}})}{\epsilon^{2}} \\
&amp;amp;= P((\bar{X}_{n} - \mu)^{2} \geq \epsilon^{2}) \leq \frac{\sigma^{2}}{n^{2}\epsilon^{2}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $\frac{\sigma^{2}}{n^{2} \epsilon^{2}} \rightarrow 0$ as $n \rightarrow \infty$.  Intuitively, this means, that the sample mean converges to the population mean &amp;ndash; and the probability that their difference is larger than some value is bounded by the variance of the estimator.  Because we showed that the variance of the estimator (right hand side) shrinks to zero, we can show that the difference between the sample mean and population mean converges to zero.&lt;/p&gt;
&lt;p&gt;We can also show a similar WLLN result for the sample variance using Chebyshev&amp;rsquo;s Inequality, as:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
S_{n}^{2} = \frac{1}{n-1} \sum_{i=1}^{n} (X_{i} - \bar{X}_{n})^{2}
\end{align}$$&lt;/p&gt;
&lt;p&gt;using the unbiased estimator, $S_{n}^{2}$, of $\sigma^{2}$ as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
P(|S_{n}^{2} - \sigma^{2}| \geq \epsilon) \leq \frac{E\Big[(S_{n}^{2} - \sigma^{2})^{2}\Big]}{\epsilon^{2}} = \frac{Var(S_{n}^{2})}{\epsilon^{2}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;so all we need to do is show that $Var(S_{n}^{2}) \rightarrow 0$ as $n \rightarrow \infty$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at some (simple) real-world examples.  We&amp;rsquo;ll start by sampling from a $N(0,1)$ distribution, and compute the sample mean and variance using their unbiased estimators.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import numpy and scipy libraries
import numpy as np
from scipy.stats import norm

%matplotlib inline
import matplotlib.pyplot as plt

plt.rc(&#39;text&#39;, usetex=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Generate set of samples sizes
samples = np.concatenate([np.arange(0, 105, 5), 
                          10*np.arange(10, 110, 10),
                         100*np.arange(10, 210, 10)])

# number of repeated samplings for each sample size
iterations = 500

# store sample mean and variance
means = np.zeros((iterations, len(samples)))
vsrs = np.zeros((iterations, len(samples)))

for i in np.arange(iterations):
    for j, s in enumerate(samples):
        
        # generate samples from N(0,1) distribution
        N = norm.rvs(loc=0, scale=1, size=s)
        mu = np.mean(N)
        
        # unbiased estimate of variance
        vr = ((N - mu)**2).sum()/(s-1)

        means[i, j] = mu
        vsrs[i, j] = vr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the sample means and variances as a function of the sample size.  Empirically, we see that both the sample mean and variance estimates converge to their population parameters, 0 and 1.&lt;/p&gt;
&lt;p&gt;




  
  











&lt;figure id=&#34;figure-sample-mean-estimates-as-a-function-of-sample-size&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/convergence-in-probability/WLLN_Mean_hub07fe3b7d9c1af829dbcf59ec07d859c_56103_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Sample mean estimates as a function of sample size.&#34;&gt;


  &lt;img data-src=&#34;/post/convergence-in-probability/WLLN_Mean_hub07fe3b7d9c1af829dbcf59ec07d859c_56103_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;720&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample mean estimates as a function of sample size.
  &lt;/figcaption&gt;


&lt;/figure&gt;






  
  











&lt;figure id=&#34;figure-sample-variance-estimates-as-a-function-of-sample-size&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/convergence-in-probability/WLLN_Variance_hu105b8336e2721323d12490083206465a_57888_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Sample variance estimates as a function of sample size.&#34;&gt;


  &lt;img data-src=&#34;/post/convergence-in-probability/WLLN_Variance_hu105b8336e2721323d12490083206465a_57888_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;720&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sample variance estimates as a function of sample size.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Below is a simple method to compute the empirical probability that an estimate exceeds the epsilon threshold.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ecdf(data, pparam, epsilon):
    
    &amp;quot;&amp;quot;&amp;quot;
    Compute empirical probability P( |estimate - pop-param| &amp;lt; epsilon).
    
    Parameters:
    - - - - -
    data: array, float
        array of samples
    pparam: float
        true population parameter
    epsilon: float
        threshold value
    &amp;quot;&amp;quot;&amp;quot;
    
    compare = (np.abs(data - pparam) &amp;lt; epsilon)
    prob = compare.mean(0)
    
    return prob
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# test multiple epsilon thresholds
e = [0.9, 0.75, 0.5, 0.25, 0.1, 0.05, 0.01]

mean_probs = []
vrs_probs = []
# compute empirical probabilities at each threshold
for E in e:
    mean_probs.append(1 - ecdf(means, pparam=0, epsilon=E))
    vrs_probs.append(1-ecdf(vsrs, pparam=1, epsilon=E))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  
  











&lt;figure id=&#34;figure-empirical-probability-that-mean-estimate-exceeds-population-mean-by-epsilon&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/convergence-in-probability/ECDF_Mean_hud239a6530d10b86e3a0a175784090642_43944_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Empirical probability that mean estimate exceeds population mean by epsilon.&#34;&gt;


  &lt;img data-src=&#34;/post/convergence-in-probability/ECDF_Mean_hud239a6530d10b86e3a0a175784090642_43944_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;720&#34; height=&#34;360&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Empirical probability that mean estimate exceeds population mean by epsilon.
  &lt;/figcaption&gt;


&lt;/figure&gt;






  
  











&lt;figure id=&#34;figure-empirical-probability-that-variance-estimate-exceeds-population-variance-by-epsilon&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/convergence-in-probability/ECDF_Variance_hu4e0ed10e18641d62e69d3210311d0280_46360_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Empirical probability that variance estimate exceeds population variance by epsilon.&#34;&gt;


  &lt;img data-src=&#34;/post/convergence-in-probability/ECDF_Variance_hu4e0ed10e18641d62e69d3210311d0280_46360_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;720&#34; height=&#34;360&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Empirical probability that variance estimate exceeds population variance by epsilon.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The above plots show that, as sample size increases, the mean estimator and variance estimator both converge to their true population parameters.  Likewise, examining the empirical probability plots, we can see that the probability that either estimate exceeds the epsilon thresholds shrinks to zero as the sample size increases.&lt;/p&gt;
&lt;p&gt;If we wish to consider a stronger degree of convergence, we can consider &lt;em&gt;convergence almost surely&lt;/em&gt;, which says the following:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
P(\lim_{n \to \infty} |X_{n} - X| \geq \epsilon) = 0 \
\end{align}$$&lt;/p&gt;
&lt;p&gt;which considers the entire joint distribution of estimates $( X_{1}, X_{2}&amp;hellip;X_{n}, X)$, rather than all pairwise estimates $(X_{1},X), (X_{2},X)&amp;hellip; (X_{n},X)$ &amp;ndash; the entire set of estimates must converge to $X$ as the sample size approaches infinity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overview of Poisson-Multinomial Relationship</title>
      <link>/post/overview-of-poisson-multinomial-relationship/</link>
      <pubDate>Thu, 08 Nov 2018 01:12:32 -0700</pubDate>
      <guid>/post/overview-of-poisson-multinomial-relationship/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;m going to briefly cover the relationship between the Poisson distribution and the Multinomial distribution.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say that we have a set of independent, Poisson-distributed random variables $Y_{1}, Y_{2}&amp;hellip; Y_{k}$ with rate parameters $\lambda_{1}, \lambda_{2}, &amp;hellip;\lambda_{k}$.  We can model the sum of these random variables as a new random variable $N = \sum_{i=1}^{k} Y_{i}$.&lt;/p&gt;
&lt;p&gt;Let start with $k=2$.  We can define the distrbution of $F_{N}(n)$ as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;= P(N \leq n) \\
&amp;amp;= P(Y_{1} + Y_{2} \leq n) \\
&amp;amp;= P(Y_{1} = y_{1}, Y_{2} = n - y_{1}) \\
&amp;amp;= P(Y_{1} = y_{1}) \cdot P(Y_{2} = n-y_{1}) \\
&amp;amp;= \sum_{y_{1}=0}^{n} \frac{e^{-\lambda_{1}}\lambda_{1}^{y_{1}}}{y_{1}!} \cdot \frac{e^{-\lambda_{2}}\lambda_{2}^{n-y_{1}}}{(n-y_{1})!} \\
&amp;amp;= e^{-(\lambda_{1}+\lambda_{2})} \sum_{y_{1}=0}^{n} \frac{\lambda_{1}^{y_{1}}\lambda_{2}^{n-y_{1}}}{y_{1}!(n-y_{1})!} \\
&amp;amp;= e^{-(\lambda_{1}+\lambda_{2})} \sum_{y_{1}=0}^{n} \frac{n!}{n!}\frac{\lambda_{1}^{y_{1}}\lambda_{2}^{n-y_{1}}}{y_{1}!(n-y_{1})!} \\
&amp;amp;= \frac{e^{-(\lambda_{1}+\lambda_{2})}}{n!} \sum_{y_{1}=0}^{n} {n\choose y_{1}} \lambda_{1}^{y_{1}}\lambda_{2}^{n-y_{1}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Here, we can apply the Binomial Theorem to the summation to get the following (remember that the Binomial Theorem says, for two numbers $x$ and $y$, that $(x+y)^{n} = \sum_{i=0}^{n} {n \choose i}x^{i}y^{n-i}$):&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\frac{e^{-(\lambda_{1}+\lambda_{2})}(\lambda_{1} + \lambda_{2})^{n}}{n!} \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;which we see is in fact just another Poisson distribution with rate parameter equal to $\lambda_{1} + \lambda_{2}$.  This shows that the sum of independent Poisson distributed random variables is also a Poisson random variable, with rate parameter equal to the sum of the univariate rates.  By induction, we see that for $k$ independent Poisson distributed random variables $Y_{1}&amp;hellip;Y_{k}$, their sum $\sum_{i=1}^{k} Y_{i} \sim Poisson(\sum_{i=1}^{k} \lambda_{i})$.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s say we&amp;rsquo;re interested in modeling the conditional distribution of $(Y_{1}&amp;hellip;Y_{k}) \mid \sum_{i=1}^{k} = n$.  By definition of conditional probability, we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
P(\bar{Y} \mid N=n) &amp;amp;= \frac{P(\bar{Y} ; \cap ; N=n)}{P(N=n)} \\
&amp;amp;= \frac{P(\bar{Y})}{P(N=n)}
\end{align}$$&lt;/p&gt;
&lt;p&gt;We have the following:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
P(\bar{Y} \mid N=n) &amp;amp;= \frac{P(\bar{Y} \; \cap \; N=n)}{P(N=n)} \\
&amp;amp;= \Big( \prod_{i=1}^{k} \frac{e^{-\lambda_{i}} \cdot \lambda_{i}^{y_{i}}}{y_{i}!} \Big) \Big/ \frac{e^{-\sum_{i=1}^{k} \lambda_{i}}(\sum_{i}^{k} \lambda_{i})^{n}}{n!} \\
&amp;amp;= \Big( \frac{ e^{-\sum_{i=1}^{k}} \prod_{i=1}^{k} \lambda_{i}^{y_{i}}}{\prod_{i=1}^{k} y_{i}!} \Big) \Big/ \frac{e^{-\sum_{i=1}^{k} \lambda_{i}}(\sum_{i}^{k} \lambda_{i})^{n}}{n!} \\
&amp;amp;= { n \choose y_{1}, y_{2}, &amp;hellip;y_{k}} \frac{\prod_{i=1}^{k} \lambda_{i}^{y_{i}}} { \sum_{i}^{k} \lambda_{i})^{n}} \\
&amp;amp;= { n \choose y_{1}, y_{2}, &amp;hellip;y_{k}}  \prod_{i=1}^{k} \Big( \frac{ \lambda_{i} }{\sum_{i}^{k} \lambda_{i}} \Big)^{y_{i}} \\
&amp;amp;\sim MultiNom(n; \frac{\lambda_{1}}{\sum_{i=1}^{k}}, \frac{\lambda_{2}}{\sum_{i=1}^{k}}, &amp;hellip; \frac{\lambda_{k}}{\sum_{i=1}^{k}})
\end{align}$$&lt;/p&gt;
&lt;p&gt;So finally, we see that, given the sum of independent Poisson random variables, that conditional distribution of each element of the Poisson vector is Multinomial distributed, with count probabilities scaled by the sum of the individual rates.  Importantly, we can extend these ideas (specifically the sum of independent Poisson random variables) to other models, such as splitting and merging homogenous and non-homogenous Poisson Point Processes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Headfirst into (Uncommented) C&#43;&#43;</title>
      <link>/post/headfirst-into-uncommented-c&#43;&#43;/</link>
      <pubDate>Mon, 29 Oct 2018 08:12:32 -0700</pubDate>
      <guid>/post/headfirst-into-uncommented-c&#43;&#43;/</guid>
      <description>&lt;p&gt;While most of my day-to-day research entails writing Python code, I also make heavy use of pre-written software.  Most software comes pre-compiled, but whenever possible, I like to get access to the source code.  I&amp;rsquo;m going to refer to some modifications I made to pre-existing packages &amp;ndash; you can find those 
&lt;a href=&#34;https://github.com/kristianeschenburg/ptx3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in my repository here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The most-commonly used open-source package for brain imaging is called 
&lt;a href=&#34;https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FMRIB Software Library&lt;/a&gt; (FSL), which includes tools for processing MRI data, with applications ranging from motion correction and image registration, to modal decomposition methods, among many others.  All of this is made available as a set of pre-compiled C++ binaries.&lt;/p&gt;
&lt;p&gt;I needed to modify FSL&amp;rsquo;s 
&lt;a href=&#34;https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#PROBTRACKX_-_probabilistic_tracking_with_crossing_fibres&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;probtrackx2&lt;/a&gt; tool.  &lt;code&gt;probtrackx2&lt;/code&gt; is a tool for generating probabilistic tractography.  Using diffusion MRI, we can model the movement of water in the brain.  At the voxel level, diffusion tends to be high when water moves along neuronal axon bundles, and low when moving against the myelin or in the extracellular matrix &amp;ndash; this water movement can be modeled using a variety of approaches.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-diffusion-tractography-from-biomedical-image-computing-group-at-usc&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/headfirst-into-uncommented-c&amp;#43;&amp;#43;/tractography_hu8ae5fba2fb76724b387c190a224453c7_1527079_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Diffusion tractography from Biomedical Image Computing Group at USC.&#34;&gt;


  &lt;img data-src=&#34;/post/headfirst-into-uncommented-c&amp;#43;&amp;#43;/tractography_hu8ae5fba2fb76724b387c190a224453c7_1527079_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1194&#34; height=&#34;964&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Diffusion tractography from Biomedical Image Computing Group at USC.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;At the simplest level, the diffusion can be modeled as a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;diffusion tensor&lt;/a&gt;, where the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eigenvalues&lt;/a&gt; of the tensor correspond to the amount of diffusion in the direction of the corresponding eigenvector.  At the more complex levels, we can represent the diffusion as a 3D 
&lt;a href=&#34;https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.22365&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;probability distribution function&lt;/a&gt;, whose marginal distributions are called &lt;strong&gt;orientation distribution functions&lt;/strong&gt; (ODF), and represent these continuous functions using a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Spherical_harmonics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spherical harmonics&lt;/a&gt; basis set of the ODF.  Using &lt;code&gt;probtrackx2&lt;/code&gt;, we can sample these ODFs using a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Markov Chain Monte Carlo&lt;/a&gt; approach and &amp;ldquo;walk&amp;rdquo; throught the brain.  Directions where the diffusion signal is high will be sampled more often, and we can generate a robust representation of the macroscale neuronal structural in the brain using these random walks.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-orientation-distribution-functions-from-vega-et-al-2009&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/headfirst-into-uncommented-c&amp;#43;&amp;#43;/ODFs_huceff4469533f5f4fc77dbf3d55079dc6_189490_2000x2000_fit_q90_lanczos.jpeg&#34; data-caption=&#34;Orientation distribution functions from Vega et al. 2009.&#34;&gt;


  &lt;img data-src=&#34;/post/headfirst-into-uncommented-c&amp;#43;&amp;#43;/ODFs_huceff4469533f5f4fc77dbf3d55079dc6_189490_2000x2000_fit_q90_lanczos.jpeg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;838&#34; height=&#34;556&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Orientation distribution functions from Vega et al. 2009.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The diffusion signal at the gray matter / white matter interface of the cortex is more isotropic than within the white matter (e.g. the diffusion tensors in these regions are more spherical).  To reduce noise in my fiber tracking results due to this low signal, &lt;strong&gt;I wanted to be able to force the first steps of the streamline propagation algorithm to follow a specific direction into the white matter, before beginning the MCMC sampling procedure&lt;/strong&gt;. Essentially what this boils down to is providing &lt;code&gt;probtrackx2&lt;/code&gt; with prespecified spherical coordinates (azimuthal and polar angles) for the first propagation step.  More specifically, I computed the initial spherical coordinates using surfaces computed from the mesh curvature flow results of 
&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1053811917310583&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;St.-Onge et al.&lt;/a&gt;  Importantly, I wanted to make use of the &lt;code&gt;probtrackx2&lt;/code&gt; infrastructure as much as possible e.g. I didn&amp;rsquo;t want to write my own classes for loading in surface data, and wanted to minimally update the members of any other classes I found useful.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-surface-flow-seeded-tractography-from-st-onge-et-al-2018&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/headfirst-into-uncommented-c&amp;#43;&amp;#43;/StOngeSurfaceFlow_huccc9786cfee1a75fb9df9e205b998b03_376537_2000x2000_fit_lanczos_3.png&#34; data-caption=&#34;Surface-flow seeded tractography from St-Onge et al. 2018.&#34;&gt;


  &lt;img data-src=&#34;/post/headfirst-into-uncommented-c&amp;#43;&amp;#43;/StOngeSurfaceFlow_huccc9786cfee1a75fb9df9e205b998b03_376537_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;821&#34; height=&#34;237&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Surface-flow seeded tractography from St-Onge et al. 2018.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Jumping under the hood into the &lt;code&gt;probtrackx2&lt;/code&gt; code was a &lt;strong&gt;feat&lt;/strong&gt;.  While the software is sophistcated, it is &lt;em&gt;quite&lt;/em&gt; poorly documented.  As is common with academic code, development generally begins as a way to solve a specific problem in the lab, rather than as a package to be made available for public use.  FSL has been around for a while, and grows in complexity all the time, so the initial academic-oriented mindset has somewhat propagated through their development cycles.  I was able to identify the important classes and make my modifications to these three classes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Particle&lt;/code&gt; in particle.h :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;performs the tracking for a single streamline for a single seed, where MCMC sampling happens&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Seedmanager&lt;/code&gt; in streamlines.h :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;manages the individual seeds, instantiates &lt;code&gt;Particle&lt;/code&gt; objects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Counter&lt;/code&gt; in streamlines.h :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;keeps track of streamline coordinates in 3D-space, successful streamlines, binary brain masks, saves fiber count distributions as brain volumes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The bulk of the tracking is done using these three &amp;ndash; the rest of the &lt;code&gt;probtrackx2&lt;/code&gt; code is almost entirely devoted to parsing other options and handling other input data.  While I now have a lot of work to do in actually &lt;em&gt;using&lt;/em&gt; my modifications, this foray into FSL&amp;rsquo;s source code re-emphasized three important lessons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Documentation is &lt;strong&gt;critical&lt;/strong&gt;.  But not just any documentation &amp;ndash; &lt;strong&gt;meaningful&lt;/strong&gt; documentation.  Even if you aren&amp;rsquo;t the best at object-oriented software development, at least describe what your code does, and give your variables meaningful names.  Had their code been effectively documented, I could have been in and out of there in two or three days, but instead spent about a week figuring out what was actually going on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You should be equally comfortable working with raw code developed by others, as you are writing your own.  Do not expect everything to be written correctly, and do not assume that just because others have used a piece of software before, that you won&amp;rsquo;t need to make modifications.  Be ready to get your hands dirty.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do not underestimate the power of compiled languages.  Most data scientists work with Python and R due to the speed of development and low barrier to entry, but each is based primarily in C (and I believe not in C++ due to timing of original development cycles).  Many large-scale software packages are based on languages like C, C++, and Java.  Likewise, if your work bridges the gap between data scientist and engineer, you&amp;rsquo;ll definitely need to be comfortable working with compiled languages for production-level development and deployment.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Image Transformations With OpenCV</title>
      <link>/post/image-transformations-with-opencv/</link>
      <pubDate>Sat, 01 Sep 2018 17:12:32 -0700</pubDate>
      <guid>/post/image-transformations-with-opencv/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been toying around with 
&lt;a href=&#34;https://opencv.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;openCV&lt;/a&gt; for generating MRI images with synethetic motion injected into them.  I&amp;rsquo;d never used this library before, so I tested a couple examples.  Below I detail a few tools that I found interesting, and that can quickly be used to generate image transformations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import necessary libraries
import matplotlib.pyplot as plt
import nibabel as nb
import cv2

# load image file
image_file = &#39;./data/T1w_restore_brain.nii.gz&#39;
img_obj = nb.load(image_file)
img = img_obj.get_data()

# reorient so Anterior-Posterior axis corresponds to dim(0)
img = np.fliplr(img)
img = np.swapaxes(img, 0, 2)

# get single image slice and rescale
data = img[130, :, :]
data = (data-data.min())/data.max()
plt.imshow(data)
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/image-transformations-with-opencv/original_hu750cff2b29f8ab0c1202b1613b5f60dd_36557_2000x2000_fit_q90_lanczos.jpg&#34; &gt;


  &lt;img data-src=&#34;/post/image-transformations-with-opencv/original_hu750cff2b29f8ab0c1202b1613b5f60dd_36557_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;432&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;For any linear transformations with &lt;code&gt;cv2&lt;/code&gt;, we can use the &lt;code&gt;cv2.warpAffine&lt;/code&gt; method, which takes in the original image, some transformation matrix, and the size of the output image.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with translations.  The matrix will translate the image 10 pixels to the right (width), and 0 pixels down (height).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use the identity rotation matrix
# Third column specifies translation in corresponding direction
translation = np.array([[1, 0, 20],
                        [0, 1, 0]])

translated = cv2.warpAffine(data, translation, data.T.shape)
plt.imshow(translated)
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/image-transformations-with-opencv/translated_hu94c81a5d7bb60bf4db2e87af659f010a_33254_2000x2000_fit_q90_lanczos.jpg&#34; &gt;


  &lt;img data-src=&#34;/post/image-transformations-with-opencv/translated_hu94c81a5d7bb60bf4db2e87af659f010a_33254_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;432&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Now, in order to rotate the image, we can use &lt;code&gt;cv2.getRotationMatrix2D&lt;/code&gt;.  We&amp;rsquo;ll rotate our image by 45$^{\circ}$ .&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# get shape of input image
rows, cols = data.shape

# specify angle of rotation around central pixel
M = cv2.getRotationMatrix2D((cols/2,rows/2), 45, 1)
rotated = cv2.warpAffine(data, M, (cols, rows))
plt.imshow(rotated)
&lt;/code&gt;&lt;/pre&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/image-transformations-with-opencv/rotated_hue596f733d1d3cf78252003188d86a0a7_34285_2000x2000_fit_q90_lanczos.jpg&#34; &gt;


  &lt;img data-src=&#34;/post/image-transformations-with-opencv/rotated_hue596f733d1d3cf78252003188d86a0a7_34285_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;432&#34; height=&#34;432&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here are a few examples of randomly translating +/- 1, 5, or 9 voxels in the X and Y directions, and randomly rotating by 1, 5, or 9 degrees:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# get shape of input image
rows, cols = data.shape
# specify range of rotations and translations
txfn = [1, 5, 9]

for rt in txfn:

    # generate rotation matrix
    # randonly rotate to left or right
    M = cv2.getRotationMatrix2D(
            (cols/2, rows/2),
            np.random.choice([-1, 1], 1)[0]*rt, 1)

    # apply rotation matrix
    rotated = cv2.warpAffine(data, M, data.T.shape)

    # generate translation matrix
    # randomly translate to left or right
    T = np.array(
            [[1,0,np.random.choice([-1, 1], 1)[0]*rt],
            [0, 1,np.random.choice([-1, 1], 1)[0]*rt]]).astype(np.float32)

    # apply translation matrix
    translated = cv2.warpAffine(data, T, data.T.shape)

    # compose rotated and translated images
    movement = (rotated + translated)/2
    # compute difference between input and transformed
    difference = data-movement
    res = difference.reshape(np.product(difference.shape))

    fig,[ax1,ax2,ax3] = plt.subplots(1,3,figsize=(15,5))
    ax1.imshow(movement, cmap=&#39;gray&#39;)
    ax1.set_title(&#39;Composed Random Rotation and Translation \n Magnitude = {:}&#39;.format(rt), fontsize=15)
    ax2.imshow(D-rotated, cmap=&#39;gray&#39;)
    ax2.set_title(&#39;Difference Map&#39;.format(rt), fontsize=15)
    ax3.hist(res[res!=0],100,density=True)
    ax3.set_title(&#39;Difference Density&#39;, fontsize=15)
    plt.tight_layout()
    plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;




  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/image-transformations-with-opencv/Composed.1_hu9d0cf9c043b63202c6d7fbae794c90aa_75234_2000x2000_fit_q90_lanczos.jpg&#34; &gt;


  &lt;img data-src=&#34;/post/image-transformations-with-opencv/Composed.1_hu9d0cf9c043b63202c6d7fbae794c90aa_75234_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1080&#34; height=&#34;360&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;






  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/image-transformations-with-opencv/Composed.5_hu9d0cf9c043b63202c6d7fbae794c90aa_79081_2000x2000_fit_q90_lanczos.jpg&#34; &gt;


  &lt;img data-src=&#34;/post/image-transformations-with-opencv/Composed.5_hu9d0cf9c043b63202c6d7fbae794c90aa_79081_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1080&#34; height=&#34;360&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;






  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/image-transformations-with-opencv/Composed.9_hu9d0cf9c043b63202c6d7fbae794c90aa_81096_2000x2000_fit_q90_lanczos.jpg&#34; &gt;


  &lt;img data-src=&#34;/post/image-transformations-with-opencv/Composed.9_hu9d0cf9c043b63202c6d7fbae794c90aa_81096_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1080&#34; height=&#34;360&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;While this approach of generating synthetic motion into MRI images is a poor model of how motion actually occurs during an MRI scan, there are a few things I learned here.  For example, if you define a measure of image similarity, like mutual information, entropy, or correlation ratio as a cost function, we can see how we can use &lt;code&gt;warpAffine&lt;/code&gt; to find the optimal transformation matrix between two images.&lt;/p&gt;
&lt;p&gt;I was hoping to use openCV to generate and apply 3d affine transformations to volumetric MRI data.  One approach to doing this is to iteratively apply rotations and transformations along each axis &amp;ndash; however, openCV will interpolate the data after each transformation, resulting in a greater loss of signal than I am willing to compromise on.  It doesn&amp;rsquo;t seem like openCV has ability to apply 3d affine transformations to volumetric data in a single interpolation step.&lt;/p&gt;
&lt;p&gt;A more realistic approach to generating synthetic motion artifacts that would more accurately parallell the noise-generating process, is to compute the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Fast_Fourier_transform&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast Fourier Transform&lt;/a&gt; of my 3d volume, and then apply phase-shifts to the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/K-space_%28magnetic_resonance_imaging%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;k-space&lt;/a&gt; signal &amp;ndash; this will also manifest as motion after applying the inverse FFT.&lt;/p&gt;
&lt;p&gt;After doing a bit more digging through the openCV API, it seems there&amp;rsquo;s a lot of cool material for exploration &amp;ndash; these applications specifically caught my eye and would be fun to include in projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_video/py_table_of_contents_video/py_table_of_contents_video.html#py-table-of-content-video&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video analysis&lt;/a&gt; for motion tracking&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html#face-detection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;object recognition&lt;/a&gt; for detecting faces&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://opencv.org/platforms/android/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;openCV Android&lt;/a&gt; for app development&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But alas &amp;ndash; the search continues!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two Weeks of Open Science: A Rekindled Flame</title>
      <link>/post/two-weeks-of-open-science/</link>
      <pubDate>Sat, 18 Aug 2018 01:12:33 -0700</pubDate>
      <guid>/post/two-weeks-of-open-science/</guid>
      <description>&lt;p&gt;I recently attended 
&lt;a href=&#34;http://neurohackademy.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neurohackademy 2018&lt;/a&gt;, hosted by the University of Washington&amp;rsquo;s 
&lt;a href=&#34;https://escience.washington.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eScience Institute&lt;/a&gt;, and organized by Dr. Ariel Rokem and Dr. Tal Yarkoni.&lt;/p&gt;
&lt;p&gt;This was a 2-week long event, beginning with a series of daily lectures, and ending with a fast-paced, high-intensity scramble to put together a beta (but working) version of some project coupling neuroimaging with software development.  The lectures varied in topic, from how to test academic code and how organize open source projects, to machine learning and algorithms for low-dimensional representations of neural recordings, to neuroethics (full lecture list 
&lt;a href=&#34;https://neurohackademy.org/neurohack_year/2018/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Many of the lecturers are scientists and developers who I&amp;rsquo;ve looked up to for years &amp;ndash; a few have even been my intellectual, and now post-Neurohackademy, philosophical role models.  While the lectures were enlightening in their own right, there was a level of intimacy during these 2 weeks that&amp;rsquo;s been unmatched during grad school so far.  Rarely do young researchers like myself get to pick the brains of and engage in scientific banter with scientists whose papers they read, or whose updates they follow, in such a fluid and collaborative setting.  I feel a little weird being so enthusiastic about it (specifically because I know some of them might read this), but (and I think I speak for all of us who participated) it was a richly rewarding experience.
&lt;br/&gt;&lt;/p&gt;





  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;/post/two-weeks-of-open-science/Couch_RickMorty_hua8d62f86464ff5493405d877a1b50d1e_453080_2000x2000_fit_lanczos_3.png&#34; &gt;


  &lt;img data-src=&#34;/post/two-weeks-of-open-science/Couch_RickMorty_hua8d62f86464ff5493405d877a1b50d1e_453080_2000x2000_fit_lanczos_3.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;950&#34; height=&#34;534&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The 
&lt;a href=&#34;https://twitter.com/search?q=%23nh18&amp;amp;src=tyah&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#NH18&lt;/a&gt; participants varied in status from graduate students, to post-docs, to industry members, and traveled from all around the world to Seattle, but each one of us was, in one way or another, involved with neuroscience research.  As one of my new friends put it on Twitter: 
&lt;a href=&#34;https://twitter.com/rxxqx/status/1027238653662093313&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Spent yesterday in a room full of relative strangers who were collaborating, mentoring, &amp;amp; supporting. Devoid of egos or tribalism. Feels like what science (&amp;amp; society) should be.&amp;rdquo;&lt;/a&gt;  The sense of community was strong, positivity was plentiful, and people supported one another &amp;ndash; without regard for experience or any sense of &lt;em&gt;return-on-investment&lt;/em&gt;.  We had an established &lt;strong&gt;Git person&lt;/strong&gt;, &lt;strong&gt;Python person&lt;/strong&gt;, &lt;strong&gt;data viz person&lt;/strong&gt;, &lt;strong&gt;fMRI person&lt;/strong&gt;, &lt;strong&gt;C++ person&lt;/strong&gt;, etc. &amp;ndash; if you had a question or ran into an issue, with high probability there was someone who could and would help you out.  Everyone was genuinely exicted to learn, to share, to create, to bond, and especially, &lt;em&gt;~to neuro/computer/data-science~&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&amp;mdash; &lt;strong&gt;to science&lt;/strong&gt; &amp;mdash;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(origin: probably Newton)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;verb&lt;/em&gt;. To perform scientific research, almost always in a smooth or cool way.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I worked on a project directly related to my research, but that I&amp;rsquo;d only previously written some messy, non-shareable scripts for.  Conveniently, my co-NeuroHacker 
&lt;a href=&#34;https://twitter.com/miyka_el&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michael Notter&lt;/a&gt; had a similar idea and we hit the ground running with some of our colleagues.  The project, titled 
&lt;a href=&#34;https://kristianeschenburg.github.io/parcellation_fragmenter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;parcellation_fragmenter&lt;/a&gt;, provides a means for fragmenting the brain cortex into a predefined number of regions, or a set of regions each of the same size.  These regions can be anatomically constrained, or arbitratilly spread across the cortex.  Our goal was to use this tool to speed up statistical tests, like 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/17825583&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SearchLight FDR&lt;/a&gt;, or as a feature extraction method for down-stream machine learning applications.  I&amp;rsquo;m currently using this tool to examine how cortical network resolution impacts pairwise regional network properties.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet tw-align-center&#34; data-lang=&#34;en&#34; display=&#34;block&#34; margin-left=&#34;auto&#34; margin-right=&#34;auto&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Here is something pretty! It was created with the new parcellation fragmenter (&lt;a href=&#34;https://t.co/9VLCpr336Y&#34;&gt;https://t.co/9VLCpr336Y&lt;/a&gt;), developed by Kristian Eschenburg, &lt;a href=&#34;https://twitter.com/kako_toro?ref_src=twsrc%5Etfw&#34;&gt;@kako_toro&lt;/a&gt;, Amanda Sidwell &amp;amp; me during the &lt;a href=&#34;https://twitter.com/hashtag/NHW18?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#NHW18&lt;/a&gt;. Thank&amp;#39;s to &lt;a href=&#34;https://twitter.com/hashtag/nilearn?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#nilearn&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://twitter.com/hashtag/nibabel?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#nibabel&lt;/a&gt; creating this toolbox was straightforward and a lot of fun! &lt;a href=&#34;https://t.co/LdqTCMSyrJ&#34;&gt;pic.twitter.com/LdqTCMSyrJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Michael Notter (@miyka_el) &lt;a href=&#34;https://twitter.com/miyka_el/status/1028027334245285889?ref_src=twsrc%5Etfw&#34;&gt;August 10, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;My most important takeaway from our project was learning how to collaboratively write and develop software with a group of people.  Not only did each of our team members have unique ideas about how to approach our specific problem, we also each had different ways of thinking about &lt;em&gt;how to write software in general&lt;/em&gt;.  Clear communication, open-mindedness, and understanding on all of our parts were integral to seeing this development through.  Overall, the project was a success!&lt;/p&gt;
&lt;p&gt;Here are a few things things I learned, that I&amp;rsquo;m going to incorporate into my own work (and hopefuly convince people at lab to do the same):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unit-test my code using &lt;code&gt;pytest&lt;/code&gt; and &lt;code&gt;nose&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Incorporate continuous integration (I&amp;rsquo;ve already made use of [TravisCI]({% post_url 2018-08-12-custom-plugins-with-travisci %})!)&lt;/li&gt;
&lt;li&gt;Learn web-dev, and specifically, JavaScript (to use D3, and develop interactive posters and publications)&lt;/li&gt;
&lt;li&gt;Contribute to issues / create pull-requests on GitHub repos that I use or find interesting&lt;/li&gt;
&lt;li&gt;Pre-register my papers and submit to open-source journals&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This event was what I&amp;rsquo;d hoped graduate school would be like all along.  While idealistic and naiive to some degree, I still think it can be.  The open-source model is shifting how research is performed &amp;ndash; the act of doing research is evolving in such a way that it is no longer tethered to specific institutions or labs, and given tools like Docker and AWS, you can almost perfectly recreate specific computing environments needed to perform the work.  With the rise of open-source datasets, especially due to researchers willingly distributing their data and code, collaborative environments like that fostered by Neurohackademy (even if digital), and the ability to replicate workflows, results, and analyses, are becoming more and more feasible.  It only takes a few proponents of the open-source model to give the idea momentum.&lt;/p&gt;
&lt;p&gt;If this is the future, the future is looking good.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Enabling Custom Jekyll Plugins with TravisCI</title>
      <link>/post/enabling-custom-jekyll-plugins/</link>
      <pubDate>Sun, 12 Aug 2018 02:14:14 -0700</pubDate>
      <guid>/post/enabling-custom-jekyll-plugins/</guid>
      <description>&lt;p&gt;I just learned about 
&lt;a href=&#34;https://travis-ci.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TravisCI&lt;/a&gt; (actually, about continuous integration (CI) in general) after attending 
&lt;a href=&#34;http://neurohackademy.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neurohackademy 2018&lt;/a&gt;.  We learned about CI from the perspective of ensuring that your code builds properly when you update files in your packages, incorporate new methods, refactor your code, etc.  Pretty neat.&lt;/p&gt;
&lt;p&gt;Fast forward a couple days, and I&amp;rsquo;m trying to incorporate custom Jekyll plugins into my blog &amp;ndash; I quickly realized GitHub doesn&amp;rsquo;t allow this for security reasons, but I couldn&amp;rsquo;t find  a convenient work-around.  Some posts suggested using a separate repo branch to build the site, and then push the static HTML files up to a remote repo to do the actual hosting, but for some reason I couldn&amp;rsquo;t get that approach to work.&lt;/p&gt;
&lt;p&gt;Finally, I saw some mentions of using TravisCI and 
&lt;a href=&#34;https://circleci.com/pricing/?utm_source=gb&amp;amp;utm_medium=SEM&amp;amp;utm_campaign=SEM-gb-200-Eng-ni&amp;amp;utm_content=SEM-gb-200-Eng-ni-Circle-CI&amp;amp;gclid=Cj0KCQjwtb_bBRCFARIsAO5fVvGQIO23w0ahWrTj3v8MrGLEnjI00KcEClqUuQda-Q_cz05h8jjEC5QaAjeREALw_wcB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CircleCI&lt;/a&gt; to build and push the site using continuous integration.  I ended up using the approach suggested by 
&lt;a href=&#34;http://joshfrankel.me/blog/deploying-a-jekyll-blog-to-github-pages-with-custom-plugins-and-travisci/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Josh Frankel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Josh&amp;rsquo;s site gives a really clear explanation of the necessary steps, given some very minmal prequisite knowledge about using Git.  His instructions actually worked almost perfectly for me, so I won&amp;rsquo;t repeat them again here (just follow the link above, if you&amp;rsquo;re interested) &amp;ndash; however, there were a few issues that arose on my end:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For some reason, I had an &lt;code&gt;about.html&lt;/code&gt; file and &lt;code&gt;index.html&lt;/code&gt; file in the main repo directory &amp;ndash; my built blog wouldn&amp;rsquo;t register any updates I made to &lt;code&gt;about.md&lt;/code&gt; or &lt;code&gt;index.md&lt;/code&gt; while these files were around, so I deleted the HTML files.  This might have been an obvious bug to someone with more web programming experience, but I&amp;rsquo;m a novice at that.  If you&amp;rsquo;re seeing any wonky behavior, check to make sure you don&amp;rsquo;t have any unnecessary files hanging around.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ruby version&lt;/strong&gt;:  I had to change the version of Ruby I was using to &lt;code&gt;ruby-2.4.1&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Plugins&lt;/strong&gt;: Make sure any Jekyll plugins you want to use are already installed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Emails&lt;/strong&gt;: You can turn off email reporting from TravisCI by adding
&lt;code&gt;notifications: email: false&lt;/code&gt; to your &lt;code&gt;.travis.yml&lt;/code&gt; file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But now, you can incorporate custom, user-built Jekyll plugins and let TravisCI do the heavy lifting!  I specifically wanted the ability to reference papers using BibTex-style citation links with Jekyll, like you can with LaTex or Endnote &amp;ndash; this capability isn&amp;rsquo;t currently supported by GitHub.  Happy blogging!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rendering LaTex In Markdown Using Jekyll</title>
      <link>/post/rendering-latex-in-markdown-using-jekyll/</link>
      <pubDate>Sat, 11 Aug 2018 02:14:14 -0700</pubDate>
      <guid>/post/rendering-latex-in-markdown-using-jekyll/</guid>
      <description>&lt;p&gt;In putting together this blog, I wanted to be able to talk about various mathematical topics that I found interesting, which inevitably lead to using LaTex in my posts.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m currently using Atom as my editor (having converted from Sublime), and needed to install a bunch of packages first.  First and foremost, I wanted to be able to render my markdown posts before hosting them on the blog, and consequentially needed a way to render LaTex.  For this, I installed a few Atom packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://atom.io/packages/markdown-it-preview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Markdown-Preview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://atom.io/packages/latex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://atom.io/packages/language-latex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language-Latex&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To preview your post in Atom, you just type &lt;code&gt;ctrl+shift+M&lt;/code&gt;, which will display both in-line and block math sections.&lt;/p&gt;
&lt;p&gt;However, if you build your site locally with the command &lt;code&gt;bundle exec jekyll serve&lt;/code&gt; or push it to a remote repo, the LaTex no longer renders properly.  After Googling around a bit, I determined that this was due to the way markdown converters in Jekyll, like &lt;strong&gt;kramdown&lt;/strong&gt; and &lt;strong&gt;redcarpet&lt;/strong&gt;, do the conversion using MathJax &amp;ndash; specifically, in-line math segments are not properly rendered.  I wanted a way to both preview the LaTex in Atom, and properly render it usng Jekyll.  I found two links that solved the problem for me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.gastonsanchez.com/visually-enforced/opinion/2014/02/16/Mathjax-with-jekyll/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visually Enforced&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.iangoodfellow.com/blog/jekyll/markdown/tex/2016/11/07/latex-in-markdown.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX in Jekyll&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In short, the following steps solved the problem of LaTex not rendering for me.  I&amp;rsquo;m using the &lt;strong&gt;minima&lt;/strong&gt; theme, so I first found the theme directory with &lt;code&gt;bundle show minima&lt;/code&gt;.  In this directory, I copied the &lt;strong&gt;./layouts/post.html&lt;/strong&gt; to a local directory in my project folder called &lt;strong&gt;./_layouts/post.html&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Within this file, I pasted the following two sections of HTML code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html:&#34;&gt;&amp;lt;script type=&amp;quot;text/x-mathjax-config&amp;quot;&amp;gt;
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],
        inlineMath: [[&#39;$&#39;,&#39;$&#39;]]
      }
    });
  &amp;lt;/script&amp;gt;
&amp;lt;script src=&amp;quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And voila &amp;ndash; building the posts now correctly renders LaTex!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Neurological Dynamical Systems: Part 2</title>
      <link>/post/exploring-neurological-dynamical-systems-part-2/</link>
      <pubDate>Thu, 24 May 2018 11:30:43 -0700</pubDate>
      <guid>/post/exploring-neurological-dynamical-systems-part-2/</guid>
      <description>&lt;p&gt;In my previous post on 
&lt;a href=&#34;/post/exploring-neurological-dynamical-systems-part-1/&#34;&gt;dynamic mode decomposition&lt;/a&gt;, I discussed the foundations of DMD as a means for linearizing a dynamical system&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.  In this post, I want to look at a way in which we can use rank-updates to incorporate new information into the spectral decomposition of our linear operator, $A$, in the event that we are generating online measurements from our dynamical system&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; &amp;ndash; see the citation below if you want a more-detailed overview of this topic along with open source code for testing this method.&lt;/p&gt;
&lt;p&gt;Recall that we are given an initial data matrix&lt;/p&gt;
&lt;p&gt;$$\begin{align}
X = \begin{bmatrix}
x_{n_{1},m_{1}} &amp;amp; x_{n_{1},m_{2}} &amp;amp; x_{n_{1},m_{3}} &amp;amp; &amp;hellip; \\
x_{n_{2},m_{1}} &amp;amp; x_{n_{1},m_{2}} &amp;amp; x_{n_{2},m_{3}} &amp;amp; &amp;hellip; \\
x_{n_{3},m_{1}} &amp;amp; x_{n_{1},m_{2}} &amp;amp; x_{n_{3},m_{3}} &amp;amp; &amp;hellip; \\
&amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; \\
\end{bmatrix}
\in R^{n \times m}
\end{align}$$&lt;/p&gt;
&lt;p&gt;which we can split into two matrices, shifted one unit in time apart:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
X^{\ast} &amp;amp;=
\begin{bmatrix}
\vert &amp;amp; \vert &amp;amp; \dots &amp;amp; \vert \\
\vec{x}_1 &amp;amp; \vec{x}_2  &amp;amp; \dots &amp;amp; \vec{x}_{m-1}  \\
\vert &amp;amp; \vert &amp;amp; \dots &amp;amp; \vert \\
\end{bmatrix} \in R^{n \times (m-1)} \\
Y &amp;amp;= \begin{bmatrix}
\vert &amp;amp; \vert &amp;amp; \dots &amp;amp; \vert \\
\vec{x}_2 &amp;amp; \vec{x}_3  &amp;amp; \dots &amp;amp; \vec{x}_{m}  \\
\vert &amp;amp; \vert &amp;amp; \dots &amp;amp; \vert \\
\end{bmatrix} \in R^{n \times (m-1)}
\end{align}$$&lt;/p&gt;
&lt;p&gt;and we are interested in solving for the linear operator $A$, such that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y = AX^{\ast}
\end{align}$$&lt;/p&gt;
&lt;p&gt;For simplicity, since we are no longer using the full matrix, I&amp;rsquo;ll just refer to $X^{\ast}$ as $X$.  In the previous post, we made the constraint that $n &amp;gt; m$, and that rank($X$) $\leq m &amp;lt; n$.  Here, however, we&amp;rsquo;ll reverse this assumption, and such that $m &amp;gt; n$, and that rank($X$) $\leq m &amp;lt; n$, such that $XX^{T}$ is invertible, so by multiplying both sides by $X^{T}$ we have&lt;/p&gt;
&lt;p&gt;$$\begin{align}
AXX^{T} &amp;amp;= YX^{T} \\
A &amp;amp;= YX^{T}(XX^{T})^{-1} \\
A &amp;amp;= QP_{x}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $Q = YX^{T}$ and $P_{x} = (XX^{T})^{-1}$.  Now, let&amp;rsquo;s say you observe some new data $x_{m+1}, y_{m+1}$, and you want to incorporate this new data into your $A$ matrix.  As in the previous post on 
&lt;a href=&#34;/post/rank-one-updates/&#34;&gt;rank-one updates&lt;/a&gt;, we saw that directly computing the inverse could potentially be costly, so we want to refrain from doing that if possible.  Instead, we&amp;rsquo;ll use the Shermann-Morrison-Woodbury theorem again to incorporate our new $x_{m+1}$ sample into our inverse matrix, just as before:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
(X_{m+1}X^{T}&lt;em&gt;{m+1})^{-1} = P&lt;/em&gt;{x} + \frac{P_{x}x_{m+1}x_{m+1}^{T}P_{x}}{1 + x_{m+1}^{T}P_{x}x_{m+1}}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Likewise, since we&amp;rsquo;re appending new data to our $Y$ and $X$ matrices, we also have&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y_{m+1} = \begin{bmatrix}
Y &amp;amp; y_{m+1} \end{bmatrix} \\
X_{m+1} = \begin{bmatrix} \\
X &amp;amp; x_{m+1} \end{bmatrix} \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;such that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y_{m+1} X_{m+1}^{T} &amp;amp;= YX^{T} + y_{m+1}x_{m+1}^{T} \\
&amp;amp;= Q + y_{m+1}x_{m+1}^{T}
\end{align}$$&lt;/p&gt;
&lt;p&gt;which is simply the sum of our original matrix $Q$, plus a rank-one matrix.  The authors go on to describe some pretty cool &amp;ldquo;local&amp;rdquo; DMD schemes, by incorporating weights, as well as binary thresholds, that are time-dependent into the computation of the linear operator, $A$.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;P.J. Schmid. 
&lt;a href=&#34;https://hal-polytechnique.archives-ouvertes.fr/file/index/docid/1020654/filename/DMS0022112010001217a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic mode decomposition of numerical and experimental data&lt;/a&gt;. Journal of Fluid Mechanics 656.1. 2010.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Tu et al. 
&lt;a href=&#34;http://cwrowley.princeton.edu/papers/Tu-DMD.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Dynamic Mode Decomposition: Theory And Applications&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Kunert-Graf et al. 
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2019.00075/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extracting Reproducible Time-Resolved Resting State Networks Using Dynamic Mode Decomposition&lt;/a&gt;. Front. Comput. Neurosci. 2019.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Zhang et al. 
&lt;a href=&#34;https://arxiv.org/abs/1707.02876&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Online dynamic mode decomposition for time-varying systems&lt;/a&gt;. 2017.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Neurological Dynamical Systems: Part 1</title>
      <link>/post/exploring-neurological-dynamical-systems-part-1/</link>
      <pubDate>Tue, 22 May 2018 14:02:52 -0700</pubDate>
      <guid>/post/exploring-neurological-dynamical-systems-part-1/</guid>
      <description>&lt;p&gt;In the next two posts, I want to talk briefly about an algorithm called Dynamic Mode Decomposition (DMD).  DMD is a spatiotemporal modal decomposition technique that can be used to identify spatial patterns in a signal (modes), along with the time course of these spatial patterns (dynamics).  As such, the algorithm assumes that the input data has a both a spatial and a temporal component.  We are interested in modeling &lt;em&gt;how&lt;/em&gt; the system evolves over time.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;d like to find more information about DMD, Peter Schmid&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and Jonathan Tu&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; have written excellent expositions on the topic.  Likewise, if you&amp;rsquo;d like to follow along with the code for the following analysis, see 
&lt;a href=&#34;https://github.com/kristianeschenburg/dmd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my repo&lt;/a&gt;.  For a more in-depth analysis that applies DMD to brain activity in the resting brain, see a recent publication by my colleagues and I&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, along with the 
&lt;a href=&#34;https://github.com/kunert/DMD_RSN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; used for our analysis.&lt;/p&gt;
&lt;h2 id=&#34;the-dmd-algorithm&#34;&gt;The DMD Algorithm&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume that you&amp;rsquo;ve taken $n$ measurements from specific points in space for $m$ time points, where for now we assume that $m\lt n$.  For now, we&amp;rsquo;ll assume that the sampling frequency, $\omega$, is stable across the entire experiment.  We define our entire data matrix as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
X = \begin{bmatrix}
x_{n_{1},m_{1}} &amp;amp; x_{n_{1},m_{2}} &amp;amp; x_{n_{1},m_{3}} &amp;amp; &amp;hellip; \\
x_{n_{2},m_{1}} &amp;amp; x_{n_{1},m_{2}} &amp;amp; x_{n_{2},m_{3}} &amp;amp; &amp;hellip; \\
x_{n_{3},m_{1}} &amp;amp; x_{n_{1},m_{2}} &amp;amp; x_{n_{3},m_{3}} &amp;amp; &amp;hellip; \\
&amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; &amp;amp; &amp;hellip; \\
\end{bmatrix}
\in R^{n \times m}
\end{align}$$&lt;/p&gt;
&lt;p&gt;We are interested in solving for the matrix, $A \in R^{n \times n}$, such that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
x_{t+1} = A x_{t} \; \; \forall \; \; t = 1,2,&amp;hellip;m-1
\end{align}$$&lt;/p&gt;
&lt;p&gt;Given our full data matrix $X$, we can define two matrices $X^{*}$ and $Y$ such that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
X^{*} &amp;amp;= \begin{bmatrix}
\vert &amp;amp; \vert &amp;amp; \dots &amp;amp; \vert \\
\vec{x}_{1} &amp;amp; \vec{x}_{2}  &amp;amp; \dots &amp;amp; \vec{x}_{m-1} \\
\vert &amp;amp; \vert &amp;amp; \dots &amp;amp; \vert \\
\end{bmatrix} \in R^{n \times (m-1)} \\
Y &amp;amp;= \begin{bmatrix}
\vert &amp;amp; \vert &amp;amp; \dots &amp;amp; \vert \\
\vec{x}_2 &amp;amp; \vec{x}_3  &amp;amp; \dots &amp;amp; \vec{x}_{m}  \\
\vert &amp;amp; \vert &amp;amp; \dots &amp;amp; \vert \\
\end{bmatrix} \in R^{n \times (m-1)}
\end{align}$$&lt;/p&gt;
&lt;p&gt;so that we can write&lt;/p&gt;
&lt;p&gt;$$\begin{align}
Y = AX^{\ast}
\end{align}$$&lt;/p&gt;
&lt;p&gt;If $n$ is small, this is relatively easy to compute &amp;ndash; however, if $n$ is large, as is the case when modeling temporal dynamics in resting-state MRI, it would be computationally inefficient to compute A directly.  To alleviate this, we can make use of the Singular Value Decomposition (SVD) of our predictor matrix $X^{\ast}$.  We define the SVD of $X^{\ast}$ as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
X^{\ast} = U \Sigma V^{T} \
\end{align}$$&lt;/p&gt;
&lt;p&gt;as well as the Moore-Penrose psuedo-inverse of $X^{\ast} = X^{\dagger}$ as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
X^{\dagger} = V \Sigma^{-1} U^{T} \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;such that we can write&lt;/p&gt;
&lt;p&gt;$$\begin{align}
YX^{\dagger}  = YV \Sigma^{-1} U^{T} = A X^{\ast}X^{\dagger} = A \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;Additionally, if we assume that $rank(X^{\ast}) = r \leq m$, then we can use the truncated SVD such that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
U &amp;amp; \in R^{n \times r} \\
V^{T} &amp;amp; \in R^{r \times m} \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\Sigma = \begin{bmatrix}
\sigma_{1} &amp;amp; 0 &amp;amp; 0 &amp;amp; &amp;hellip; \\
0 &amp;amp; \sigma_{2} &amp;amp; 0 &amp;amp; &amp;hellip; \\
0 &amp;amp; 0 &amp;amp; \ddots &amp;amp; &amp;hellip; \\
\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \sigma_{r} \\
\end{bmatrix} \in R^{r \times r}
\end{align}$$&lt;/p&gt;
&lt;p&gt;As it stands now, we still compute an $A \in R^{n \times n}$ matrix.  However, because we have a potentially low-rank system, we can apply a Similarity Transformation to $A$ in order to reduce its dimensionality, without changing its spectrum.  Using our spatial singular vectors $U$, we define&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\tilde{A} &amp;amp;= U^{T} A U \\
&amp;amp;= U^{T} (YV \Sigma^{-1} U^{T}) U \\
&amp;amp;= U^{T} Y V \Sigma^{-1} \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $\tilde{A} \in R^{r \times r}$.  If we consider the above SVD, we see that $U$ is the matrix of left singular vectors, an orthogonal basis that spans $C(X^{\ast})$, which is an r-dimensional subspace of $R^{n}$.  Thus, the similarity transform represents a mapping $f(A) = U^{T} A U : R^{n} \rightarrow R^{r}$.  We now have a reduced-dimensional representation of our linear operator, from which we can compute the spatial modes and dynamic behavior of each mode.  First, however, because of the notion of variance captured by the singular values of our original predictor matrix, we weight $\tilde{A}$ by the singular values as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\hat{A} = \Sigma^{\frac{1}{2}} \tilde{A} \Sigma^{\frac{1}{2}} \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;such that our computed spatial modes have been weighted by the amount they contribute to our measured signal.  We can now compute the eigendecomposition of $\hat{A}$ as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\hat{A} W = W \Lambda \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the eigenvectors $W$ are the reduced-dimension representations of our spatial modes, and the eigenvalues $\Lambda$ capture the dynamic behavior of our spatial modes.  Because our original data matrix $X^{\ast}$ had spatial dimension $n$ and our eigenvectors have dimension $r$, we need to up-project our eigenvectors $W$ to compute the final spatial modes, via&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\Phi = Y V \Sigma^{\frac{-1}{2}}W
\end{align}$$&lt;/p&gt;
&lt;p&gt;From the SVD of our prediction matrix $X^\ast=U \Sigma V^{T}$, the matrix $V \in R^{m \times r}$ is the matrix of right singular vectors, an orthogonal basis spanning the space of $X^{\ast T}$ (i.e. $r$ basis vectors spanning the space of the measured time courses).  Thus, we see that $H = (V \Sigma^{\frac{-1}{2}})W$ represents a linear combination of the temporal basis vectors (a mapping from $R^{r} \rightarrow R^{m}$) for each eigenvector $w_{i}$ of $W$, weighted by the corresponding singular value $\sigma_{i}^{\frac{-1}{2}}$ (that acts to normalize the spatial mode amplitudes).  Finally, we see that $\Phi = X^{\ast}H$ computes how much of each temporal basis vector is present in the measured time course at each point in space.&lt;/p&gt;
&lt;p&gt;Because we are modeling a dynamical system, we can compute the continuous time dynamics of our system using our spatial modes and eigenvalues as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\vec{x}(t) \approx \sum_{i=1}^{r} b_{i}\exp^{((\gamma_{i} + 2i\pi f_{i})\cdot t)} \vec{\phi}_{i}
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $\gamma_{i}$ is a growth-decay constant and $f_{i}$ is the frequency of oscillation of the spatial mode $\phi_{i}$.  We can compute these two constants as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\gamma_{i} &amp;amp;= \frac{\text{real}(\text{ln}(\lambda_{i}))}{\Delta t} \\
f_{i} &amp;amp;= \frac{\text{imag}(\text{ln}(\lambda_{i}))}{2\pi \Delta t}
\end{align}$$&lt;/p&gt;
&lt;p&gt;So, we can see that DMD linearizes our measured time series, by fitting what can be analogized to a &amp;ldquo;global&amp;rdquo; regression.  That is, instead of computing how a single time point predicts the next time point, which could readily be solved using the simple &lt;strong&gt;Normal equations&lt;/strong&gt;, DMD computes how a matrix of time points predicts another matrix of time points that is shifted one unit of time into the future.  To this extent, DMD minimizes the Frobenius norm of&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\min \limits_{A} \lVert Y - AX^{\ast} \rVert^{2}_{F} \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;However, rather than explicitly computing the matrix $A$, DMD computes the eigenvectors and eigenvalues of $A$, by utilizing the &lt;strong&gt;Singular Value Decomposition&lt;/strong&gt;, along with a &lt;strong&gt;Similarity Transformation&lt;/strong&gt;, in order to generate a reduced-dimensional representation of $A$.&lt;/p&gt;
&lt;p&gt;This spectral decomposition of our linear operator is of particular importance, because it sheds light on the fact the DMD models the temporal dynamics of our system using a &lt;strong&gt;Fourier basis&lt;/strong&gt;.  Each spatial mode is represented by a particular Fourier frequency along and growth-decay constant that determines the future behavior of our spatial mode.  Additionally, the Fourier basis also determines what sorts of time series can be modeled using DMD &amp;ndash; time series that are expected to have sinusoidal behavior will be more reliably modeled using DMD, whereas signals that show abrupt spike patterns might be more difficult to model.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;P.J. Schmid. 
&lt;a href=&#34;https://hal-polytechnique.archives-ouvertes.fr/file/index/docid/1020654/filename/DMS0022112010001217a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dynamic mode decomposition of numerical and experimental data&lt;/a&gt;. Journal of Fluid Mechanics 656.1. 2010.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Tu et al. 
&lt;a href=&#34;http://cwrowley.princeton.edu/papers/Tu-DMD.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Dynamic Mode Decomposition: Theory And Applications&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Kunert-Graf et al. 
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2019.00075/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extracting Reproducible Time-Resolved Resting State Networks Using Dynamic Mode Decomposition&lt;/a&gt;. Front. Comput. Neurosci. 2019.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multivariate Normal Distribution</title>
      <link>/post/multivariate-normal-distribution/</link>
      <pubDate>Sat, 12 May 2018 03:14:14 -0700</pubDate>
      <guid>/post/multivariate-normal-distribution/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;ll be covering the basics of Multivariate Normal Distributions, with special emphasis on deriving the conditional and marginal distributions.&lt;/p&gt;
&lt;p&gt;Given a random variable under the usual Gauss-Markov assumptions, with $y_{i} \sim N(\mu, \sigma^{2})$ with $e \sim N(0,\sigma^{2})$, and $N$ independent samples $y_{1}&amp;hellip;y_{n}$, we can define vector $\mathbf{y} = [y_{1}, y_{2},&amp;hellip;y_{n}] \sim N_{n}(\mathbf{\mu},\sigma^{2}I)$ with $\mathbf{e} \sim N_{n}(\mathbf{0},\sigma^{2}I)$.  We can see from the covariance structure of the errors that all off-diagonal elements are 0, indicating that our samples are independent with equal variances.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Marginal Distributions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now assume that $\mathbf{y} = [\mathbf{y_{1}}, \mathbf{y_{2}} ] \sim N(\mathbf{\mu},\Sigma)$, where $\mathbf{\mu} = \begin{bmatrix} \mu_{1} \ \mu_{2} \end{bmatrix}$, and $\Sigma$ is an arbitrary covariance matrix, where we cannot assume independence.  If $\Sigma$ is non-singular, we can decompose $\Sigma$ as&lt;/p&gt;
&lt;p&gt;$$ \Sigma = \begin{bmatrix}
\Sigma_{11} &amp;amp; \Sigma_{21}^{T} \\
\Sigma_{21} &amp;amp; \Sigma_{22}
\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;and, using the inversion lemmas from [Blockwise Matrix Inversion]({% post_url 2018-05-08-blockwise-matrix-inversion %}), define its inverse $\Sigma^{-1} = V$ as&lt;/p&gt;
&lt;p&gt;$$
V = \begin{bmatrix}
V_{11} &amp;amp; V_{21}^{T} \\
V_{21} &amp;amp; V_{22} \\
\end{bmatrix}
\begin{bmatrix}
(\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})^{-1} &amp;amp; -\Sigma^{-1}\Sigma_{12}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1} \\
-\Sigma_{22}^{-1}\Sigma_{21}(\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})^{-1} &amp;amp; (\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}
\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;From the properties of transformations of Normal random variables, we can define the marginal of $$y_{1}$$ as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
By \sim N(B\mu,B\Sigma B^{T})
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $B = \begin{bmatrix} \mathbf{I} &amp;amp; 0 \end{bmatrix}$ such that&lt;/p&gt;
&lt;p&gt;$$ \begin{bmatrix} \mathbf{I} &amp;amp; 0 \end{bmatrix} \begin{bmatrix} \mathbf{\mu_{1}} \ \mathbf{\mu_{2}} \end{bmatrix} = \mathbf{\mu_{1}}$$
$$\begin{bmatrix} \mathbf{I} &amp;amp; 0
\end{bmatrix} \begin{bmatrix}
\Sigma_{11} &amp;amp; \Sigma_{12} \\
\Sigma_{21} &amp;amp; \Sigma_{22}
\end{bmatrix}
\begin{bmatrix}
\mathbf{I} \\
0
\end{bmatrix} = \Sigma_{11}$$&lt;/p&gt;
&lt;p&gt;so that $\mathbf{y_{1}} \sim N(\mathbf{\mu_{1}},\Sigma_{11})$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conditional Distributions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Showing the conditional distribution is a bit long-winded, so bear with me.  We are interested in finding the distribution of $y_{2}\mid y_{1}$, which we can explicitly represent as&lt;/p&gt;
&lt;p&gt;$$\begin{align}
f_{y_{1}}(y_{2} \mid y_{1}) = \frac{f_{y_{1},y_{2}}(y_{1},y_{2})}{f_{y_{1}}(y_{1})}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Writing out the joint density for $y$, we have the following&lt;/p&gt;
&lt;p&gt;$$\begin{align}
f(y) =  \frac{1}{(2\pi)^{n/2}\mid \Sigma \mid ^{1/2}}\exp^{(-1/2)(y-\mu)^{T}\Sigma^{-1}(y-\mu)}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Partitioning this expression up into the individual terms related to $y_{1}$ and $y_{2}$, the exponent becomes&lt;/p&gt;
&lt;p&gt;$$ (y-\mu)^{T}V(y-\mu) = \begin{bmatrix}
y_{1} - \mu_{1} \\
y_{2} - \mu_{2} \end{bmatrix}^{T}
\begin{bmatrix}
V_{11} &amp;amp; V_{12} \\
V_{21} &amp;amp; V_{22} \end{bmatrix}
\begin{bmatrix}
y_{1} - \mu_{1} \\
y_{2} - \mu_{2}
\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;Expanding this quadratic form out, we see that we end up with&lt;/p&gt;
&lt;p&gt;$$\begin{align}
(y_{1} - \mu_{1})^{T} V_{11}^{-1}(y_{1}-\mu_{1}) + 2(y_{1}-\mu_{1})^{T}V_{12}(y_{2}-\mu_{2}) + (y_{2} - \mu_{2})^{T}V_{22}(y_{2}-\mu_{2})
\end{align}$$&lt;/p&gt;
&lt;p&gt;Let us, for simpliciy set $z_{1} = (y_{1} - \mu_{1})$ and $z_{2} = (y_{2} - \mu_{2})$.  Substituting back in our definitions of $V_{11}$,$V_{12}$,$V_{21}$, and $V_{22}$, and and using the Sherman-Morrison-Woodbury definition for $V_{11}$, we have the following&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;z_{1}^{T}(\Sigma_{11}^{-1} + \Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}\Sigma_{21}\Sigma_{11})z_{1} \\
&amp;amp;- 2z_{1}^{T}(\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}^{-1})^{-1})z_{2} \\
&amp;amp;+ z_{2}^{T}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1})z_{2}
\end{align}$$&lt;/p&gt;
&lt;p&gt;which, by distribution of $z_{1}$ across the first term and splitting the second term into its two sums, we have&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;z_{1}^{T}\Sigma_{11}^{-1}z_{11} + z_{1}^{T}\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22} - \Sigma_{21}V_{11}^{-1}\Sigma_{12})^{-1}\Sigma_{21}\Sigma_{11}^{-1}z_{1} \\
&amp;amp;- z_{1}^{T}(\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1})z_{2} - z_{1}^{T}(\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1})z_{2} \\
&amp;amp;+ z_{2}^{T}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1})z_{2}
\end{align}$$&lt;/p&gt;
&lt;p&gt;We can pull out forms $z_{1}^{T}\Sigma_{11}^{-1}\Sigma_{12}(\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}$ to the left and $(\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}z_{2}$ to the right and, after applying a transpose, have&lt;/p&gt;
&lt;p&gt;$$\begin{align}
=z_{1}^{T}\Sigma_{11}^{-1}z_{11} + (z_{2} -\Sigma_{21}\Sigma_{11}^{-1}z_{1})^{T}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}(z_{2} - \Sigma_{21}\Sigma_{11}^{-1}z_{1})
\end{align}$$&lt;/p&gt;
&lt;p&gt;Plugging the above back into our exponential term in our original density function, we see that we have a product of two exponential terms&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;\frac{1}{C_{1}} \exp(\frac{-1}{2}(z_{1}^{T}\Sigma_{11}^{-1}z_{11})) \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\begin{align}
&amp;amp;\frac{1}{C_{2}}\exp(\frac{-1}{2}(z_{2} - z_{1}\Sigma_{11}^{-1}\Sigma_{12})^{T}(\Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}(z_{2} - \Sigma_{21}\Sigma_{11}^{-1}z_{1}))
\end{align}$$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$\begin{align}
C_{1} &amp;amp;= (2\pi)^{p/2}\mid \Sigma_{11} \mid^{1/2} \\
C_{2} &amp;amp;= (2\pi)^{q/2}\mid \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12} \mid ^{1/2}
\end{align}$$&lt;/p&gt;
&lt;p&gt;The first term is the marginal density of $y_{1}$ and the second is the conditional density of $y_{2} \mid y_{1}$ with conditional mean $\mu_{2\mid 1} = \mu_{2} + \Sigma_{11}^{-1}\Sigma_{12}(y_{1} - \mu_{1})$ and conditional variance $\Sigma_{2\mid 1} = \Sigma_{22} - \Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}$.&lt;/p&gt;
&lt;p&gt;While long and drawn out, the formulas show that the conditional distribution of any subset of Normal random variables, given another subset, is also a Normal distribution, with conditional mean and variance defined by functions of the means and covariances of the original random vector.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rank One Updates</title>
      <link>/post/rank-one-updates/</link>
      <pubDate>Fri, 11 May 2018 16:53:45 -0700</pubDate>
      <guid>/post/rank-one-updates/</guid>
      <description>&lt;p&gt;In this post, I&amp;rsquo;m going to go over some examples of rank-one updates of matrices.  To compute rank-one updates, we rely on the Sherman-Morrison-Woodbury theorem.  From the previous post on [Blockwise Matrix Inversion]({% post_url 2018-05-08-blockwise-matrix-inversion %}), recall that, given a matrix and its inverse&lt;/p&gt;
&lt;p&gt;$$R = \begin{bmatrix}
A &amp;amp; B \\
C &amp;amp; D
\end{bmatrix}  \; \; \; \; R^{-1} = \begin{bmatrix}
W &amp;amp; X \\
Y &amp;amp; Z
\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;we have that&lt;/p&gt;
&lt;p&gt;$$\begin{align}
W = (A-BD^{-1}C)^{-1} = C^{-1}D(D-CA^{-1}B)^{-1}CA^{-1}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Expanding this further, the Woodbury formula proves the following identity&lt;/p&gt;
&lt;p&gt;$$\begin{align}
(A+BD^{-1}C)^{-1}=A^{-1}-A^{-1}B(D−CA^{-1}B)^{-1}CA^{-1}
\end{align}$$&lt;/p&gt;
&lt;p&gt;Given an initial matrix $A$ and its inverse $A^{-1}$, and a new matrix $R=BD^{-1}C$, we see that we can define the inverse of our new updated matrix $A+R$ in terms of the inverse of our original matrix $A$ and components of $R$.  Importantly, we can perform rank-$k$ updates, where $rank(R) = k$.&lt;/p&gt;
&lt;p&gt;For example, if we want to update our matrix $A$ with a new vector, $v$, we can rewrite the formula above as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
(A+vv^{T})^{-1} &amp;amp;=A^{-1}-A^{-1}v(1+v^{T}A^{-1}v)^{-1}v^{T}A^{-1} \\
&amp;amp;=A^{-1}-\frac{A^{-1}vv^{T}A^{-1}}{1+v^{T}A^{-1}v} \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;where the updated inverse is defined so long as the quadratic form $v^{T}A^{-1}v \neq -1$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Rank-One Updates for Linear Models&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recall the Normal equations for linear models:&lt;/p&gt;
&lt;p&gt;$$\begin{align}
X^{T}X\beta = X^{T}y
\end{align}$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\beta = (X^{T}X)^{g}X^{T}y
\end{align}$$&lt;/p&gt;
&lt;p&gt;where $X$ is our design matrix, $y$ is our dependent variable, and $\beta$ is a solution to the Normal equation, due to the fact that the Normal equations are consistent.  $(X^{T}X)^{g}$ is the generalized inverse of $X^{T}X$, which is unique (i.e. $(X^{T}X)^{g} = (X^{T}X)^{-1}$) only if $X$ has full column-rank.  For our immediate purpose, we assume that $X$ has full column rank.&lt;/p&gt;
&lt;p&gt;Assume that we observe a set of observations, $X \in R^{n \times p}$ and response variable, $y$, and compute our coefficient estimates $\hat{\beta}$ via the Normal equations above, using $(X^{T}X)^{-1}$.  Now given a new observation, $v \in R^{p}$, how can we update our coefficient estimates?  We can append $v$ to $X$ as&lt;/p&gt;
&lt;p&gt;$$ X^{\text{*}} = \begin{bmatrix}
X \\
v
\end{bmatrix} \in R^{(n+1) \times p}$$&lt;/p&gt;
&lt;p&gt;and directly compute $(X^{\text{*T}}X^{\text{*}})^{-1}$, &lt;strong&gt;or&lt;/strong&gt; we can use the Sherman-Morrison-Woodbury theorem:&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
(X^{\text{*T}}X^{\text{*}})^{-1} = (X^{T}X + vv^{T})^{-1} = (X^{T}X)^{-1} - \frac{(X^{T}X)^{-1}vv^{T}(X^{T}X)^{-1}}{1+v^{T}(X^{T}X)^{-1}v} \\
\end{align}
$$&lt;/p&gt;
&lt;p&gt;from which we can easily compute our new coefficient estimates with $$.&lt;/p&gt;
&lt;p&gt;$$\begin{align}
\beta^{\text{*}} = (X^{\text{*T}}X^{\text{*}})^{-1}X^{\text{*}T}y \\
\end{align}$$&lt;/p&gt;
&lt;p&gt;Importantly, in the case of regression, for example, this means that we can update our linear model via simple matrix calculations, rather than having to refit the model from scratch to incorporate our new data.  In the next few posts, I&amp;rsquo;ll go over an example of an implementation of rank-updating methods that I&amp;rsquo;ve been using in lab to study brain dynamics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blockwise Matrix Inversion</title>
      <link>/post/blockwise-matrix-inversion/</link>
      <pubDate>Tue, 08 May 2018 23:24:17 -0700</pubDate>
      <guid>/post/blockwise-matrix-inversion/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m taking a Statistics course on the theory of linear models, which covers Gauss-Markov models and various extensions of them.  Sometimes, when dealing with partitioned matrices, and commonly Multivariate Normal Distributions, we&amp;rsquo;ll often need to invert matrices in a blockwise manner.  This has happened often enough during this course (coincidentally was necessary knowledge for a midterm question), so I figured I should just document some of the inversion lemmas.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s define our partitioned matrix as&lt;/p&gt;
&lt;p&gt;$$ R = \begin{bmatrix}
A &amp;amp; B \\
C &amp;amp; D
\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;We specifically interested in finding&lt;/p&gt;
&lt;p&gt;$$ R^{-1} = \begin{bmatrix}
W &amp;amp; X \\
Y &amp;amp; Z
\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;such that&lt;/p&gt;
&lt;p&gt;$$ R R^{-1} = R^{-1}R =
\begin{bmatrix}
I &amp;amp; 0 \\
0 &amp;amp; I
\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 1: $R R^{-1}$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the right inverse ($R R^{-1}$), we can define&lt;/p&gt;
&lt;p&gt;$$ \begin{aligned}
AW + BY = I \\
AX + BZ = 0 \\
CW + DY = 0 \\
CX + DZ = I \\
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;and, assuming $A$ and $D$ are invertible,&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
X = -A^{-1}BZ \\
Y = -D^{-1}CW \\
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;We can plug these identities back into the first system of equations as&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
AW + B(-D^{-1}CW) &amp;amp;= (A - BD^{-1}C)W = I \\
C(-A^{-1}BZ) + DZ &amp;amp;= (D - CA^{-1}B)Z = I \\
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
W = (A-BD^{-1}C)^{-1} \\
Z = (D-CA^{-1}B)^{-1} \\
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;and finally&lt;/p&gt;
&lt;p&gt;$$ R^{-1} = \begin{bmatrix}
W &amp;amp; X \\
Y &amp;amp; Z
\end{bmatrix}
= \begin{bmatrix}
(A-BD^{-1}C)^{-1} &amp;amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\
-D^{-1}C(A-BD^{-1}C)^{-1} &amp;amp; (D-CA^{-1}B)^{-1} \\
\end{bmatrix}$$&lt;/p&gt;
&lt;p&gt;It is important to note that the above result only holds if $A$, $D$, $(D-CA^{-1}B)$, and $(A-BD^{-1}C)$ are invertible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 2: $R^{-1} R$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Following the same logic as above, we have the following systems of equations for the left inverse ($R^{-1}R$)&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
WA + XC = I \\
WB + XD = 0 \\
YA + ZC = 0 \\
YB + ZD = I \\
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
X = WBD^{-1} = A^{-1}BZ \\
Y = ZCA^{-1} = D^{-1}CW \\
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;which indicates that&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}
W = (A-BD^{-1}C)^{-1} = C^{-1}D(D-CA^{-1}B)^{-1}CA^{-1} \\
X = (A-BD^{-1}C)^{-1}BD^{-1} = A^{-1}B(D-CA^{-1}B)^{-1} \\
\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Importantly, blockwise matrix inversion allows us to define the inverse of a larger matrix, with respect to its subcomponents.  Likewise, from here, we can go on to derive the Sherman-Morrison formula and Woodbury theorem, which allows us to do all kinds of cool stuff, like rank-one matrix updates.  In the next few posts, I&amp;rsquo;ll go over a few examples of where blockwise matrix inversions are useful, and common scenarios where rank-one updates of matrices are applicable in the next few posts.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
