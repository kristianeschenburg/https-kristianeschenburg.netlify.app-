[{"authors":["admin"],"categories":null,"content":" I\u0026rsquo;m a Data Scientist, and former Biomedical Engineer, living in the Pacific Northwest. I completed my PhD at the University of Washington where I developed computational methods to explore relationships between brain structure and brain function using magnetic resonance imaging. My primary research focus was on designing computer vision models to map the human brain using various MRI modalities, and relating these learned cortical maps to various biological and neurological processes. I have extensive experience in medical imaging, image processing, machine learning, high-dimensional data analysis, and data visualization.\nI\u0026rsquo;m a firm believer in the idea that code is as much a part of hypothesis-driven design and the scientific process as any other tool or instrument, and that one of the best ways to drive computational sciences forward is through open-source software development, open-access publication, and technical writing.\nWhen I\u0026rsquo;m not thinking about science, I like to ski up (and down) hill, mountaineer, trail run, and tend to my plants. I\u0026rsquo;m driven by curiosity and fueled by coffee. I\u0026rsquo;m currently looking for full-time Data Scientist and Machine Learning Engineer roles.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I\u0026rsquo;m a Data Scientist, and former Biomedical Engineer, living in the Pacific Northwest. I completed my PhD at the University of Washington where I developed computational methods to explore relationships between brain structure and brain function using magnetic resonance imaging.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":"I was recently tasked with examining databases related to some computer vision tools that my company had acquired. Basically, the framework was as follows\u0026hellip; Clients/users would sign up for some service with the goal in mind of building a model to classify a set of microscopy images. These models could then be used by the client for downstream services. The users interacted with this tool via through a WebApp, through which they could upload the relevant training and validation data, as well as the full suite of models that they had previously trained. The software used AWS EC2 instances to train and test their models, and a database to store all the relevant metadata associated with the users and experiments.\nHowever, I was not provided with any relevant information about the database schema, rendering interpretation of the API and results difficult. Given that I had access to the MySQL database, I wanted to be able to visualize the interactions between all the relevant tables.\nAssuming that mysql and MySQL Workbench are installed, and that you know the database name and corresponding account password, you can run the following command to export the database to an SQL file. The -u, -p, and --no-data options correspond to the user ID, password, and desire (or lack thereof) to export the data entries as well.\nmysqldump -u root -p --no-data ${DB_NAME} \u0026gt; ${OUTPUT_NAME}.sql  You can then reverse engineer the database schema by clicking Database -\u0026gt; Reverse Engineer and subsequent the steps:\n  We can then actually visualize the database schema and table structures in MySQL Workbench. Below, we see that all SQL tables are joined to the table called users via their corresponding user_id field. At a minimum, we\u0026rsquo;ll be able to sort experiments by user_id \u0026ndash; however, I\u0026rsquo;m hoping there is also a join for model_name or something along those lines, so that we can more easily interrogate the specific parameterizations of each model.\n  ","date":1645550657,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645550657,"objectID":"736d0e2c82d35f210adf3e89eea6c0cb","permalink":"/post/sql-schema/","publishdate":"2022-02-22T10:24:17-07:00","relpermalink":"/post/sql-schema/","section":"post","summary":"I was recently tasked with examining databases related to some computer vision tools that my company had acquired. Basically, the framework was as follows\u0026hellip; Clients/users would sign up for some service with the goal in mind of building a model to classify a set of microscopy images.","tags":[],"title":"Visualizing SQL Schemas","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m interested in looking at some spatial mappings between pairs of cortical regions, and believe that these mappings are mediated, to some degree, by the temporal coupling between cortical areas. I don\u0026rsquo;t necessarily know the functional form of these mappings, but neurobiologically predict that these mappings are not random and have some inherent structure. I want to examine the relationship between spatial location and strength of temporal coupling. I\u0026rsquo;m going to use mutual information to measure this association.\nIt\u0026rsquo;s been a while since I\u0026rsquo;ve worked with information-based statistics, so I thought I\u0026rsquo;d review some proofs here.\nEntropy Given a random variable $X$, we define the entropy of $X$ as\n$$\\begin{align} H(X) = - \\sum_{x} p(x) \\cdot log(p(x)) \\end{align}$$\nEntropy measures the degree of uncertainty in a probability distribution. It is independent of the values $X$ takes, and is entirely dependent on the density of $X$. We can think of entropy as measuring how \u0026ldquo;peaked\u0026rdquo; a distribution is. Assume we are given a binary random variable $Y$\n$$\\begin{align} Y \\sim Bernoulli(p) \\\\\n\\end{align}$$\nsuch that\n$$\\begin{align} Y = \\begin{cases} 1 \u0026amp; \\text{with probability $p$} \\\\\n0 \u0026amp; \\text{with probability $1-p$} \\end{cases} \\end{align}$$\nIf we compute $H(Y)$ as a function of $p$ and plot this result, we see the canonical curve:\n  Immediately evident is that the entropy curve peaks when $p=0.5$. We are entirely uncertain what value $y$ will take if we have an equal chance of sampling either 0 or 1. However, when $p = 0$ or $p=1$, we know exactly which value $y$ will take \u0026ndash; we aren\u0026rsquo;t uncertain at all.\nEntropy is naturally related to the conditional entropy. Given two variables $X$ and $Y$, conditional entropy is defined as\n$$\\begin{align} H(Y|X) \u0026amp;= -\\sum_{x}\\sum_{y} = p(x,y) \\cdot log(\\frac{p(x,y)}{p(x)}) \\\\\n\u0026amp;= -\\sum_{x}\\sum_{y} p(y|x) \\cdot p(x) \\cdot log(p(y|x)) \\\\\n\u0026amp;= -\\sum_{x} p(x) \\sum_{y} p(y|X=x) \\cdot log(p(y|X=x)) \\end{align}$$\nwhere $H(Y|X=x) = -\\sum_{y} p(y|X=x) \\cdot log(p(y|X=x))$, the conditional of entropy of $Y$ given that $X=x$. Here, we\u0026rsquo;ve used the fact that $p(x,y) = p(y|x) \\cdot p(x) = p(x|y) \\cdot p(y)$.To compute $H(Y|X)$, we take the weighted average of these conditional entropies, where weights are defined by the marginal probabilities of $X$.\nMutual Information Related to entropy is the idea of mutual information. Mutual information is a measure of the mutual dependence between two variables. We can ask the following question: does knowing something about variable $X$ tell us anything about variable $Y$?\nThe mutual information between $X$ and $Y$ is defined as:\n$$\\begin{align} I(X,Y) \u0026amp;= \\sum_{x}\\sum_{y} p(x,y) \\cdot log\\Big(\\frac{p(x,y)}{p(x) \\cdot p(y)}\\Big) \\\\\n\u0026amp;= \\sum_{x}\\sum_{y}p(x,y) \\cdot \\log(p(x,y)) - \\sum_{x}\\sum_{y}p(x,y) \\cdot log(p(x)) - \\sum_{x}\\sum_{y}p(x,y) \\cdot log(p(y)) \\\\\n\u0026amp;= -H(X,Y) - \\sum_{x}p(x) \\cdot log(p(x)) - \\sum_{y}p(y) \\cdot log(p(y)) \\\\\n\u0026amp;= H(X) + H(Y) - H(X,Y) \\end{align}$$\n$I(X,Y)$ is symmetric in $X$ and $Y$:\n$$\\begin{align} I(X,Y) \u0026amp;= \\sum_{x}\\sum_{y} p(x,y) \\cdot log\\Big(\\frac{p(x,y)}{p(x) \\cdot p(y)}\\Big) \\\\\n\u0026amp;= \\sum_{x}\\sum_{y} p(x,y) \\cdot log\\Big(\\frac{p(x|y)}{p(y)}\\Big) \\\\\n\u0026amp;= \\sum_{x}\\sum_{y} p(x,y) \\cdot log(p(x|y)) - \\sum_{x}\\sum_{y}p(x,y) \\cdot log(p(x)) \\\\\n\u0026amp;= -H(X|Y) - \\sum_{x}\\sum_{y} p(x|y) \\cdot p(y) \\cdot log(p(x)) \\\\\n\u0026amp;= -H(X|Y) - \\sum_{x}log(p(x) \\sum_{y}p(x|y) \\cdot p(y) \\\\\n\u0026amp;= -H(X|Y) - \\sum_{x} p(x) \\cdot log(p(x)) \\\\\n\u0026amp;= H(X) - H(X|Y) \\\\\n\u0026amp;= H(Y) - H(Y|X) \\end{align}$$\nWe interpret the above to mean the following: if we are given any information about X (Y), can we reduce the uncertainty around what Y (X) should be? We understand how much variability there is in each variable independently \u0026ndash; this is measured by the marginal entropy $H(Y)$. If knowing $X$ reduces this uncertainty, then the conditional entropy $H(Y|X)$ should be small. If knowing $X$ does not reduce this uncertainty, then $H(Y|X)$ can be at most as large as $H(Y)$, and we have learned nothing about our dependent variable $Y$.\nPut another way, if $I(X,Y) = H(Y) - H(Y|X)$ is large, then the mutual information between $X$ and $Y$ is large, indicating that $X$ is informative of $Y$. However, if $I(X,Y)$ is small, then the mutual information is small, and $X$ is not informative of $Y$.\nApplication For my problem, I\u0026rsquo;m given two variables, $Z$ and $C$. I\u0026rsquo;m interested in examining how knowledge of $C$ might reduce our uncertainty about $Z$. $C$ itself is defined by a pair of variables, $A$, and $B$, such that we have $C_{1} = (a_{1}, b_{1})$. $Z$ is distributed over the tensor-product space of $A$ and $B$, that is:\n$$\\begin{align} Z = f(A \\otimes B) \\end{align}$$\nwhere $A \\otimes B$ is defined as\n$$\\begin{align} A \\otimes B = \\begin{bmatrix} (a_{1},b_{1}) \u0026amp; \\dots \u0026amp; (a_{1},b_{k}) \\\\\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n(a_{p}, b_{1}) \u0026amp; \\dots \u0026amp; (a_{p}, b_{k}) \\end{bmatrix} \\end{align}$$\nsuch that $z_{i,j} = f(a_{i}, b_{j}$)\n  We define the mutual information between $Z$ and $C$ as\n$$\\begin{align} I(Z,C) \u0026amp;= \\sum_{z}\\sum_{c} p(z,c) \\cdot log\\Big(\\frac{p(z,c)}{p(z)p(c)} \\Big) \\\\\n\u0026amp;= H(Z) - H(Z|C) \\\\\n\u0026amp;= H(Z) - H(X|(A,B)) \\\\\n\u0026amp;= H(Z) - \\sum_{a,b} p(a,b) \\sum_{z} p(z|(A,B)=(a,b)) \\cdot log(p(z|(A,B)=(a,b))) \\end{align}$$\nwhere the pair $(a,b)$ represents a bin or subsection of the tensor-product space. The code for this approach can be found below:\nimport numpy as np def entropy(sample): \u0026quot;\u0026quot;\u0026quot; Compute the entropy of a given data sample. Bins are estimated using `Freedman Diaconis` Estimator. Args: sample: float, NDarray data from which to compute entropy of Returns: H: entropy \u0026quot;\u0026quot;\u0026quot; if np.ndim(sample) \u0026gt; 1: sample = np.reshape(sample, np.product(sample.shape)) edges = np.histogram_bin_edges(sample, bins='fd') [counts,_] = np.histogram(sample, bins=edges) # compute marginal distribution of sample m_sample = counts/counts.sum() return (-1)*np.nansum(m_sample*np.ma.log2(m_sample)) def mutual_information_grid(X,Y,Z): \u0026quot;\u0026quot;\u0026quot; Compute the mutual information of a dependent variable over a grid defined by two indepedent variables. Args: X,Y: float, NDarray coordinates over which dependent variable is distributed Z: float, array dependent variable Returns: estimates: dict keys: I: float mutual information I(Z; (X,Y)) W: float, NDarray matrix of weighted conditional entropies marginal: float marginal entropy conditional: float conditional entropy \u0026quot;\u0026quot;\u0026quot; x_edges = np.histogram_bin_edges(X, bins='fd') [xc, _] = np.histogram(X, bins=x_edges) xc = xc/xc.sum() nx = x_edges.shape[0]-1 y_edges = np.histogram_bin_edges(Y, bins='fd') [yc, _] = np.histogram(Y, bins=y_edges) yc = yc/yc.sum() ny = y_edges.shape[0]-1 # matrix of conditional entropies for each bin H = np.zeros((nx, ny)) # compute pairwise marginal probability of X/Y bins mxy = xc[:,None]*yc[None,:] for x_bin in range(nx): for y_bin in range(ny): x_idx = np.where((X\u0026gt;=x_edges[x_bin]) \u0026amp; (X\u0026lt;x_edges[x_bin+1]))[0] y_idx = np.where((Y\u0026gt;=y_edges[y_bin]) \u0026amp; (Y\u0026lt;y_edges[y_bin+1]))[0] bin_samples = Z[x_idx,:][:,y_idx] bin_samples = np.reshape(bin_samples, np.product(bin_samples.shape)) H[x_bin, y_bin] = entropy(bin_samples) W = H*mxy conditional = np.nansum(W) marginal = entropy(Z) I = marginal - conditional estimates = {'mi': I, 'weighted-conditional': W, 'marginal': marginal, 'conditional': conditional} return estimates  ","date":1633731857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633731857,"objectID":"5cefd43dacf0726c1040765d418f5e19","permalink":"/post/information/","publishdate":"2021-10-08T15:24:17-07:00","relpermalink":"/post/information/","section":"post","summary":"I\u0026rsquo;m interested in looking at some spatial mappings between pairs of cortical regions, and believe that these mappings are mediated, to some degree, by the temporal coupling between cortical areas. I don\u0026rsquo;t necessarily know the functional form of these mappings, but neurobiologically predict that these mappings are not random and have some inherent structure.","tags":["probability","entropy","mutual information"],"title":"Entropy and Mutual Information","type":"post"},{"authors":[],"categories":[],"content":"For one reason or another, I\u0026rsquo;ll often find myself running into the following issue:\nremote: error: GH001: Large files detected. You may want to try Git Large File Storage — https://git-lfs.github.com. remote: error: Trace: b5116d865251981c96d4b32cdf7ef464 remote: error: See http://git.io/iEPt8g for more information. remote: error: File fixtures/11_user_answer.json is 131.37 MB; this exceeds GitHub’s file size limit of 100.00 MB  ","date":1632291857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632291857,"objectID":"c743ea9d7bb18e6207f8f9892c78a3a4","permalink":"/drafts/large-files-fit/","publishdate":"2021-09-21T23:24:17-07:00","relpermalink":"/drafts/large-files-fit/","section":"drafts","summary":"For one reason or another, I\u0026rsquo;ll often find myself running into the following issue:\nremote: error: GH001: Large files detected. You may want to try Git Large File Storage — https://git-lfs.github.com. remote: error: Trace: b5116d865251981c96d4b32cdf7ef464 remote: error: See http://git.","tags":[],"title":"Removing large files in Git","type":"drafts"},{"authors":[],"categories":[],"content":"Some updates on my literary endeavors:\n I\u0026rsquo;ve put Gravity\u0026rsquo;s Rainbow aside. I truly appreciate that this is the style of writing that some people enjoy, but by God, it\u0026rsquo;s so convoluted. Back to the shelf. I finished When Breath Becomes Air by Paul Kalanithi over the course of two nights, and found myself crying myself to sleep at the ending. I can\u0026rsquo;t remember the last time I was so emotionally caught up in a book. In brief, a brilliant Stanford neurosurgeon and polymath (Paul himself) is diagnosed with stage 4 terminal lung cancer in his early 30s, before even really getting a chance to explore his own medical and scientific potential to change the world. This book is Paul\u0026rsquo;s homage to medicine and his (brief) philosophical exploration of purpose, life, and death, and how those three intertwined in his own life, prior to his passing.  I\u0026rsquo;ve started reading:\n The Memory Police by Yoko Ogawa  I\u0026rsquo;m only two chapters in but I\u0026rsquo;m hooked. It actually reminds me quite viscerally of Saramago\u0026rsquo;s Blindness    Still reading:\n The Island of the Day Before by Umberto Eco  Favorite recent new words and quotes:\n matutinal \u0026ndash; of or occuring in the morning marcescent \u0026ndash; withering but staying attached to the stem \u0026ldquo;A little learning is a dangerous thing, drink deep or taste not the Pierian Spring\u0026rdquo; \u0026ndash; Alexander Pope, as quoted in When Breath Becomes Air. The Pierian Springs of Macedonia were springs sacred to the Greek muses, and the metaphorical source of knowledge in art and science. It resonated with me because it aptly describes my learning style. When something new catches my eye, I dig and explore deeply. I\u0026rsquo;m not satisfied with superficial understanding \u0026ndash; I need to know the mechanisms, the relationships, the relevance.  ","date":1616394257,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616394257,"objectID":"5ffea9eb2ae624547c47943d3f65b689","permalink":"/personal/literature-2/","publishdate":"2021-03-21T23:24:17-07:00","relpermalink":"/personal/literature-2/","section":"personal","summary":"Some updates on my literary endeavors:\n I\u0026rsquo;ve put Gravity\u0026rsquo;s Rainbow aside. I truly appreciate that this is the style of writing that some people enjoy, but by God, it\u0026rsquo;s so convoluted. Back to the shelf.","tags":["literature","fiction","non-fiction"],"title":"Literature #2","type":"personal"},{"authors":[],"categories":[],"content":"One week after checking Hidden Lake Lookout off of my list, I skied the biggest line of my life. Ross and I had only half-heartedly talked about skiing this peak, expecting the dangerous avalanche conditions that had for so long plagued the Cascades in 2021 to restrict any reasonable access to this line, but by some luck, conditions cleared up for the perfect outing.\nMount Shuksan is a glaciated massif (a bunch of peaks close together covered in large glaciers) in the North Cascades just slightly northwest of Koma Kulshan (Mount Baker). Of these two northern peaks, Shuksan is the less-summited and less-skied of the pair, with most outdoor enthusiasts being drawn to the prospect of summitting an actual volcano. However, Shuksan is a beast of a mountain \u0026ndash; the terrain is formidable and heavily crevassed, even in deep winter.\nOn Shuksan\u0026rsquo;s northwest face is a line deemed one of North America\u0026rsquo;s 50 Classic Ski Descents (a list made recently-famous by professional skier Cody Townsend) called the Northwest Couloir. And boy is this a doozy. After ascending nearly five thousand vertical feet in just 4 miles across the White Salmon glacier, the line immediately drops from 8200 feet to 3700 feet in 2 miles \u0026ndash; an ungodly steep line. The initial descent is angled at roughly 50 degrees. However, this is followed by a short by hyper-exposed traverse over an 80-foot cliff section. In good conditions, this traverse will be covered in fresh, shaded snow, and confident skiers can make quick work of it \u0026ndash; however, in our case, the traverse was bullet-proof ice. We scooted along, ice axes in hand, praying to the Norse god Ullr (a.k.a. mechanical engineers at Volkl and Blizzard) to grant us safe and short passage. The term \u0026ldquo;survival skiing\u0026rdquo; would be apt, to say the least. The remainder of the couloir consists of steep 50-55 degree slopes, with overhead danger from the above cliffs, and also from the sheer amount of steep-angled snow in the actual couloir. But if you keep your wits about you, make precise jump turns, and try your best not to fall, this skiing could be some of the best in your life.\n  Mount Shuksan in the early morning light, from the Mt. Baker resort parking lot. We got a good view of our line, just to the right of the large black rock face.     Ross\u0026rsquo; photo of me as we ski out of the Mt. Baker resort towards our objective in the distance.     Ross, just after ascending the White Salmon glacier. Fortunately, we beat the sun on the way up!     The views of Koma Kulshan were absolutely gorgeous.     Ross entering the first part of the couloir, about to reach the cliff traverse. I didn\u0026rsquo;t take any pictures after this until the bottom (for obvious reasons \u0026ndash; I needed my hands completely dedicated to my ski poles).     We made it to the bottom! With beautiful views of the Shuksan Arm to boot.     It was pretty hard not to smile after making it down as smoothly and safely as we did. Pictures can\u0026rsquo;t put into perspective how steep this was\u0026hellip;     Our line. What. A. Day.   ","date":1616221457,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616221457,"objectID":"3021a07351007956d75e66b97b9fd075","permalink":"/personal/nw-couloir/","publishdate":"2021-03-19T23:24:17-07:00","relpermalink":"/personal/nw-couloir/","section":"personal","summary":"One week after checking Hidden Lake Lookout off of my list, I skied the biggest line of my life. Ross and I had only half-heartedly talked about skiing this peak, expecting the dangerous avalanche conditions that had for so long plagued the Cascades in 2021 to restrict any reasonable access to this line, but by some luck, conditions cleared up for the perfect outing.","tags":[],"title":"Northwest Couloir, Mt. Shuksan","type":"personal"},{"authors":[],"categories":[],"content":"I\u0026rsquo;d been vying to ski the Hidden Lake Lookout in the North Cascades nearly the entire time I\u0026rsquo;ve lived in the PNW. Sneakily tucked away off of Cascade River Road near Marblemount, this peak offers some of the best views the Cascades have to offer. On a clear day, you can see Koma Kulshan, Shuksan, the Canadian Cascades, and the entire Boston Basin with peaks like Forbidden, Boston, Sahale, Eldorado, and Johannesberg all seemingly within grasp. The weather for this weekend was shaping up to be beautiful, and avalanche danger was rated as Moderate by NWAC, so I convinced my friends Ross, Mike, and Sam to join.\nThe approach in the Winter follows a gravel Summer road, so we ended up skinning on flat and mild terrain for roughly 3 miles before we even started the actual uphill off-trail section, but the grade quickly ramps up after leaving the road. The first section consisted mostly of skinning over frozen chucks of snow dropped from the tree tops, which made the intial ascent slow, but we quickly ascended nearly four thousand feet, and popped out of the trees near six thousand feet.\nNo, this was not a ski tour for my birthday\u0026hellip; but it could have been! The Birthday Tour is another classic PNW route on the east side of the North Cascades off of Highway 20 near Washington Pass. This is a relatively quick tour, with a mild amount of elevation gain, but some pretty enjoyable downhill bits.\n  Mike, Sam, and Ross ascending Hidden Lake Peak from the North.     Sam staring back up at the line he just skied! Shortly after this, we saw a bald eagle circling the peak \u0026ndash; we took this to be a good omen for fun to be had!     Sam and Ross giving each other the classic skiers\u0026rsquo; pole bump a.k.a. high-five.     Sam\u0026rsquo;s behind the lens this time. Ross, Mike, and myself (sitting down) stopping for lunch on Hidden Lake.     View from the West near the lookout of Hidden Lake Peak in the foreground, and Eldorado in the background. What a day!   ","date":1615703057,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615703057,"objectID":"9400b035c960e1b1778021dd3ac9a106","permalink":"/personal/hidden-lake-2021/","publishdate":"2021-03-13T23:24:17-07:00","relpermalink":"/personal/hidden-lake-2021/","section":"personal","summary":"I\u0026rsquo;d been vying to ski the Hidden Lake Lookout in the North Cascades nearly the entire time I\u0026rsquo;ve lived in the PNW. Sneakily tucked away off of Cascade River Road near Marblemount, this peak offers some of the best views the Cascades have to offer.","tags":[],"title":"Hidden Lake Lookout, Winter 2021","type":"personal"},{"authors":[],"categories":[],"content":"Background As I mentioned in my previous post on constrained graph attention networks, graph neural networks suffer from overfitting and oversmoothing as network depth increases. These issues can ultimately be linked to the local topologies of the graph.\nIf we consider a 2d image as a graph (i.e. pixels become nodes), we see that images are highly regular \u0026ndash; that is, each node has the same number of neighbors, except for those at the image periphery. When we apply convolution kernels over node signals, filters at any given layer are aggregating information from the same sized neighborhods irrespective of their location.\nHowever, if we consider a graph, there is no guarantee that the graph will be regular. In fact, in many situations, graphs are highly irregular, and are characterized by unique topological neighborhood properties such as tree-like structures or expander graphs, that are sparse yet highly connected. If we compare an expander node to a node whose local topology is more regular, we would find that the number of signals that each node implicitly convolves at each network layer would vary considerably. These topological discrepancies have important implications when we consider problems like node and graph classification, as well as edge prediction. The problem ultimately boils down to one of flexibility: can we account for unique local topologies of a graph in order to dynamically aggregate local information on a node-by-node basis?\n  Node signal aggregation as a function of network depth. At each layer, the neural network implicitly aggregates signals over an increasingly-larger neighborhood. In this example, the network is highly regular \u0026ndash; however, not all graphs are.   In a recent paper, the authors propose one approach to address this question, which they call \u0026ldquo;jumping knowledge representation learning\u0026rdquo;1. Instead of utilizing the output of the last convolution layer to inform the prediction, jumping-knowledge networks aggregate the embeddings from all hidden layers to inform the final prediction. The authors develop an approach to study the \u0026ldquo;influence distribution\u0026rdquo; of nodes: for a given node $x$, the influence distribution $I_{x}$ characterizes how much the final embedding of node $x$ is influenced by the input features of every other node:\n$$ \\begin{align} I(x,y) \u0026amp;= \\sum_{i=1}^{m} |\\frac{\\delta h_{x}^{k}}{\\delta h_{y}^{0}}|_{i} \\\\\nI_{x}(y) \u0026amp;= I(x,y) \\Big/\\sum_{z} I(x,z) \\end{align} $$\nThey show that influence distribution $I_{x}$ for a $k$-layer graph convolution network is equal, in expectation, to the $k$-step random walk distribution. They point out that the random walk distribution of expander-like nodes converge quickly \u0026ndash; the final embeddings of these nodes are represenative of the whole graph and carry global information \u0026ndash; while the random-walk distribution of nodes with tree-like topology converge slowly \u0026ndash; these nodes carry more-local information2. These two conclusions are related to the spectral gap of the graph \u0026ndash; the smallest non-zero eigenvalue of the graph Laplacian. A large spectral gap indicates high-connectivity, while a low spectral gap indicates low connectivity. From a graph theory perspective, this local connectivity is related to the idea of centrality. Nodes with high centrality will easily saturate their random walk distribution, but will also aggregate information from large neighborhoods quickly. For graph neural networks with fixed aggregation kernels, this has important implications for representation learning, because the feature distributions of nodes with different topologies will not correspond to the same degree of locality, which may not lead to the best learned representations for all nodes. A radius that is too large may result in over-smoothing of node features, while a radius that is too small may not be robust enough to learn optimal node embeddings.\nThe jumping knowledge network architecture is conceptually similar to other graph neural networks, and we can, in fact, simply incorporate the jumping knowledge mechanism as an additional layer. The goal is to adaptively learn the effective neighborhood size on a node-by-node basis, rather than enforcing the same aggregation radius for every node (remember, we want to account for local topological and feature variations). The authors suggest three possible aggregation functions: concatentation, max-pooling, and an LSTM-attention mechanism 1 3. Each aggregator learns an optimal combination of the hidden embeddings, which is then pushed through a linear layer to generate the final network output. Concatenation determines the optimal linear combination of hidden embeddings for the entire dataset simultaneously, so it is not a node-specific aggregator. Max-pooling selects the most important hidden layer for each feature element on a node-by-node basis \u0026ndash; however, empirically, I found that max-pooling was highly unstable in terms of model learning. The LSTM-attention aggregator treats the hidden embeddings as a sequence of elements, and assigns a unique attention score to each hidden embedding 4.\n  Schematic of a jumping-knowledge network. The neural network generates an embedding for each hidden layer. The aggregator function then optimally combines these hidden embeddinggs to learn the optimal abstraction of input information. Some alternative aggregation functions include max-pooling, concatenation, and an LSTM layer. In the case of an LSTM layer coupled with an attention mechanism, the aggregator computes a convex combination of hidden embeddings.   Long-Short Term Memory Briefly, given a sequence of samples $X_{1}, X_{2}, \\dots X_{t}$, the LSTM cell learns temporal dependencies between elements of a sequence by maintaining a memory of previously observed elements \u0026ndash; in our case, the sequence elements are the embeddings learned by each consecutive hidden layer. An LSTM cell is characterized by three gates controlling information flow between elements in the sequence: input, forget, and output, as well as a cell state vector, which captures the memory and temporal dependencies between sequence elements5:\n  Schematic of an LSTM cell. The cell controls what information is remembered from previous elements in a sequence, and what information is incorporated into memory given a new element in the sequence.   $$ \\begin{align} f_{t} \u0026amp;= \\sigma(W_{f}X_{t} + U_{f}h_{t-1} + b_{f}) \\\\\ni_{t} \u0026amp;= \\sigma(W_{i}X_{t} + U_{i}h_{t-1} + b_{i}) \\\\\no_{t} \u0026amp;= \\sigma(W_{o}X_{t} + U_{o}h_{t-1} + b_{o}) \\\\\n\\end{align} $$\nwhere $W$, $U$, and $b$ are learnable parameters of the gates. Here, $X_{t}$ is the $t$-th sequence element, $h_{t-1}$ represents the learned LSTM cell embedding for element $t-1$, and $C_{t-1}$ represents the current memory state, given the previous $1, 2 \\dots t-1$ elements. The input and forget gates determine which aspects of a sequence element are informative / uninformative, and decide what information to keep / forget, while the output gate combines the previous memory state with our new knowledge. We can roughly think of this process as updating our prior beliefs, in the Bayesian sense, with new incoming data.\n$$ \\begin{align} \\tilde{c_{t}} \u0026amp;= \\sigma(W_{c}X_{t} + U_{c}h_{t-1} + b_{c}) \\\\\nc_{t} \u0026amp;= f_{t}\\circ c_{t-1} + i_{t} \\tilde{c}_{t} \\\\\nh_{t} \u0026amp;= o_{t} \\circ tanh(c_{t}) \\end{align} $$\nThe embeddings for each element learned by the LSTM cell are represented by $h_{t}$. In the original paper1, the authors propose to apply a bi-directional LSTM to simultaneously learn forwards and backwards embeddings, which are concatenated and pushed through a single-layer perceptron to compute layer-specific attention weights for each node:\n$$ \\begin{align} \\alpha_{i}^{t} \u0026amp;= \\sigma(\\vec{w}_{t}^{T}(h^{F}_{i, t} || h^{B}_{i, t})) \\\\\n\\alpha_{i}^{t} \u0026amp;= \\frac{\\exp{(\\alpha_{i}^{t})}}{\\sum_{t=1}^{L} \\exp{(\\alpha_{i}^{t})}} \\end{align} $$\nThe softmax-normalized attention weights represent a probability distribution over attention weights\n$$\\begin{align} \\sum_{t=1}^{L} \\alpha_{i}^{t} \u0026amp;= 1 \\\\\n\\\\\n\\alpha_{i}^{t} \u0026amp;\u0026gt;= 0 \\end{align} $$\nwhere $\\alpha_{i}^{t}$ represents how much node $i$ attends to the embedding of hidden layer $t$. The optimal embedding is then computed as the attention-weighted convex combination of hidden embeddings:\n$$ \\begin{align} X_{i, \\mu} = \\sum_{t=1}^{L} \\alpha_{i}^{t}X_{i, t} \\end{align} $$\nAn Application of Jumping Knowledge Networks to Cortical Segmentation I\u0026rsquo;ve implemented the jumping knowledge network using DGL here. Below, I\u0026rsquo;ll demonstrate the application of jumping knowledge representation learning to a cortical segmentation task. Neuroscientifically, we have reason to believe that the influence radius will vary along the cortical manifold, even if the mesh structure is highly regular. As such, I am specifically interested in examining the importance that each node assigns to the embeddings of each hidden layer. To that end, I utilize the LSTM-attention aggregator. Similarly, as the jumping-knowledge mechanism can be incorporated as an additional layer to any general graph neural network, I will use graph attention networks (GAT) as the base network architecture, and compare vanilla GAT performance to GATs with a jumping knowledge mechanism (JKGAT).\nBelow, I show the prediction generated by a 9-layer JKGAT model, with 4 attention heads and 32 hidden channels per layer, with respect to the \u0026ldquo;known\u0026rdquo; or \u0026ldquo;true\u0026rdquo; cortical map. We find slight differences in the performance of our JKGAT model with respect to the ground truth map, notably in the lateral occipital cortex and the medial prefrontal cortex.\n  Comparison of the group-average predicted cortical segmentation produced by the JKGAT model, to the ground truth cortical segmentation. The ground truth was previously generated here. The consensus cortical map corresponds very well to the true map.   When we consider the accuracies for various parameterizations of our models, we see that the JKGAT performs quite well. Notably, it performs better than the GAT model in most cases. Likewise, as hypothesized, the JKGAT performs better than the GAT model as network depth increases, specifically because we are able to dynamically learn the optimal influence radii for each node, rather than constraining the same radius size for the entire graph. This allows us to learn more abstract representations of the input features by mitigating oversmoothing and by accounting for node topological variability, which is important for additional use-cases like graph classification.\n  Model accuracy comparison between GAT and JKGAT models on a node classification problem for cortical segmentation. Accuracy is represented as the fraction of correctly-labeled nodes in a graph, across 150 validation subjects. Each node in the graph has 80 features, and each graph has 30K nodes.   Similarly, we find that JKGAT networks generate segmentation predictions that are more reproducible and consistent across resampled datasets. This is important, especially in the case where we might acquire data on an individual multiple times, and want to generate a cortical map for each acquisition instance. Unless an individual suffers from an accelerating neurological disorder, experiences a traumatic neurological injury, or the time between consecutive scans is very long (on the order of years), we expect the cortical map of any given individual to remain quite static (though examining how the \u0026ldquo;map\u0026rdquo; of an individual changes over time is still an open-ended topic).\n  Model reproducibility comparison between GAT and JKGAT models on a node classification problem for cortical segmentation, using 150 validation subjects. Each subject has four repeated datasets. Within a given subject, the topology of each graph is the same, but the node features are re-sampled for each graph. Reproducibility is computed using the F1-score between all pairs of predicted node classifications, such that we compute 6 F1 scores for each subject.   Finally, when we consider the importance that each cortical node assigns to the unique embedding at the $k$-th layer via the LSTM-attention aggregation function, we see very interesting results. Notably, we see high spatial auto-correlation of the attention weights. Even more striking, is that this spatial correlation seems to correspond to well-studied patterns of resting-state networks identified using functional MRI. Apart from the adjacency structure of our graphs, we do not encode any a priori information of brain connectivity. That the LSTM-attention aggregator of the jumping-knowledge layer idenfities maps corresponding reasonably well to known functional networks of the human brain is indicative, to some extent, of how the model is learning, and more importantly, of which features are useful in distinguishing cortical areas from one another.\nLet us consider the attention map for layer 4. We can interpret the maps as follows: for a given network architecture (in this case, a network with 9 layers), we find that areas in the primary motor (i.e. Brodmann areas 3a and banks of area 4) and primary auditory cortex (Broddmann areas A1 and R1) preferentially attend to the embedding of hidden layer 4, relative to the rest of the cortex \u0026ndash; this indicates that the implicit aggregation over an influence radius of 4 layers is deemed more informative for the classification of nodes in the primary motor and auditory regions than for orther cortical areas. However, whether this says anything about the implicit complexitiy of the cortical signals of these areas remains to be studied.\n  Maps of learned LSTM-attention aggregator weights. Each inset corresponds to the weights learned by every cortical node for the $k$-th layer hidden embedding (black: low, red: high). We see that most of the attention mass is distributed over layers 4-7, indicating that most nodes assign maximal importance to intermediate levels of abstraction. However, we do see spatially varying attention. Notably, within a given attention map, we find that nodes of the lateral Default Mode Network preferentially attend to the embeddings of layers 1-3, while layer 4 is preferentially attended to by the primary motor and auditory areas.     Xu et al. Representation Learning on Graphs with Jumping Knowledge Networks. 2018. \u0026#x21a9;\u0026#xfe0e;\n Dinitz et a. Large Low-Diameter Graphs are Good Expanders. 2017. \u0026#x21a9;\u0026#xfe0e;\n Lutzeyer et al. Comparing Graph Spectra of Adjacency and Laplacian Matrices. 2017. \u0026#x21a9;\u0026#xfe0e;\n Gers, Felix. Long Short-Term Memory in Recurrent Neural Networks. 2001. \u0026#x21a9;\u0026#xfe0e;\n Fan et al. Comparison of Long Short Term Memory Networks and the Hydrological Model in Runoff Simulation. 2020. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1613370257,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613370257,"objectID":"231129d2547a64b184b32ff951eb4995","permalink":"/post/jumping-knowledge/","publishdate":"2021-02-14T23:24:17-07:00","relpermalink":"/post/jumping-knowledge/","section":"post","summary":"Background As I mentioned in my previous post on constrained graph attention networks, graph neural networks suffer from overfitting and oversmoothing as network depth increases. These issues can ultimately be linked to the local topologies of the graph.","tags":["pytorch","AI","graph neural networks","representation learning","lstm"],"title":"Jumping-Knowledge Representation Learning With LSTMs","type":"post"},{"authors":[],"categories":[],"content":"In their recent paper, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click here). Briefly, GATs are a recently-developed neural network architecture applied to data distributed over a graph domain. We can think of graph convolutional networks as progressively transforming and aggregating signals from within a local neighborhood of a node. At each iteration of this process, we implicitly merge signals from larger and larger neighborhoods of the node of interest, and thereby learn unique representations of nodes that are dependent on their surroundings.\nGATs incorporate the seminal idea of \u0026ldquo;attention\u0026rdquo; into this learning process. In each message-passing step, rather than updating the features of a source-node via equally-weighted contributions of neighborhood nodes, GAT models learn an attention function \u0026ndash; i.e. they learn how to differentially pay attention to various signals in the neighborhood. In this way, the algorithm can learn to focus on imporant signals and disregard superfluous signals. If we consider neural networks as universal funtion approximators, the attention mechanism improves the approximating ability by incorporating multiplicative weight factors into the learning.\n  Figure from Velickovic et al. For a source node $i$ and destination node $j$, vectors $\\vec{h_{i}}$ and $\\vec{h_{j}}$ are the input feature vectors of nodes $i$ and $j$ in layer $l$. $\\mathbf{W}$ is a learned affine projection matrix. $\\mathbf{\\vec{a}}$ is the learned attention function. The source and destination node input features are pushed through the attention layer as $\\alpha_{i,j} = \\sigma\\Big(\\vec{a}^{T}\\mathbf{W}\\Big(\\vec{h_{i}} || \\vec{h_{j}}\\Big)\\Big)$ where $\\sigma$ is an activation function, and $\\alpha_{i,j}$ the unnormalized attention that node $i$ pays to node $j$ in layer $l$. Attention weights are then passed through a softmax layer, mapping the attentions between [0,1].   However, GATs are not without their pitfals, as noted by Wang et al. Notably, the authors point to two important issues that GATs suffer from: overfitting of attention values and oversmoothing of signals across class boundaries. The authors propose that GATs overfit the attention function because the learning process is driven only by classification error, with complexity $O(|V|)$ i.e. the number of nodes in the graph. With regards to oversmoothing, the authors note that a single attention layer can be viewed as a form of Laplacian smoothing:\n$$\\begin{align} Y = AX^{l} \\end{align}$$\nwhere $A_{n \\times n}$ is the attention weight matrix with $A_{i,j} = \\alpha_{i,j}$ if $j \\in \\mathcal{N_{i}}$ and $0$ otherwise. Because $\\sum_{j\\in \\mathcal{N_{i}}} \\alpha_{i,j} = 1$, we can view $A$ as a random walk transition probability matrix. If we assume that graph $G=(V,E)$ has $K$ connected components, repeated application of $A$ to $X$ distributed over $G$ will result in a stationary distribution of node features within each connected component \u0026ndash; that is, the features vectors of the nodes within each connected component will converge on the component mean. However, as the authors point out, we typically have multiple layers $l_{1}\\dots l_{j}$, each with their own attention matrix $A_{1} \\dots A_{j}$, each representing a unique transition probability matrix. Because we generally do not have disconnected components, nodes from different classes will be connected \u0026ndash; consequentially, deep GAT networks will mix and smooth signals from different adjacent components, resulting in classification performance that worsens with network depth. Importantly, multi-head attention networks do not alleviate this convergence issue \u0026ndash; each head can be viewed as a unique probability transition matrix, which all suffer from the same oversmoothing issue as $l \\rightarrow \\infty$.\nWang et al. propose to incorporate two margin-based constraints into the learning process. The first constraint, $\\mathcal{L_{g}}$, addresses the overfitting issue, by enforcing that learned attentions between adjacent pairs of nodes be higher than attentions between distant pairs of nodes.\n$$\\begin{align} \\mathcal{L_{g}} \u0026amp;= \\sum_{i \\in V} \\sum_{j \\in \\mathcal{N_{i}} \\setminus \\mathcal{N_{i}^{-}}} \\sum_{k \\in V\\setminus \\mathcal{N_{i}}} max(0, \\phi(v_{i},v_{k}) + \\zeta_{g} - \\phi(v_{i},v_{j})) \\end{align}$$\nThe second constraint, $\\mathcal{L_{b}}$, address the oversmoothing issue, by enforcing that learned attentions between pairs of adjacent nodes with the same label be higher than attention between pairs of adjacent nodes with different labels:\n$$\\begin{align} \\mathcal{L_{b}} \u0026amp;= \\sum_{i \\in V}\\sum_{j \\in \\mathcal{N_{i}^{+}}} \\sum_{k \\in \\mathcal{N_{i}^{-}}} max(0, \\phi(v_{i},v_{k}) + \\zeta_{b} - \\phi(v_{i},v_{j})) \\end{align}$$\nIn both cases, $\\phi(,)$ is the attention function between a pair of nodes, $\\mathcal{N_{i}^{+}}$ and $\\mathcal{N_{i}^{-}}$ are the nodes adjacent to node $i$ with the same (+) and different (-) labels as $i$, and $\\zeta_{g}$ and $\\zeta_{b}$ are slack variables controlling the margin between attention values. The first loss function, $\\mathcal{L_{g}}$, can be implemented via negative sampling of nodes (the authors actually perform importance-based negative sampling based on attention-weighted node degrees, but showed that this only marginally improved classification accuracy in benchmark datasets).\nThe authors propose one final addition to alleviate the oversmoothing issue posed by vanilla GATs. Rather than aggregating over all adjacent nodes in a neighborhood, the authors propose to aggregate over the nodes with the $K$ greatest attention values. Because the class boundary loss $\\mathcal{L_{b}}$ enforces large attentions on nodes of the same label and small attention on nodes of different labels, aggregating over the top $K$ nodes will tend to exclude nodes of different labels than the source node in the message passing step, thereby preventing oversmoothing. The authors show that this constrained aggregation approach is preferable to attention dropout proposed in the original GAT paper. Taken together, the authors deem these margin-based loss and constrained aggregation \u0026ldquo;Constrained Graph Attention Network\u0026rdquo; (C-GAT).\n Implementation I wasn\u0026rsquo;t able to find an implementation of the Constrained Graph Attention Network for my own purposes, so I\u0026rsquo;ve implemented the algorithm myself in Deep Graph Library (DGL) \u0026ndash; the source code for this convolutional layer can be found here. This implementation makes use of the original DGL GATConv layer structure, with modifications made for the constraints and aggregations. Specifically, the API for CGATConv has the following modifications:\nCGATCONV(in_feats, out_feats, num_heads, feat_drop=0., graph_margin=0.1, # graph structure loss slack variable class_margin=0.1, # class boundary loss slack variable top_k=3, # number of messages to aggregate over negative_slope=0.2, residual=False, activation=None, allow_zero_in_degree=False)  Of note is the fact that the attn_drop parameter has been substituted by the top_k parameter in order to mitigate oversmoothing, and the two slack variables $\\zeta_{g}$ and $\\zeta_{b}$ are provided as graph_margin and class_margin.\nWith regards to the loss functions, the authors compute all-pairs differences between all edges incident on a source node, instead of summing over the positive / negative sample attentions ($\\mathcal{L_{g}}$) and same / different label attentions ($\\mathcal{L_{b}}$) and then differencing these summations. In this way, the C-GAT model anchors the loss values to specific nodes, promoting learning of more generalizable attention weights. The graph structure loss function $\\mathcal{L_{g}}$ is implemented with the graph_loss reduction function below:\ndef graph_loss(nodes): \u0026quot;\u0026quot;\u0026quot; Loss function on graph structure. Enforces high attention to adjacent nodes and lower attention to distant nodes via negative sampling. \u0026quot;\u0026quot;\u0026quot; msg = nodes.mailbox['m'] pw = msg[:, :, :, 0, :].unsqueeze(1) nw = msg[:, :, :, 1, :].unsqueeze(2) loss = (nw + self._graph_margin - pw).clamp(0) loss = loss.sum(1).sum(1).squeeze() return {'graph_loss': loss} . . . graph.srcdata.update({'ft': feat_src, 'el': el}) graph.dstdata.update({'er': er}) graph.apply_edges(fn.u_add_v('el', 'er', 'e')) e = self.leaky_relu(graph.edata.pop('e')) # construct the negative graph by shuffling edges # does not assume a single graph or blocked graphs # see cgatconv.py for ```construct_negative_graph``` function neg_graph = [construct_negative_graph(i, k=1) for i in dgl.unbatch(graph)] neg_graph = dgl.batch(neg_graph) neg_graph.srcdata.update({'ft': feat_src, 'el': el}) neg_graph.dstdata.update({'er': er}) neg_graph.apply_edges(fn.u_add_v('el', 'er', 'e')) ne = self.leaky_relu(neg_graph.edata.pop('e')) combined = th.stack([e, ne]).transpose(0, 1).transpose(1, 2) graph.edata['combined'] = combined graph.update_all(fn.copy_e('combined', 'm'), graph_loss) # compute graph structured loss Lg = graph.ndata['graph_loss'].sum() / (graph.num_nodes() * self._num_heads)  Similarly, the class boundary loss function $\\mathcal{L_{b}}$ is implemented with the following message and reduce functions:\ndef adjacency_message(edges): \u0026quot;\u0026quot;\u0026quot; Compute binary message on edges. Compares whether source and destination nodes have the same or different labels. \u0026quot;\u0026quot;\u0026quot; l_src = edges.src['l'] l_dst = edges.dst['l'] if l_src.ndim \u0026gt; 1: adj = th.all(l_src == l_dst, dim=1) else: adj = (l_src == l_dst) return {'adj': adj.detach()} def class_loss(nodes): \u0026quot;\u0026quot;\u0026quot; Loss function on class boundaries. Enforces high attention to adjacent nodes with the same label and lower attention to adjacent nodes with different labels. \u0026quot;\u0026quot;\u0026quot; m = nodes.mailbox['m'] w = m[:, :, :-1] adj = m[:, :, -1].unsqueeze(-1).bool() same_class = w.masked_fill(adj == 0, np.nan).unsqueeze(2) diff_class = w.masked_fill(adj == 1, np.nan).unsqueeze(1) difference = (diff_class + self._class_margin - same_class).clamp(0) loss = th.nansum(th.nansum(difference, 1), 1) return {'boundary_loss': loss} . . . graph.ndata['l'] = label graph.apply_edges(adjacency_message) adj = graph.edata.pop('adj').float() combined = th.cat([e.squeeze(), adj.unsqueeze(-1)], dim=1) graph.edata['combined'] = combined graph.update_all(fn.copy_e('combined', 'm'), class_loss) Lb = graph.ndata['boundary_loss'].sum() / (graph.num_nodes() * self._num_heads)  And finally, the constrained message aggregation is implemented using the following reduction function:\ndef topk_reduce_func(nodes): `\u0026quot;\u0026quot;\u0026quot; Aggregate attention-weighted messages over the top-K attention-valued destination nodes \u0026quot;\u0026quot;\u0026quot; K = self._top_k m = nodes.mailbox['m'] [m,_] = th.sort(m, dim=1, descending=True) m = m[:,:K,:,:].sum(1) return {'ft': m} . . . # message passing if self._top_k is not None: graph.update_all(fn.u_mul_e('ft', 'a', 'm'), topk_reduce_func) else: graph.update_all(fn.u_mul_e('ft', 'a', 'm'), fn.sum('m', 'ft'))  ","date":1608963857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608963857,"objectID":"63952edab471e0e08d691d342488a66c","permalink":"/post/constrained-gat/","publishdate":"2020-12-25T23:24:17-07:00","relpermalink":"/post/constrained-gat/","section":"post","summary":"In their recent paper, Wang et al. propose a few updates to the Graph Attention Network (GAT) neural network algorithm (if you want to skip the technical bit and get to the code, click here). Briefly, GATs are a recently-developed neural network architecture applied to data distributed over a graph domain.","tags":["pytorch","AI","graph convolution networks"],"title":"Constrained Graph Attention Networks","type":"post"},{"authors":null,"categories":null,"content":"This is a python package to apply various graph neural network models to brain connectivity data to learn cortical brain maps. These models address the issue of node classification, as commonly seen in the graph representation learning community. These models are based on the Deep Graph Library.\nCurrent models include the following:\n Graph Convolution Network ( paper) Graph Attention Network ( paper) Gaussian Kernel Graph Convolution Network ( paper) Constrained Graph Attention Network ( paper) Jumping Knowledge Representation Learning ( paper)  ","date":1608854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608854400,"objectID":"2d347eccce09def87a8898655a405fde","permalink":"/project/parcellearning/","publishdate":"2020-12-25T00:00:00Z","relpermalink":"/project/parcellearning/","section":"project","summary":"package of neural network modules for learning cortical architectures from brain connectivity data","tags":["image processing","artificial intelligence","neural networks","segmentation"],"title":"parcellearning","type":"project"},{"authors":[],"categories":[],"content":"As I mentioned in my previous post, I work with cortical surface segmentation data. Due to the biology of the human brain, there is considerable reproducible structure and function across individuals (thankfully!). One manifestion of this reproducibility is exemplified by the neocortex a.k.a. the thin (~2.5mm) gray matter layer of cell-bodies at the periphery of the brain. The neocortex is well known to have local homogeneity in terms of types of neuronal cells, protein and gene expression, and large-scale function, for example. Naturally, researchers have been trying to identify discrete delineations of the cortex for nearly 100 years, by looking for regions of local homogeneity of various features along the cortical manifold.\nAs in my previous post, I\u0026rsquo;m working on this problem using graph convolution networks (GCN). Given the logits output by a forward pass of a GCN, I want to classify a cortical node as belonging to some previously identified cortical area. Using categorical cross-entropy, we can calculate the loss of a given foward pass of the model $h(X; \\Theta)$:\n$$ \\begin{align} L = -\\sum_{k=1}^{K} \\sum_{l \\in \\mathcal{L}} x_{l}^{k} \\cdot log(\\sigma(x^{k})_{l}) \\end{align} $$\nwhere $x^{k}$ is the output of the model for a single node, $x_{l}^{k}$ is the one-hot-encoding value of the true labels, and $\\sigma$ is the softmax function. Importantly, the cross-entropy cost is high when the probability assigned to the true label of a node is small i.e. $log(0) = \\infty$, while $log(1) = 0$ \u0026ndash; as such, the cross-entropy tries to minimize the rate of false negatives.\nHowever, we can incorporate more structure into this loss function. As I mentioned previously, we know that the brain is highly reproducible across individuals. In our case, we have years of biological evidence pointing to the fact that functional brain areas i.e. like the primary visual area (V1), will always be in the same anatomical location i.e. posterior occipital cortex \u0026ndash; and will always be adjacent to a small subjset of other functionally-defined areas, like the secondary visual area (V2), for example.\n  Various maps of the primate visual cortex. Tootell et al, 2003.   This leads us to the idea of assigning a high cost when nodes which should be in V1, for example, are assigned labels of regions that are not adjacent to V1. We do so by by defining another cost function:\n\\begin{align} G = -\\sum_{k=1}^{k}\\sum_{l \\in \\mathcal{L}} \\sum_{h \\in \\mathcal{L} \\setminus \\mathcal{N_{l}}} w_{l}^{k} \\cdot log(1-\\sigma(x^{k})_{l}) \\end{align}\nwhere $w_{l}^{k}$ is the probability weight assigned to label $h \\in \\mathcal{L}\\setminus \\mathcal{N_{l}}$ i.e. the set of labels not adjacent to label $l$. In order to follow the idea of a cross-entropy, we enforce the following constraints on weights $\\mathbf{w}$:\n$$ \\begin{align} w_{l}^{k} \u0026gt;\u0026amp;= 0 \\\\\n\\sum_{l \\in \\mathcal{L}} w_{l}^{k} \u0026amp;= 1 \\end{align} $$\nsuch that the vector $\\mathbf{w}$ is a probability distribution over labels. Importantly, if we consider more closely what this loss-function is doing, we are encouraging the predicted label of $x^{k}$ to not be in the set $\\mathcal{L} \\setminus \\mathcal{N_{l}}$. Assume, for example, that the true label of $x^{k}$ is $t$, and that label $j$ is not adjacent to label $t$ on the cortical surface. If the softmax function assigns a probability $p(x^{k}_{l} = j) = 0.05$, then $log(1-p(x^{k}_{l} = j))$ will be small. However, if $p(x^{k}_{l} = j) = 0.95$, then $log(1-p(x^{k}_{l} = j))$ will be large. Consequentially, we penalize higher probabilities assigned to labels not adjacent to our true label \u0026ndash; i.e. ones that are not even biologically plausible. If a candidate label of $x^{k}_{l} \\in \\mathcal{N_{t}}$, we simply set $w_{l}^{k} = 0$ \u0026ndash; that is, we do not penalize the true label (obviously), or labels adjacent to the true label, since these are the regions we really want to consider.\nBelow, I\u0026rsquo;ve implemented this loss function using Pytorch and Deep Graph Library. Assume that we are given the adjacency matrix of our mesh, the logits of our model, and the true label of our training data:\nimport numpy as np import dgl import dgl.function as fn import torch.nn.functional as F import torch as th def structured_cross_entropy(graph, logits, target): \u0026quot;\u0026quot;\u0026quot; Compute a structured cross-entropy loss. Loss penalizes high logit probabilities assigned to labels that are not directly adjacent to the true label. Parameters: - - - - - graph: DGL graph input graph structure input: torch tensor logits from model target: torch tensor true node labeling Returns: - - - - loss: torch tensor structured cross-entropy loss \u0026quot;\u0026quot;\u0026quot; # compute one-hot encoding of true labels hot_encoding = F.one_hot(target).float() # identify adjacent labels weight = th.matmul(hot_encoding.t(), th.matmul(graph.adjacency_matrix(), hot_encoding)) weight = (1-(weight\u0026gt;0).float()) # compute inverted encoding (non-adjacent labels receive value of 1) inv_encoding = weight[target] # weight by 1/(# non adjacent) # all non-adjacent labels receive the same probability # adjacent labels and self-label receive probability of 0 inv_encoding = inv_encoding / inv_encoding.sum(1).unsqueeze(1) loss = th.sum(inv_encoding*th.log(1-F.softmax(logits)), 1) return -loss.mean()  If we wanted to use this loss function in conjunction with another loss, like the usual cross-entropy, we could perform something like the following:\n# define a regularizing parameter gamma = 0.1 # define the usual cross-entropy loss function loss_fcn = torch.nn.CrossEntropyLoss() loss = loss_function(logits, target) + gamma*structured_cross_entropy(graph, logits, target) # because our new loss functions performs computations using Pytorch # the computation history is stored, and we can compute the gradient # with respect to this combined loss as optimizer.zero_grad() # zero the gradients (no history) loss.backward() # compute new gradients optimizer.step() # update weights and parameters w.r.t new gradient  Because we\u0026rsquo;re optimizing two loss functions now i.e. the global accuracy of the model as defined using the conventional cross-entropy, and the desire for predicted labels to not be far away from the true label using the structured cross-entropy, this combination of loss functions will likely have the effect of slightly reducing global accuracy \u0026ndash; however, it will have the effect of generating predictions showing fewer anatomically spurious labels i.e. we are less likely to see vertices in the frontal lobe labeled as V1, or vertices in the lateral parietal cortex labeled as Anterior Cingulate. Global predictions will be more biologically plausible. While GCNs as a whole are alreadly better-able to incorporate local spatial information than other models due to the fact that they convolve signals based on the adjacency structure of the network in question, I have found empirically that these anatomically spurious predictions are still possible \u0026ndash; hence the need for this more-structured regularization.\n","date":1607501552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607501552,"objectID":"ab933de4d5dddf40d8e20c8cec19b1e0","permalink":"/post/structured-cross-entropy/","publishdate":"2020-12-09T01:12:32-07:00","relpermalink":"/post/structured-cross-entropy/","section":"post","summary":"As I mentioned in my previous post, I work with cortical surface segmentation data. Due to the biology of the human brain, there is considerable reproducible structure and function across individuals (thankfully!). One manifestion of this reproducibility is exemplified by the neocortex a.","tags":["pytorch","regularization","loss functions"],"title":"Cross-Entropy With Structure","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m using graph convolutional networks as a tool to segment the cortical surface of the brain. This research resides in the domain of node classification using inductive learning. By node classification, I mean that we wish to assign a discrete label to cortical surface locations (nodes / vertices in a graph) on the basis of some feature data and brain network topology. By inductive learning, I mean that we will train, validate, and test on datasets with possibly different graph topologies \u0026ndash; this is in contrast to transductive learning that learns models that do not generalize to arbitrary network topology.\nIn conventional convolutions over regular grid domains, such as images, using approaches like ConvNet, we learn the parameters of a sliding filter that convolves the signal around a pixel of interest $p_{i,j}$, such that we aggregate the information from pixels $p_{\\Delta i, \\Delta j}$ for some fixed distance $\\Delta$ away from $p$. Oftentimes, however, we encounter data that is distributed over a graphical domain, such as social networks, journal citations, brain connectivity, or the electrical power grid. In such cases, concepts like \u0026ldquo;up\u0026rdquo;, \u0026ldquo;down\u0026rdquo;, \u0026ldquo;left\u0026rdquo;, and \u0026ldquo;right\u0026rdquo; do not make sense \u0026ndash; what does it mean to be \u0026ldquo;up\u0026rdquo; from something in a network? \u0026ndash; so we need some other notion of neighborhood.\nIn come graph convolutional networks (GCNs). GCNs generalize the idea of neighborhood aggregation to the graph domain by utilizing the adjacency structure of a network \u0026ndash; we can now aggregate signals near a node by using some neighborhood around it. While vanilla GCNs learn rotationally-invariant filters, recent developments in the world of Transformer networks have opened up the door for much more flexible and inductive models (see: Graph Attention Networks, GraphSAGE).\n  Demonstration of graph convolution network from Thomas Kipf.   I was specifically interested in applying the methodology described here, where the authors utilitize Gaussian kernels as filters over the neighborhood of nodes. However, the authors did not open-source their code \u0026ndash; as such, I needed to implement this method myself. Assume our input data to layer $l$ is $Y^{(l)} \\in \\mathbb{R}^{N \\times q}$ for $N$ nodes in the graph. We can define the Gaussian kernel-weighted convolution as follows:\n$$ \\begin{align} z_{i,p}^{(l)} = \\sum_{j \\in \\mathcal{N}_{i}} \\sum_{q=1}^{M_{(l)}} \\sum_{k=1}^{K_{(l)}} w_{p,q,k}^{(l)} \\cdot y_{j,q}^{(l)} \\cdot \\phi(\\hat{\\mu}_{i}, \\hat{\\mu}_{j}; \\Theta_{k}^{(l)}) + b_{p}^{(l)} \\end{align} $$\nAbove, $y_{j,q}^{(l)}$ is the $q$-th input feature of neighboring node $j$, $w_{p,q,k}^{(l)}$ is the linear weight assigned to this feature for the $k$-th kernel, and $\\phi(\\hat{\\mu}_{i}, \\hat{\\mu}_{j}; \\Theta_{k}^{(l)})$ is the $k$-th kernel weight between node $i$ and node $j$, defined as:\n$$ \\begin{align} \\phi(\\hat{\\mu_{i}}, \\hat{\\mu_{j}}; \\sigma_{k}^{(l)}, \\mu_{k}^{(l)} ) = \\exp^{-\\sigma_{k}^{(l)} \\left\\Vert (\\hat{\\mu_{i}} - \\hat{\\mu_{j}}) - \\mu_{k}^{(l)} \\right\\Vert^{2}} \\end{align} $$\nExtrinsically, the kernel weights are represented by edges in a sparse affinity matrix, such that index $(i,j)$ is the Gaussian kernel weight between node $i$ and node $j$ for the $k$-th kernel in the $l$-th layer, where nodes $j$ are restricted to be within a certain neighborhood or distance of node $i$. This can be seen more clearly here:\n  Figure from Wu et al.. $v_{i}$ is our voxel of interest, and $v_{k}^{i}$, for demonstration purposes, is an adjacent node. Both $v_{i}$ and $v_{k}^{i}$ are characterized by embedding vectors $e_{i}, e_{k}^{i} \\in \\mathbb{R}^{q}$, from which we compute the kernel weight $\\phi_{i,k}$ characterizing how similar the two vertices\u0026rsquo; embedding vectors are.   I implemented a new convolutional layer called GAUSConv (available here). To implement this algorithm, I utilized the Deep Graph Library (DGL), which offers a stellar single unifed API based on message passing (I\u0026rsquo;m using Pytorch as the backend). I noticed that I could formulate this problem using attention mechanisms described in the Graph Attention Network paper \u0026ndash; however, instead of computing attention weights using a fully connected layer as described in that work, I would compute kernel weights using Gaussian filters. Similarly, just as the GAT paper describes multi-head attention for multiple attention channels, I could analogize my fomulation to multi-head kernels for multiple kernel channels. To this end, I could make use of the GATConv API quite easily by replacing the attention computations with the Gaussian kernel filtrations. Likewise, I utilized the GraphConv API to incorporate linear weights from the Graph Convolution Network paper.\nThe GAUSConv layer is similar to both the GraphConv and GATConv layers but differs in a few places. Rather than initializing the layer with attention heads, we initialize it with the number of kernels and a kernel dropout probability.\nGAUSConv(in_feats, # number of input dimensions out_feats, # number of output features num_kernels, # number of kernels for current layer feat_drop=0., # dropout probability of features kernel_drop=0., # dropout probability of kernels negative_slope=0.2, # leakly relu slope activation=None, # activation function to apply after forward pass random_seed=None, # for example / reproducibility purposes allow_zero_in_degree=False)  Importantly, in the layer instantiation, we define linear weights and kernel mean and sigma parameters, mu and sigma. We initialize both kernel parameters with the flag require_grad=True, which enables us to update these kernel parameters during the backward pass of the layer. Both parameters are initialized with values in the reset_parameters method.\n# initialize feature weights and bias vector self.weights = nn.Parameter( th.Tensor(num_kernels, in_feats, out_feats), requires_grad=True) self.bias = nn.Parameter( th.Tensor(num_kernels, out_feats), requires_grad=True) # initialize kernel perameters self.mu = nn.Parameter( th.Tensor(1, num_kernels, in_feats), requires_grad=True) self.sigma = nn.Parameter( th.Tensor(num_kernels, 1), requires_grad=True)  Now here is the clever part, and where the DGL message passing interface really shines through. DGL fuses the send and receive messages so that no messages between nodes are ever explicitly stored, using built-in message and reduce functions. To compute the kernel weights between all pairs of source and destrination nodes, we use these built-in functions. The important steps are:\n  compute node feature differences between all source / destination node pairs\n  aggregate and reduce incoming messages from destination nodes scaled by the kernel weights, to update the source node features\n  In the forward pass of our layer, we perform the following steps:\n### forward pass of GAUSConv layer ### # compute all pairwise differences between adjacent node features graph.ndata['h'] = feat graph.apply_edges(fn.u_sub_v('h', 'h', 'diff')) # compute kernel weights for each source / desintation pair e = graph.edata['diff'].unsqueeze(1) - mu e = -1*sigma*th.norm(e, dim=2).unsqueeze(2) e = e.exp() graph.edata['e'] = e # apply kernel weights to destination node features graph.apply_edges(fn.v_mul_e('h', 'e', 'kw')) # apply linear projection to kernel-weighted destination node features a = th.sum(th.matmul(graph.edata['kw'].transpose(1, 0), weights), dim=0) # apply kernel dropout a = self.kernel_drop(a) graph.edata['a'] = a # final message-passing and reduction step # aggregate weighted destination node features to update source node features graph.update_all(fn.copy_e('a', 'm'), fn.sum('m', 'h')) rst = graph.ndata['h']  As an example, given a graph and features, we instantiate a GAUSConv layer and propogate our features through the network via:\n# set random seed random_seed=1 # define arbitrary input/output feature shape n_samples = 4 in_feats=4 out_feats=2 features = th.ones(n_samples, in_feats) # define number of kernels num_kernels=2 # create graph structure u, v = th.tensor([0, 0, 0, 1]), th.tensor([1, 2, 3, 3]) g = dgl.graph((u, v)) g = dgl.to_bidirected(g) g = dgl.add_self_loop(g) # instantiate layer GausConv = GAUSConv(in_feats=in_feats, out_feats=out_feats, random_seed=random_seed, num_kernels=num_kernels, feat_drop=0, kernel_drop=0) # forward pass of layer logits = GausConv(g, features) print(logits) tensor([[0.1873, 0.7217], [0.1405, 0.5413], [0.0936, 0.3608], [0.1405, 0.5413]], grad_fn=\u0026lt;AddBackward0\u0026gt;)  ","date":1607408657,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607408657,"objectID":"a9923b0fc7db991c10c368e39704fc8c","permalink":"/post/gaussian-kernel-convolution/","publishdate":"2020-12-07T23:24:17-07:00","relpermalink":"/post/gaussian-kernel-convolution/","section":"post","summary":"I\u0026rsquo;m using graph convolutional networks as a tool to segment the cortical surface of the brain. This research resides in the domain of node classification using inductive learning. By node classification, I mean that we wish to assign a discrete label to cortical surface locations (nodes / vertices in a graph) on the basis of some feature data and brain network topology.","tags":["pytorch","AI","graph convolution networks"],"title":"Gaussian Graph Convolutional Networks","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m interested in keeping better track of the books I\u0026rsquo;ve read recently, and those I\u0026rsquo;m currently reading. One of my recent goals was to be more meticulous about defining words I don\u0026rsquo;t know, which, in all honestly, has made reading a much slower process, but has added so much more depth to the material. I\u0026rsquo;ve also found, empirically, that physically looking words up as I\u0026rsquo;m reading and defining them in context helps me retain their meaning. I like the tangibility of actual books, so I ended up buying an Oxford dictionary for that purpose. It\u0026rsquo;s useful for me to annotate material in the books I\u0026rsquo;m reading without constantly going back and forth with my phone. I actually think it might be kind of fun to keep an active list of new words I come across!\nAnyways, currently reading:\n Gravity\u0026rsquo;s Rainbow by Thomas Pynchon  This is honestly a doozy, a head-scratcher, and a literal tome, that seems to lack any semblence of coherence. But people keep saying it\u0026rsquo;s one of the greatest American novels, so I\u0026rsquo;m going to stick with it for the time being\u0026hellip;   The Island of the Day Before by Umberto Eco  I\u0026rsquo;ve had this one on my shelf for so long, I found it high-time to finally read it.    Recently read:\n Death with Interruptions by Jose Saramago  I loved Blindness, but I didn\u0026rsquo;t feel as invested in this one. The concept was pretty unique, and I liked the parallel timelines devoted to Death herself, and to the crumbling of society as Death removes herself from business.   The Overstory by Richard Powers  Warning: spoiler alert to follow\u0026hellip; Arguably one of the best books I\u0026rsquo;ve ever read. Powers devotes 500 pages as a devotional to trees and the living biosphere as a whole, taking the point of view that the Earth will be fine, irrespective of climate change, but humans need Nature\u0026rsquo;s help escaping and surviving ourselves. Although the ending is up in the air, Patricia Westerford\u0026rsquo;s character, a biologist who devotes her life to the study of forests (and who has to be an homage to Dr. Suzanne Simard at UBC, who first identified that forests communicate with one another through mycorrhizal networks in the soil) adopts the radical eco-activist mentality that the Earth will be better off without humans at all, and takes the extreme-solution of \u0026ldquo;unsuicide\u0026rdquo; \u0026ndash; decentering humans and putting the focus on the survival of nature. This was a gut-wrencher, but one that has had a long-lasting effect on my perception of humanity\u0026rsquo;s deep-rooted cognitive dissonance and selfishness.    Up next:\n A Woman Looking At Men Looking At Women: Essays on Art, Sex, and the Mind by Siri Hustvedt  This was a gift from my sister. I\u0026rsquo;m excited to read this, since non-fiction doesn\u0026rsquo;t often makes its way into my literary ledgers.   The Memory Police by Yoko Ogawa  I love dystopian and surrealism \u0026ndash; really looking forward to this.    ","date":1606199057,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606199057,"objectID":"4b3bdc1a22f618834a68bbbd9609b2d8","permalink":"/personal/literature-1/","publishdate":"2020-11-23T23:24:17-07:00","relpermalink":"/personal/literature-1/","section":"personal","summary":"I\u0026rsquo;m interested in keeping better track of the books I\u0026rsquo;ve read recently, and those I\u0026rsquo;m currently reading. One of my recent goals was to be more meticulous about defining words I don\u0026rsquo;t know, which, in all honestly, has made reading a much slower process, but has added so much more depth to the material.","tags":["literature","fiction","non-fiction"],"title":"Literature #1","type":"personal"},{"authors":[],"categories":[],"content":"My friend David moved to London recently, so we set out to do a long trail-run before he left the States. We decided on Buck Creek Pass Loop, a 36 mile loop on the east side of Glacier Peak in the Easter Cascades. It had been on my list for a long time, and I was considering doing it solo for a while, but am glad David ended up joining. We camped near the trailhead the night before, and departed for the long slog around 6am. This wasn\u0026rsquo;t my longest run, but this one definitely hurt the most. Scroll down for some beautiful scenery, courtesy of David\u0026rsquo;s fancy camera!\n   Nothing beats the morning alpenglow of the Cascades.     Chiwawa Peak in the background. With so much green on the western slopes of the Cascades, you could be forgiven for forgetting how magnificent the eastern is.     Ascending the Spider Glacier. We saw some small crevasses on the way up, but the ice was so melted out (nearly gone, in fact), that we chanced the rope-free climb.     Alpine tributary turbulence in Lyman Lake     Glacier Peak: another one of my top 5 Spring skiing goals for 2021.   ","date":1603002257,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603002257,"objectID":"50488e298db4b0d6b40446e9a80eb910","permalink":"/personal/buck-creek-pass/","publishdate":"2020-10-17T23:24:17-07:00","relpermalink":"/personal/buck-creek-pass/","section":"personal","summary":"My friend David moved to London recently, so we set out to do a long trail-run before he left the States. We decided on Buck Creek Pass Loop, a 36 mile loop on the east side of Glacier Peak in the Easter Cascades.","tags":[],"title":"Buck Creek Pass Loop","type":"personal"},{"authors":[],"categories":[],"content":"It\u0026rsquo;s that time of year again! Every year for the past few years, I done a solo ski tour of Eldorado Peak in the North Cascades. This doesn\u0026rsquo;t preclude doing it with friends \u0026ndash; it\u0026rsquo;s just a climb that really tests your mettle (you gain a little over 7000 feet in roughly 5 miles), and sometimes I want to attempt this sufferfest in peace! Eldorado is one of the crown jewels of the Cascade Range, and one of North America\u0026rsquo;s 50 Classic Ski Descents. I\u0026rsquo;ve been lucky to have pretty good snow conditions for each of my attempts, although the same can\u0026rsquo;t always be said for the weather!\n  Looking south from the Eldorado boulder field at Johannesburg Mountain. Just to looker\u0026rsquo;s left of Johannesburg is a narrow couloir called CJ\u0026rsquo;s Couloir \u0026ndash; one of my top-5 goals for the upcoming 2021 Spring skiing season.     Looking northwest at the Boston Glacier just as you cross the ridgeline onto Inspiration Glacier     View of the Eldorado summit from the Inspiration Glacier.     On top of the Eldorado summit, waiting for some climbers to descend off the spine.     Looking south from the top of Eldorado at Forbidden Peak and Moraine Lake (frozen over).   ","date":1594535057,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594535057,"objectID":"e22cf67bf243b331484d16b396e8b084","permalink":"/personal/eldorado-2020/","publishdate":"2020-07-11T23:24:17-07:00","relpermalink":"/personal/eldorado-2020/","section":"personal","summary":"It\u0026rsquo;s that time of year again! Every year for the past few years, I done a solo ski tour of Eldorado Peak in the North Cascades. This doesn\u0026rsquo;t preclude doing it with friends \u0026ndash; it\u0026rsquo;s just a climb that really tests your mettle (you gain a little over 7000 feet in roughly 5 miles), and sometimes I want to attempt this sufferfest in peace!","tags":[],"title":"Eldorado, 2020","type":"personal"},{"authors":null,"categories":null,"content":"See my post with a brief algorithmic introduction for computing distances between pairs of subspaces.\nsubmet is a Python package for computing pairwise distances between equidimensional subspaces. All of these subspace metrics are dependent on the principle angles between the two subspaces. submet implements an sklearn-styled interface with a fitting method via a class called SubspaceDistance, along with a variety of metrics in a class called Metric.\nimport numpy as np import submet # generate random subspaces s1 = np.random.rand((10, 2)) s2 = np.random.rand((10, 2)) # instantiate SubspaceDistance object, using the Grassmann distance metric S = submet.subspaces.SubspaceDistance(metric='Grassmann') # compute the distance between two subspaces S.fit(s1, s2) # print computed distance print(S.distance_)  Metric implements the following distance metrics:\n Asimov Binet-Cauchy Chordal Fubini-Study Martin Procrustes Projection Spectral  SubspaceDistance does not currently support pairwise distance computations between more than a single pair of subspaces, in an analogous way to Scipy\u0026rsquo;s pdist or cdist methods. I am working on allowing pairwise computations using numba.\nUpdate as of 6/30/2020: the above issue has been fixed and submet now allows for pairwise distance matrix computations, rather than only single subspace pair comparisons. See this pull request\n","date":1593388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593388800,"objectID":"a66319846c09d2e56c2f1562ccf9484b","permalink":"/project/submet/","publishdate":"2020-06-29T00:00:00Z","relpermalink":"/project/submet/","section":"project","summary":"package to compute various distance metrics between subspaces","tags":["linear subspaces","geodesics","metrics"],"title":"submet","type":"project"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m working with some multi-dimensional float-valued data \u0026ndash; I\u0026rsquo;ll call a single instance of this data $X \\in \\mathbb{R}^{n \\times k}$. I have multiple samples $X_{1}, X_{2}\u0026hellip;X_{t}$, and want to compare these subspaces \u0026ndash; namely, I want to compute the distance between pairs of subspaces.\nLet\u0026rsquo;s assume that our subspaces are not rank-deficient \u0026ndash; i.e. for a given subspace sample, all of our dimensions are linearly independent. Thus, the $k$ vectors form a basis set that spans some $k$-d subspace in $\\mathbb{R}^{n}$. We can think of each $k$-d subspace as a hyperplane in $(k+1)$-d space, just as we can think of a 2-d plane in 3-d space. One way to compare these subspaces is by using the \u0026ldquo;principle angles between subspaces\u0026rdquo; (or angles between flats). We can compare the \u0026ldquo;angles\u0026rdquo; between these hyperplanes, which will tell us how \u0026ldquo;far apart\u0026rdquo; the two subspaces are.\n  Intersecting 2D linear subspaces.   This comparison is effectively based on the QR decomposition and the Singular Value Decomposition. For two subspaces $[U, W]$, we compute the QR decomposition of both:\n$$\\begin{align} U \u0026amp;= Q_{u}R_{u}\\\\\nW \u0026amp;= Q_{w}R_{w}\\\\\n\\end{align}$$\nwhere $Q_{u}$ and $Q_{w} \\in \\mathbb{R}^{n \\times k}$ are orthonormal bases such that $Q_{u}^{T}Q_{u} = Q_{w}^{T}Q_{w} = I_{k}$ that span the same subspace as the original columns of $U$ and $W$, and $R_{u}$ and $R_{w} \\in \\mathbb{R}^{k \\times k}$ are lower triangular matrices. Next, we compute the matrix $D = \\langle Q_{u}, Q_{w} \\rangle = Q_{u}^{T} Q_{w} \\in \\mathbb{R}^{k \\times k}$, and then apply the singular value decomposition:\n$$\\begin{align} D = USV^{T} \\end{align}$$\nWe can sort of think of $D$ as the cross-covariance matrix. As such, the singular vectors represent the main orthogonal axes of cross-covariation between our two subspaces, while the singular values represent angles. In order to compute the principle angles of our subspaces, we simply take\n$$\\begin{align} \\theta \u0026amp;= cos^{-1}(S) \\\\\n\u0026amp;=cos^{-1}[\\sigma_{1}, \\sigma_{2}\u0026hellip;\\sigma_{k}] \\end{align}$$\nwhich gives us the principle angles (in radians). Because the SVD is invariant to sign (+/-), the principle angles range between $\\Big[0, \\frac{\\pi}{2}\\Big]$. This means that subspaces that span the same space have a principle angle of 0, and subspaces that are orthogonal (maximally far apart) to one another have a principle angle of $\\frac{\\pi}{2}$.\nIn order to compute the \u0026ldquo;distance\u0026rdquo; between our subspaces, we can apply various metrics to our vector of principle angles. The simplest approach is to apply the $L2$ norm to our vector of principle angles, $\\theta$, as\n$$\\begin{align} d(X_{i}, X_{j}) = \\sqrt{\\sum_{n=1}^{k} cos^{-1}(\\sigma_{n})^{2}} \\end{align}$$\nThis metric is called the Grassmann Distance and is formally related to the geodesic distance between subspaces distributed on the Grassmannian manifold.\n  Grassmann manifold and its tangent space.   This, however, is a topic for another future blog post. There are a variety of metrics we can use to compute the pairwise distance between subspaces, some of which are\n Asimov: $\\; max(\\theta)$ Fubini-Study: $\\; cos^{-1}(\\prod sin(\\theta))$ Spectral: $\\; 2 sin(\\frac{max(\\theta)}{2})$  but all are fundamentally based on some function of our vector of principle angles, $\\theta$.\n","date":1593037631,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593037631,"objectID":"904cf09255bff59b28c04a6480745ec5","permalink":"/post/comparing-subspaces/","publishdate":"2020-06-24T15:27:11-07:00","relpermalink":"/post/comparing-subspaces/","section":"post","summary":"I\u0026rsquo;m working with some multi-dimensional float-valued data \u0026ndash; I\u0026rsquo;ll call a single instance of this data $X \\in \\mathbb{R}^{n \\times k}$. I have multiple samples $X_{1}, X_{2}\u0026hellip;X_{t}$, and want to compare these subspaces \u0026ndash; namely, I want to compute the distance between pairs of subspaces.","tags":["subspaces","manifold","Riemannian","metrics"],"title":"Distances Between Subspaces","type":"post"},{"authors":["Kristian Eschenburg","David Haynor","Tom Grabowski"],"categories":null,"content":"","date":1592784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592784000,"objectID":"04fc46c22dfe21cafb97b2d4b2b81d71","permalink":"/publication/topography/topography/","publishdate":"2020-06-22T00:00:00Z","relpermalink":"/publication/topography/topography/","section":"publication","summary":"Using non-linear dimensionality reduction of functional brain connectivity patterns, and multivariate spatial statistics to characterize the functional embeddings, we analyze the spatial relationships between pairs of cortical regions to better examine how pairs of cortical regions connect and relate to one another.","tags":["Laplacian eigenmaps","Procrustes","dimensionality reduction"],"title":"Linear Mapping of Cortico-Cortico Resting-State Functional Connectivity","type":"publication"},{"authors":null,"categories":null,"content":"This is a package to apply the distance-dependent Chinese Restaurant Process (dd-CRP) to multi-dimensional graph-based data. It is based roughly on code originally written by Christopher Baldassano.\nMy contributions to this package are three-fold: In contrast to work presented by Baldassano et al. and Moyer et al., whose dd-CRP methods both model the univariate similarities within and between clusters, this method models the clusters themselves, placing multidimensional priors on the features of each cluster. It then explores the posterior parameter space using Collased Gibbs Sampling.\n  This version treats the high-dimensional feature vectors as being sampled from multivariate Gaussian distributions. In contrast to Baldassano et al. who sample similarities between feature vectors from a univariate Gaussian, and Moyer et al. who sample counts from a Poisson, this method models the full-dimensional data points themselves.\n  The prior distribution on features are based on an abstract class contained in PriorsBase.py called Prior with the following 3 abstract methods:\n  Prior.sufficient_statistics\n  Prior.posterior_parameters\n  Prior.marginal_evidence\n  Any model that implements the above three methods can be incorporated into Priors.py alongside the Normal-Inverse-Chi-Squared (NIX2) and Normal-Inverse-Wishart (NIW) models.\n  On a more aesthetic level, I have refactored much of Balassano\u0026rsquo;s original Python code to make this version object-oriented and more user-friendly under the hood.\n  How to install and use: git clone https://github.com/kristianeschenburg/ddCRP.git cd ./ddCRP pip install .  Example application to synthetic data: We begin by importing the necessary modules. Specifically, ddCRP contains the actual algorithm, Priors contain the prior distribution models, and synthetic contains a suite of methods for generating synthetic data.\nfrom ddCRP import ddCRP from ddCRP import Priors from ddCRP import synthetic  Next, we define the hyperparameters, and instantiate our prior model with these parameters. In this case, we are using the Normal-Inverse-Chi-Squared model.\nalpha=10 mu = 0 kappa = 0.0001 nu = 1 sigma = 1 nix = Priors.NIX2(mu0=mu, kappa0=kappa, nu0=nu, sigma0=sigma)  Next, we sample 5-dimensional synthetic data from the same prior distribution. The synth object computes an adjacency list of our synthetic data (synth.adj_list), as well as a ground-truth label map (synth.z_).\n# dimensionality of data d = 5 # sample synthetic features for each label # If you want to sample synthetic data from a different # Normal-Inverse-Chi-Squared distribution, # change kappa, mu, nu, and sigma synth = synthetic.SampleSynthetic(kind='ell', d=d, mu0=mu, kappa0=kappa, nu0=nu, sigma0=sigma) synth.fit()  Finally, we instantiate our ddCRP model with the concentration parameter alpha, our prior model nix, along with some other parameters that govern the number of MCMC passes to perform and how often to sample diagnostic statistics about our model performance. You can see\n# fit the ddCRP model # once fitted, crp.map_z_ is the MAP label crp = ddCRP.ddCRP(alpha, model=nix, mcmc_passes=30, stats_interval=200) crp.fit(synth.features_, synth.adj_list, gt_z=synth.z_)    Model convergence in synthetic data.   For more information on the Chinese Restaurant Process, see:\n  Baldassano et al. (2015), Parcellating Connectivity In Spatial Maps. PeerJ 3:e784; DOI 10.7717/peerj.784\n  Moyer et al. (2017), A Restaurant Process Mixture Model for Connectivity Based Parcellation of the Cortex. arXiv:1703.00981\n  Blei, David M. et al (2010), The Nested Chinese Restaurant Process and Bayesian Nonparametric Inference of Topic Hierarchies. JACM.\n  ","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"93f69227c18d90eed2045b914b5554d6","permalink":"/project/ddcrp/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/project/ddcrp/","section":"project","summary":"package to fit distance-dependent Chinese Restaurant Process models","tags":["machine learning","inifinite-dimensional","non-parametric","clustering"],"title":"ddCRP","type":"project"},{"authors":null,"categories":null,"content":"This is a Python package for fitting densities to scalar fields distributed on the domain of regular meshes. This package is an implementation of the methodology described here. Rather than using the spatial coordinates of the mesh on which the data is distributed, this package fits isotropic distributions using the geodesic distances between mesh vertices.\nGiven a scalar field assigning, let\u0026rsquo;s say, elevation values to latitude-longitude coordinates of Mount Rainier, we could use fieldmodel to fit a Gaussian distribution to this elevation profile. The output of this fitting procedure would be 2 parameters: a mean parameter (centered on the true summit coordinates) and a sigma parameter, a measure of how quickly the elevation profile decays as we move away from the summit.\n  Model convergence in synthetic data.   The code iterates over all possible mean locations in a set, and optimizes the sigma parameter using a BFGS minimization procedure. I provide 3 optimality criteria options:\n maximization of spatial correlation maximization of signal amplitude minimization of least-squares error  Along with fitting procedure, fieldmodel also offers some basic plotting functionality to visualize the fitted densities alongside the data to which those densities were fit.\nHow to install and use: git clone https://github.com/kristianeschenburg/fieldmodel.git cd ./fieldmodel pip install .  Example application to synthetic data: import numpy as np import scipy.io as sio # Create fake distance matrix between and fake scalar map # x coordinates for plotting tx_file = '../data/target.X.mat' tx = sio.loadmat(tx_file)['x'].squeeze() # y coordinates for plotting ty_file = '../data/target.Y.mat' ty = sio.loadmat(ty_file)['y'].squeeze() # geodesic distance matrix between vertices dist_file = '../data/distance.mat' dist = sio.loadmat(dist_file)['apsp'] # scalar field field_file = '../data/scalar_field.mat' field = sio.loadmat(field_file)['field'].squeeze()  Next, we can instantiate and fit our fieldmodel. For more detail on the parameters, please refer to the docstring or Demo.ipynb in the demos directory.\nfrom fieldmodel import GeodesicFieldModel as GFM # instantiate field model G = GFM.FieldModel(r=10, amplitude=False, peak_size=15, hood_size=20, verbose=False, metric='pearson') # fit field model G.fit(data=field, distances=dist, x=tx, y=ty)  We can visualize the results of our model via the plot method in GFM.fieldmodel:\nG.plot(field='pdf') G.plot(field='amplitude') G.plot(field='sigma')    Fitted fieldmodel density.     Estimated model amplitude for each candidate mean location.     Estimated model sigma for each candidate mean location.   ","date":1591747200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591747200,"objectID":"18b7714a68f31c4506e6dcdfcaf7cb5b","permalink":"/project/fieldmodel/","publishdate":"2020-06-10T00:00:00Z","relpermalink":"/project/fieldmodel/","section":"project","summary":"package to fit distributions over scalar fields on the domain of regular meshes","tags":["image processing","regular mesh","spatial densities"],"title":"fieldmodel","type":"project"},{"authors":[],"categories":[],"content":"What follows are the contents of part of a lab meeting presentation I gave recently. The topic of the meeting was \u0026ldquo;Python for Neuroimaging\u0026rdquo;, where I covered basic software development tools that brain imaging scientists might be interested in.\nCreating Python Packages In this lesson, I\u0026rsquo;ll show you how to build your own Python package that you can then install locally or upload to the Python Packaging Index (for those of you familar with R, think CRAN, but for Python).\nI\u0026rsquo;m going to be basing a lot of the material off of this documentation, but will also show a real example using some of my own personal code.\nWhat are packages? I\u0026rsquo;m sure most of you are familiar with packages and libraries already, either from Matlab, R, or Python. Packages are basically bundles of various snippets of code, i.e. methods, classes, scripts, tests etc. that are bundled together to perform some function. Generally (hopefully), there is coherence to what these snippets of code do \u0026ndash; they should interact together in some way or relate to some overarching computational goal.\nWithin a package, you can have different groupings of code, where each grouping does some unique or discrete computing. These groupings are called submodules. A common submodule in many packages is an Input / Output (io) module that will read and write data that this package interacts with or produces. Another common submodule is often related to plotting the outputs of your code. And then almost always, there are submodules that perform the brunt of the algorithmic work. So inside modules, you\u0026rsquo;ll find snippets of code that relate to the goal or concept of the module.\nThink of a package as a toolbox with a bunch of drawers, each with a label: wood-working, welding, gardening, flooring, etc. These drawers are submodules. You can tell by their names that they each cover certain topics. Each drawer contains a set of tools: woord-working might contain saw, nail, sandpaper, wood glue, while welding might containg solder, flux, oxygen, glove. These tools are the functions, classes, and scripts that relate to that submodule.\nOverall, this toolbox performs some stuff related to construction, homebuilding, repair, and has discrete bundles of code useful for a variety of those tasks.\nDirectory structure for a Python package Here we examine the skeleton of a package. All packages follow this basic structure.\npkg_name |-- __init__.py |-- LICENSE |-- pkg_name/ | |-- submodule_a/ | | |-- __init__.py | | |-- a_1.py | |-- submodule_b/ | | |-- __init__.py | | |-- b_1.py | | |-- b_2.py |-- README.md |-- setup.py |-- test/ | |-- __init__.py  __init__.py is a required file that allows your package to be imported. The only __init__.py file that needs to contain anything is the highest-level file. The others can be empty, but they must exist. Here are the contents of the highest-level __init__.py file:\n__all__ = [ 'a_1', 'b_1', 'b_2' ] from .submodule_a import (a_1) from .submodule_b import (b_1, b_2)  LICENSE tells other users / individuals in what capacity they are allowed to use your code.\nREADME.md describes how to use your code, and often contains examples. This is a markdown file, but can generally be any type of markup language.\ntest/ is a directory in which you would want to write unit tests for your code.\nsetup.py is what allows your to install your package. It\u0026rsquo;s a set of instructions that get supplied to setuptools package.\nfrom setuptools import setup, find_packages with open(\u0026quot;README.md\u0026quot;, \u0026quot;r\u0026quot;) as fh: long_description = fh.read() setup( name='pkg_name', version='0.1.0', author='Kristian M. Eschenburg', author_email='keschenb@uw.edu', packages=find_packages(), scripts=[], url=\u0026quot;https://github.com/kristianeschenburg/pkg_name\u0026quot;, license='LICENSE.txt', description='An awesome package that does something', long_description=long_description, install_requires=[ \u0026quot;numpy\u0026quot; \u0026quot;pytest\u0026quot;, \u0026quot;matplotlib\u0026quot;, ], )  Compiling, installing, and uploading your package 1. Register on PyPi\nOnce we\u0026rsquo;ve done all this, we\u0026rsquo;re just about ready to create our Python package and upload it to pypi.org. But first, we need to create an account. For testing purposes, we\u0026rsquo;ll create a test account here, but the process is the same.\nAfter you create your account, we need to create an API token, that will allow us to upload files to either Test PyPi or PyPi (depending on what we\u0026rsquo;re doing) \u0026ndash; the following steps are the same, regardless.\nUnder your Test PyPi account, click your username in the top right, go to Account Settings:\n  Scroll down and click Add API Token:\n  Follow the instructions there, making sure to select \u0026ldquo;Entire Account\u0026rdquo; option under the Scope tab.\nDO NOT CLOSE THIS WINDOW WHEN THIS IS COMPLETE\nNext, type\ncd $HOME touch .pypirc  and using your favorite text editor, enter the following:\n[testpypi] repository: https://test.pypi.org/legacy/ username = __token__ password = pypi-***  If we were creating a token for PyPi, we\u0026rsquo;d type:\n[pypi] repository: https://pypi.org/ username = __token__ password = pypi-***  Close your current terminal, and open a new window to refresh your settings. Now, when we go to upload our package to PyPi, we\u0026rsquo;ll be able to type the commands without needed to supply a username and password directly.\n2. Compile your package\nFirst, we need to make sure that a few Python packages are installed. Namely, we need to install pip\ncurl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python get-pip.py pip install -U pip  Then we can install the following packages:\npip install --upgrade pip setuptools wheel # for installing Python packages pip install tqdm # progress bar package pip install --user --upgrade twine # for publishing to PyPi  Then, we can compile our package:\npython setup.py bdist_wheel  which creates the directories dist, build, and pkg_name.egg-info. The *.egg-info file is basically some zipped meta-data about your package, but we\u0026rsquo;re really only interested in the *.whl file in dist \u0026ndash; \u0026ldquo;wheels\u0026rdquo; are a \u0026ldquo;distribution\u0026rdquo; format, newly designed to replace \u0026ldquo;eggs\u0026rdquo;. I won\u0026rsquo;t go into it here, but eggs were sort an ad hoc solution to packaging Python code \u0026ndash; wheels were part of PEP427 i.e. is actually an \u0026ldquo;enhancement\u0026rdquo; to the Python language, and the formal way of packaging Python code.\n3. Installing your code\nWe can install our code locally with:\npip install dist/pkg_name-0.0.0-py3-none-any.whl  If you want to install an \u0026ldquo;editable\u0026rdquo; version of your package, do this:\npip install -e .  This will allow you to change your *.py files and have these changes take effect immediately when importing your package, without needing to rebuild each time \u0026ndash; but this method installs from the egg distribution, and generally produces larger build files, since the build needs to keep track of your actual source code.\n4. Upload your code\nWe can upload our code to PyPi now using the following command:\npython3 -m twine upload --repository testpypi dist/*  Now, if you click \u0026ldquo;Your Projects\u0026rdquo; under your account name on PyPi, you\u0026rsquo;ll see that you project has been uploaded.\n** I should note that, any time you want to upgrade your code and upload it to PyPi again, you need to remove all files from the dist directory, increment the version number in the setup.py file \u0026ndash; i.e. 0.0.0 \u0026ndash;\u0026gt; 0.0.1 \u0026ndash; rebuild your package with\nbash python setup.py bdist_wheel  Example with personal package I don\u0026rsquo;t generally upload my code to PyPi (probably scared bugs in the code, and people finding them, and then thinking I\u0026rsquo;m terrible at software development, and going down a long spiral of self-deprecation, but I digress\u0026hellip;) but I do upload it all to GitHub. In either case, here is a walk-through of packaging some software called pysurface that I use for processing mesh-based data \u0026ndash; I use it for adjacency matrices, performing Laplacian smoothing on surfaces, sampling points from mesh triangle simplices, plotting on surfaces\u0026hellip; Just some stuff that I find myself doing a lot.\nHere is the directory containing all my code:\n  You\u0026rsquo;ll see 5 different modules: graphs, operations, plotting, spectra, and utilities, and you\u0026rsquo;ll note that each module directory has a __init__.py file.\nHere is my setup.py file:\nfrom os import path from setuptools import setup, find_packages import sys here = path.abspath(path.dirname(__file__)) with open(path.join(here, 'README.rst'), encoding='utf-8') as readme_file: readme = readme_file.read() with open(path.join(here, 'requirements.txt')) as requirements_file: # Parse requirements.txt, ignoring any commented-out lines. requirements = [line for line in requirements_file.read().splitlines() if not line.startswith('#')] setup( name='pysurface', version=\u0026quot;0.0.4\u0026quot;, description=\u0026quot;Python package for quickly processing surface meshes.\u0026quot;, long_description=readme, author=\u0026quot;Kristian Eschenburg\u0026quot;, author_email='keschenb@uw.edu', url='https://github.com/kristianeschenburg/pysurface', packages=find_packages(), entry_points={ 'console_scripts': [ # 'some.module:some_function', ], }, include_package_data=True, package_data={ 'pysurface': [ # When adding files here, remember to update MANIFEST.in as well, # or else they will not be included in the distribution on PyPI! # 'path/to/data_file', ] }, install_requires=requirements, license=\u0026quot;BSD (3-clause)\u0026quot;, classifiers=[ 'Development Status :: 2 - Pre-Alpha', 'Natural Language :: English', 'Programming Language :: Python :: 3', ], )  You can see that I\u0026rsquo;ve run\npython setup.py bdist_wheel  based off the dist, build, and pysurface.egg-info directories. dist contains a file called pysurface-0.0.4-py3-none.any.whl, which is the actual distribution that can be used for installation. I\u0026rsquo;ve uploaded the code to Test PyPi, and this is what we see:\n  We can then install the package and all of it\u0026rsquo;s dependencies from TestPypi via\npip install --index-url https://test.pypi.org/simple/ pysurface  Now I can do something like the following in a Python script:\nimport pysurface # or from pysurface import graphs, spectra # or from pysurface.spectra import eigenspectrum  ","date":1591550409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591550409,"objectID":"c0fbb944394f9681e0614305c0a296e3","permalink":"/post/pyni-packages/","publishdate":"2020-06-07T10:20:09-07:00","relpermalink":"/post/pyni-packages/","section":"post","summary":"What follows are the contents of part of a lab meeting presentation I gave recently. The topic of the meeting was \u0026ldquo;Python for Neuroimaging\u0026rdquo;, where I covered basic software development tools that brain imaging scientists might be interested in.","tags":[],"title":"Lab Meeting: pip and the Python Packaging Index","type":"post"},{"authors":[],"categories":[],"content":"No, this was not a ski tour for my birthday\u0026hellip; but it could have been! The Birthday Tour is another classic PNW route on the east side of the North Cascades off of Highway 20 near Washington Pass. This is a relatively quick tour, with a mild amount of elevation gain, but some pretty enjoyable downhill bits.\n  Mike ascending the first uphill bit near Liberty Bell.     Liberty Bell in the background, with Mike skiing up the last section prior to our first descent.     Mike descending off of the ridgeline between Cornice Peak and Blue Lake Peak. We timed this descent perfectly and were able to harvest some corn!     Our last ridgeline descent \u0026ndash; about to drop in next to Pica Peak.   ","date":1591251857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591251857,"objectID":"60cebf57258b2911f81a693973f91232","permalink":"/personal/birthday-tour-2020/","publishdate":"2020-06-03T23:24:17-07:00","relpermalink":"/personal/birthday-tour-2020/","section":"personal","summary":"No, this was not a ski tour for my birthday\u0026hellip; but it could have been! The Birthday Tour is another classic PNW route on the east side of the North Cascades off of Highway 20 near Washington Pass.","tags":[],"title":"Birthday Tour, 2020","type":"personal"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m fortunate enought to work in a lab with some high-level computing infrastructure. We have a cluster of machines using the Sun Grid Engine (SGE) software system for distributed resource management. The other day, I was searching for how to wrap my Python scripts with qsub so that I could submit a batch of jobs to our cluster. Eventually, I want to be able to submit jobs with dependencies between them, but we\u0026rsquo;ll start here.\nLet\u0026rsquo;s create an example script that computes the mean of an MRI image. Let\u0026rsquo;s call the script compute_mean.py:\n#!/usr/bin/env python import argparse import nibabel as nb import numpy as np import pandas as pd parser = ArgumentParser('Compute the mean of MRI, and save to CSV file.') parser.add_argument('-i', '--input_image', help='Path to MRI image.', required=True, type=str) parser.add_argument('-o', '--output_csv', help='Output CSV file.', required=True, type=str) args = parser.parse_args() # read in image img = nb.load(args.input_image) # get voxel-wise data data = img.get_data() # compute mean mu = np.mean(data) # save to csv df = pd.DataFrame({'mean': [mu]}) df.to_csv(args.output_csv)  The first line of this script, #!/usr/bin/env python tells the script to use the local python environment. In my case, I have a customized installation of Python, along with a bunch packages and libraries that I\u0026rsquo;ve written and installed that are not available for the rest of my lab (since they\u0026rsquo;re still in the testing phase or just something I\u0026rsquo;m experimenting with). This line tells the script to use my Python environment, rather than the default version on our servers.\nWe can then create a bash wrapper, let\u0026rsquo;s call in mean_wrapper.sh for a single subject\n#!/bin/bash #$ -M keschenb@uw.edu #$ -m abe #$ -r y #$ -o tmp.out #$ -e tmp.err # Compute mean of image image=$1 output=$2 python compute_mean.py ${image} ${output}  The second and third line here, with the M and m parameters, tell the script to email me once it completes the processing (or if there are any errors). And finally, we can create a wrapper that takes in a list of subjects to process, and the input and output directories, and submits each individual job to the queue using qsub:\n#!/bin/bash subjects=$1 image_dir=$2 output_dir=$3 # we create a variable, as our cluster has 2 different queues to use # this could be hardcoded though queue_name=$4 while read subj do image_file=${image_dir}${subj}.nii.gz output_file=${output_dir}${subj}.csv qsub -q ${queue_name}.q mean_wrapper.sh ${input_image} ${output_file} done \u0026lt;${subjects}  Here\u0026rsquo;s an example output from running qstat after submitting a batch of jobs to the cluster:\n  Example of qstat command, after submitting jobs view qsub.   One thing you\u0026rsquo;ll notice is the column priority \u0026ndash; this is literally a priority queue data structure that I mentioned in my last post on the Watershed by Flooding algorithm. Each job is submitted to the queue with a priority value assigned to it by the SGE software, and the jobs are processed in that order \u0026ndash; highest priority first, lowest priority last. Your IT manager can personalize the priority values for specific users or types of jobs, such that they are given preference or moved back in line. This represents an equitable way of distributing compute resources across users in a lab, generally using a first-come, first-serve basis, or restricting users to a certain number of nodes.\n","date":1588713857,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588800257,"objectID":"79292e82c375de9d489e03e8fca978c1","permalink":"/post/submitting-batch-jobs-with-qsub/","publishdate":"2020-05-05T14:24:17-07:00","relpermalink":"/post/submitting-batch-jobs-with-qsub/","section":"post","summary":"I\u0026rsquo;m fortunate enought to work in a lab with some high-level computing infrastructure. We have a cluster of machines using the Sun Grid Engine (SGE) software system for distributed resource management. The other day, I was searching for how to wrap my Python scripts with qsub so that I could submit a batch of jobs to our cluster.","tags":[],"title":"Submitting Batch Jobs with qsub","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m applying some methods developed in this paper for testing purposes in my own thesis research. Specifically, I have some float-valued data, $F$, that varies along the cortical surface of the brain. Visually, I can see that there are areas where these scalar maps change abruplty. I want to identify this boundary(s) \u0026ndash; eventually, I\u0026rsquo;ll segment out the regions I\u0026rsquo;m interested in.\nComputing the Gradient Map The authors use some conventional brain imaging software to compute the gradient of their data. The domain of this data is a triangulated mesh, described by the graph $G = (V, E)$, where $V$ are vertices in Euclidean space and $E$ the edges between these vertices. In short, for a vertex, $v_{i}$, we first need to compute the gradient vector of the scalar field at $v_{i}$. We \u0026ldquo;unfold\u0026rdquo; the 3D positions of adjacent vertices onto the tangent plane of $v_{i}$ \u0026ndash; which we can do by orthogonally projecting the adjacent vertices onto the affine subspace at $v_{i}$ and then weighting appropriately. We then regress the graph signal (our scalar field) onto these unfolded positions. The $L_{2}$ norm of this vector is the gradient at $v_{i}$.\nFor each vertex, we have a normal vector to the surface $N$, its spatial 3D coordinates $v_{i} = (x, y, z)$, and a list of its adjacent vertices. We can compute the orthogonal projector onto affine subspace spanned by $N$, $P_{N}$, and it\u0026rsquo;s orthogonal complement, $Q_{N}$, as:\n$$\\begin{align} P_{N} \u0026amp;= N(N^{T}N)^{-1}N^{T} \\\\\nQ_{N} \u0026amp;= I - P_{N} \\end{align}$$\nFor any vertex, $v_{j}$, we can compute the orthogonal projection onto the affine subspace spanned by $Q_{N}$ as:\n$$\\begin{align} q(v_{j}) = Q_{N}(v_{j} - v_{i}) + v_{i} \\end{align}$$\nWe generate the vectors\n$$ S_{i} = f(i) - \\begin{bmatrix} f(v_{1}) \\\\\nf(v_{2}) \\\\\n\\vdots \\\\\nf(v_{j}) \\end{bmatrix} \\in \\mathbb{R}^{j} \\;\\;\\; R_{i} = \\begin{bmatrix} q(v_{1}) \\\\\nq(v_{2}) \\\\\n\\vdots \\\\\nq(v_{j}) \\end{bmatrix} \\in \\mathbb{R}^{j \\times 3} $$\nwhere $S_{i}$ is the difference between the scalar value at our vertex $f(v_{i})$ and the vector of adjacent $j$ scalar field values, and $R_{i}$ is the matrix of $j$ orthogonally projected adjacent vertex coordinates. Then we perform least squares regression to solve for $\\beta$:\n$$\\begin{align} S_{i} = R_{i}\\beta \\end{align}$$\nwhere $\\beta \\in \\mathbb{R}^{3}$, which indicates how much each coordinate axis contributes to variation in the scalar field at $v_{i}$. The gradient value at vertex $v_{i} = \\left || \\beta \\right||_{2}$.\nWatershed By Flooding Algorithm The new scalar field of $L_{2}$ norms is our gradient field, which describes how \u0026ldquo;quickly\u0026rdquo; our original data changes at each vertex. We can now apply the Watershed Algorithm to segment our mesh data. In brief, the watershed algorithm treats the gradient field as a *topographic map*: low-elevation areas (areas with a small gradient) are \u0026ldquo;water basins\u0026rdquo;. If we imagine water flooding this map from the bottom up, basins at low elevation will flood first, while areas at higher elevations will fill last. When water basins meet, the water has reached a \u0026ldquo;boundary\u0026rdquo; (or ridgeline, if we\u0026rsquo;re using the topographic map idea).\nI\u0026rsquo;ve implemented an algorithm variant called \u0026ldquo;Priority Flooding\u0026rdquo;, using Python\u0026rsquo;s heapq priority queue data type class. The priority queue is an application of the binary heap data structure \u0026ndash; as nodes are added to the heap, the branching process determines where to put nodes (left or right of a current node), based on some value \u0026ndash; in the case of the priority queue, this value is the \u0026ldquo;priority\u0026rdquo;. We utilize the priority queue because it gives us a principled way to iterate over unlabeled vertices, and, with some auxilliary data structures, is guaranteed to converge. The algorithm proceeds as follows:\n  Identify local minima, and assign each minima a unique label\n  Add directly adjacent vertices of local minima to priority queue (lower gradient -\u0026gt; higher priority)\n  While the queue is not empty, get the highest-priority item\n  If vertices adjacent to this vertex have only one label, assign this vertex to that label\n  Else assign this vertex as a boundary vertex\n  Add unlabed adjacent vertices to the queue\n    import numpy as np from queue import PriorityQueue class PriorityFlood(object): \u0026quot;\u0026quot;\u0026quot; Class to segment a scalar field using the Priority Flooding watershed algorithm. \u0026quot;\u0026quot;\u0026quot; def __init__(self): self.pq = PriorityQueue() def fit(self, gradient, A, M): \u0026quot;\u0026quot;\u0026quot; Fitting procedure for watershed algorithm. Parameters: - - - - - gradient: float, array map of gradient values of scalar field A: dict of lists adjacency list of data domain M: list local minima in scalar field used to seed algorithm \u0026quot;\u0026quot;\u0026quot; # initialize empty label vector labels = np.zeros((gradient.shape)) labels[:] = np.nan # keep track of items in queue in_queue = np.zeros((gradient.shape)).astype(np.bool) # assign local minima unique labels # add their adjacent vertices to heap for i, loc_min in enumerate(M): # label local minima labels[loc_min] = i+1 # add neighbors of local minima to p-queue for nidx in A[loc_min]: in_queue[nidx] = True self.pq.put((gradient[nidx], nidx)) # iterate over p-queue items # items assigned to a label or # assigned as a boundary while not self.pq.empty(): # get highest priority item # mark as not in p-queue [gr, idx] = self.pq.get() in_queue[idx] = False # get item neighbors and their labels i_neighbors = np.asarray(A[idx]) i_nlabels = labels[i_neighbors] # get labels of adjacent vertices that are not nan nans = np.isnan(i_nlabels) unique_labels = np.unique(i_nlabels[~nans]) # if more than one unique label # assign current vertex as border vertex if len(unique_labels) \u0026gt; 1: continue # otherwise assign to water basin else: # basin assignment labels[idx] = unique_labels # identify neighbors without labels that arent in the p-queue gidx = np.where((nans) \u0026amp; (~in_queue[i_neighbors]))[0] # add these to p-queue for nidx in i_neighbors[gidx]: if not in_queue[nidx]: in_queue[nidx] = True self.pq.put((gradient[nidx], nidx)) self.labels_ = labels    Example of PriorityFlooding algorithm, applied to gradient of inferiorparietal region. Shown on inflated and flattened cortical surfaces.   One caveat that came up is that the PriorityQueue class does not check for duplicates \u0026ndash; that is, two vertices might share an adjacent vertex, and this vertex might have already been added to the heap. In this case, we would unnecessarily view the same vertices many times. To alleviate this, we create a boolean Numpy array, in_queue, that stores whether an item is already in the queue.\n","date":1588356752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588443152,"objectID":"a52f51dff7d31ebb6bccc612ab4c0139","permalink":"/post/watershed-by-flooding-applied-data-structures/","publishdate":"2020-05-01T11:12:32-07:00","relpermalink":"/post/watershed-by-flooding-applied-data-structures/","section":"post","summary":"I\u0026rsquo;m applying some methods developed in this paper for testing purposes in my own thesis research. Specifically, I have some float-valued data, $F$, that varies along the cortical surface of the brain. Visually, I can see that there are areas where these scalar maps change abruplty.","tags":[],"title":"Watershed by Flooding: Applied Data Structures","type":"post"},{"authors":["James M. Kunert-Graf","Kristian M. Eschenburg","David J. Galas","J. Nathan Kutz","Swati D. Rane","Bingni W. Brunton"],"categories":null,"content":"","date":1572480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572480000,"objectID":"2ffcec9e411aab6a69992aadd4195dd7","permalink":"/publication/dmd/dmd/","publishdate":"2019-10-31T00:00:00Z","relpermalink":"/publication/dmd/dmd/","section":"publication","summary":"In this paper, we develop a novel method based on dynamic mode decomposition (DMD) to extract resting-state networks from short windows of noisy, high-dimensional fMRI data, allowing RSNs from single scans to be resolved robustly at a temporal resolution of seconds.  This automated DMD-based method is a powerful tool to characterize spatial and temporal structures of RSNs in individual subjects.","tags":["dynamic networks","modal decomposition","fMRI","independent component analysis","Human Connectome Project"],"title":"Extracting Reproducible Time-Resolved Resting State Networks Using Dynamic Mode Decomposition","type":"publication"},{"authors":[],"categories":[],"content":"The other day, one of my friends and colleagues (I\u0026rsquo;ll refer to him as \u0026ldquo;Dr. A\u0026rdquo;) asked me if I knew anything about assessing biomarker diagnostic power. He went on to describe his clinical problem, which I\u0026rsquo;ll try to recant here (but will likely mess up some of the relevant detail \u0026ndash; his research pertains to generating induced pluripotent cardiac stem cells, which I have little to no experience with):\n \u0026ldquo;So chemotherapy is meant to combat cancer. But some clinicians, and their patients, have found that some forms of chemotherapy and anti-cancer drugs later result in problems with the heart and vasculature \u0026ndash; these problems are collectively referred to as \u0026lsquo;cardiotoxicity\u0026rsquo;.\n  We\u0026rsquo;re interested in developing biomarkers that will help us identify which patients might be susceptible to cardiotoxicity, and in assessing the predictive power of these biomarkers. Can you help me?\u0026rdquo;\n What follows will be my exploration into what Dr. A called dose-response curves, and my approach on how to use these curves to assess biomarker diagnostic power. I\u0026rsquo;ll do a walk through of some Python code that I\u0026rsquo;ve written up, where I\u0026rsquo;ll examine dose-response curves and their diagnostic power using the receiver operating characteristic ( ROC) and the closely related area under the curve ( AUC) metrics.\nIf you want to see the actual dose-response curve analysis, skip to the Analyzing Synthetic Dose-Response Curves section down below.\nBrief Introduction to the ROC and AUC I first wanted to describe to Dr. A how to use the ROC for biomarkers, so I made a brief Python tutorial for him to look at. Below begins a more in-depth description of what I sent him.\nWe start by importing some necessary libraries:\n%matplotlib inline import matplotlib.pyplot as plt from matplotlib.lines import Line2D import numpy as np from sklearn.metrics import auc  Next, we define a function to compute the ROC curve. While scikit-learn has a function to compute the ROC, computing it yourself makes it easier to understand what the ROC curve actually represents.\nThe ROC curve is essentially a curve that plots the true positive and false positive classification rates against one another, for various user-defined thresholds of the data. We pick a biomarker level threshold \u0026ndash; let\u0026rsquo;s say $T$ \u0026ndash; assign each sample a value of 0 or 1 depending on whether its measured biomarker level is greater than or less than $T$, and then compare these assignments to the true Case/Control classifications to compute the true and false positive rates. We plots these rates for many $T$ to generate the ROC curve.\nThe ROC let\u0026rsquo;s you look at how sensitive your model is to various parameterizations \u0026ndash; are you able to accurately identify Cases from Controls? How acceptable are misclassified results? In this situation, we don\u0026rsquo;t want to classify someone as healthy when in fact they might develop cardiotoxicity, so we want to allow some flexibility in terms of the number of false positives generated by our model \u0026ndash; it\u0026rsquo;s a classic case of \u0026ldquo;better safe than sorry\u0026rdquo;, since the \u0026ldquo;sorry\u0026rdquo; outcome might be an accidental patient death.\ndef roc(case, control, npoints, gte=True): \u0026quot;\u0026quot;\u0026quot; Compute ROC curve for given set of Case/Control samples. Parameters: - - - - - case: float, array Samples from case patients control: float, array Samples from control patients npoints: int Number of TP/FP pairs to generate gte: boolean Whether control mean expected to be greater than case mean. Returns: - - - - specificity: float, array false positive rates sensitivity: float, array true positive rates \u0026quot;\u0026quot;\u0026quot; # make sure generating more # than 1 TP/FP pair # so we can plot an actual ROC curve assert npoints \u0026gt; 1 # made sure case and control samples # are numpy arrays case = np.asarray(case) control = np.asarray(control) # check for NaN values # keep only indices without NaN case_nans = np.isnan(case) cont_nans = np.isnan(control) nans = (case_nans + cont_nans) specificity = [] sensitivity = [] # we'll define the min and max thresholds # based on the min and max of our data conc = np.concatenate([case[~nans],control[~nans]]) # function comparison map # use ```gte``` parameter # if we expect controls to be less than # cases, gte = False # otherwise gte = True comp_map = {'False': np.less, 'True': np.greater} # generate npoints equally spaced threshold values # compute the false positive / true positive rates # at each threshold for thresh in np.linspace(conc.min(), conc.max(), npoints): fp = (comp_map[gte](case[~nans], thresh)).mean() tn = 1-fp tp = (comp_map[gte](control[~nans], thresh)).mean() fn = 1-tp specificity.append(tn) sensitivity.append(tp) return [np.asarray(specificity), np.asarray(sensitivity)]  Next up, I generate 5 different datasets. Each dataset corresponds to fake samples from a \u0026ldquo;Control\u0026rdquo; distribution, and a \u0026ldquo;Case\u0026rdquo; distribution \u0026ndash; each is distributed according to a univariate Normal distribution. The Control distribution remains the same in each scenario: $N(\\mu = 10, \\sigma = 1)$, but I change the $\\mu_{Case}$ parameter of the Case distribution in each instance, such that $\\mu_{Case} \\in [5,6,7,8,9,10]$.\nWe plot the example datasets as follows, and then compute the ROC curves for each dataset.\n# define unique means m1 = np.arange(5,11) samples = {}.fromkeys(m1) # define control distribution (this stays the same across n2 = np.random.normal(loc=10, scale=1, size=2000) fig, (ax) = plt.subplots(2, 3, figsize=(15, 6)) for i, ax in enumerate(fig.axes): n1 = np.random.normal(loc=m1[i], scale=1, size=2000) samples[m1[i]] = n1 ax.hist(n1, 25, alpha=0.5, label='Case', density=True,) ax.hist(n2, 25, alpha=0.5, label='Control', density=True,) ax.set_title('Mean: {:}, Sigma: 1'.format(m1[i]), fontsize=15) ax.set_xlabel('Biomarker Measures Variable', fontsize=15) ax.set_ylabel('Density', fontsize=15) ax.legend() plt.tight_layout()    Example Case/Control datasets.   fig, (ax1) = plt.subplots(1, 1) for mean, case_data in samples.items(): [spec, sens] = roc(case_data, n2, 100, gte=True) A = auc(spec, sens) L = 'Mean: %i, AUC: %.3f' % (mean, A) plt.plot(1-np.asarray(spec), np.asarray(sens), label=L) plt.legend(bbox_to_anchor=(1.04,1), fontsize=15) plt.xlabel('1-Specificity', fontsize=15) plt.ylabel('Sensitivity', fontsize=15) plt.title('ROC for Varying Case Distributions', fontsize=15)    ROC curves for example datasets.   We see that, as the distributions become less separated, the ability to distinguish points from either distribution is diminished. This is shown by 1) a flattening of the ROC curve towards the diagonal, along with 2) the integration of the ROC curve, which generates the AUC metric. When the distributions are far apart (as when the $\\mu_{Case} = 5$), it is quite easy for a simple model to distinguish points sampled from either distribution, meaning this hypothetical model has good diagnostic power.\nAnalyzing Synthetic Dose-Response Curves We now analyze some synthetic data, generated to look like dose-response curves. As a refresher, dose-response curves measure the behavior of some tissue cells in response to increasing levels of (generally) drugs. The $x$-axis is the drug dose, measured in some concentration or volume, and the $y$-axis is generally some measure of cell death or survival, generally ranging from 0% to 100%. The curves, however, look sigmoidal. So let\u0026rsquo;s first generate a function to create sigmoid curves.\ndef sigmoid(beta, intercept, x): \u0026quot;\u0026quot;\u0026quot; Fake sigmoid function, takes in coefficient, shift, and dose values. Parameters: - - - - - beta: float slope intercept: float negative exponential intercept x: float, array data samples Returns: - - - - dose_response: float, array single-subject dose-response vector Between 0 and 1. \u0026quot;\u0026quot;\u0026quot; dose_resonse = 1 / (1 + np.exp(-beta*x + intercept)) return dose_resonse  Next, let\u0026rsquo;s actually generate some synethetic data for a dataset of fake subjects. I want to incorporate some variability into the Cases and Controls, so I\u0026rsquo;ll sample the subject parameters from distributions. In this case, for each subject, I\u0026rsquo;ll sample the logistic curve slope coefficient from $Beta$ distributions, and the intercept from $Normal$ distributions. We\u0026rsquo;ll sample 1000 Cases, and 1000 Controls.\nFor the slopes, we have\n$$ \\begin{align} \\beta_{Control} \u0026amp;\\sim Beta(a=5, b=3) \\\n\\beta_{Case} \u0026amp;\\sim Beta(a=10, b=2) \\end{align} $$\nand for the intercepts, we have\n$$ \\begin{align} I_{Control} \u0026amp;\\sim Normal(\\mu=0, \\sigma=1) \\\nI_{Case} \u0026amp;\\sim Normal(\\mu=4, \\sigma=1) \\end{align} $$\nAs such, the dose-response curve for individual, $k$, is generated as follows:\n$$ dr_{k} = \\frac{1}{1 + e^{-(\\beta_{k}X + I_{k})}} $$\nwhere $\\beta_{k}$ and $I_{k}$ are the slope and intercept values for the given subject. These distributional parameterizations are arbitrary \u0026ndash; I just wanted to be able to incorporate variability across subjects and groups.\nLet\u0026rsquo;s generate some random Case/Control dose-response data and plot the coefficient histograms:\n# n cases and controls S = 1000 # dictionary of slopes and intercept values for each subject controls = {k: {'beta': None, 'intercept': None} for k in np.arange(S)} cases = {k: {'beta': None, 'intercept': None} for k in np.arange(S)} # get lists of betas and intercepts beta_control = [] beta_case = [] intercept_control = [] intercept_case = [] for i in np.arange(S): controls[i]['beta'] = np.random.beta(a=5, b=3) controls[i]['intercept'] = np.random.normal(loc=0, scale=1) intercept_control.append(controls[i]['intercept']) beta_control.append(controls[i]['beta']) cases[i]['beta'] = np.random.beta(a=10, b=2) cases[i]['intercept'] = np.random.normal(loc=4, scale=1) intercept_case.append(cases[i]['intercept']) beta_case.append(cases[i]['beta'])  Intercept histograms look like two different $$Normal$$ distributions:\nfig = plt.figure(figsize=(12, 8)) plt.hist(intercept_control, 50, color='r', alpha=0.5, density=True, label='Control'); plt.hist(intercept_case, 50, color='b', alpha=0.5, density=True, label='Case'); plt.ylabel('Density', fontsize=15); plt.xlabel('Intercepts', fontsize=15); plt.title('Intercepts Coefficients By Group', fontsize=15); plt.legend(fontsize=15); plt.tight_layout()    Intercept distributions for synthetic dose-response curves.   Slope histograms look like two different $Beta$ distributions:\nfig = plt.figure(figsize=(12, 8)) plt.hist(beta_control, 50, color='r', alpha=0.5, label='Control', density=True); plt.hist(beta_case, 50, color='b', alpha=0.5, label='Case', density=True); plt.ylabel('Density', fontsize=15); plt.xlabel('Betas', fontsize=15); plt.title('Slope Coefficients By Group', fontsize=15); plt.legend(fontsize=15); plt.tight_layout()    Slope distributions for synthetic dose-response curves.   Now we\u0026rsquo;ll generate some fake dose-response curves for each of the 1000 Controls, and 1000 Cases. We\u0026rsquo;ll plot a subset of these curves to visualize our cross-group curve variability.\n# define synthetic dose range doses = np.linspace(-15,15,500) dose_min = doses.min() shifted_dose = doses + np.abs(dose_min) fig, (ax1) = plt.subplots(figsize=(12, 8)) ec50_control = [] ec50_case = [] for c in np.arange(S): control_sample = sigmoid(controls[c]['beta'], controls[c]['intercept'], doses) case_sample = sigmoid(cases[c]['beta'], cases[c]['intercept'], doses) ec50_control.append(shifted_dose[control_sample \u0026lt; 0.5].max()) ec50_case.append(shifted_dose[case_sample \u0026lt; 0.5].max()) if (c % 15) == 0: ax1.plot(shifted_dose, control_sample, c='r', linewidth=3, alpha=0.3) ax1.plot(shifted_dose, case_sample, c='b', linewidth=3, alpha=0.3) plt.legend({}) plt.title('Dose Response Curve', fontsize=20); plt.xlabel('Dose', fontsize=20); plt.xticks(fontsize=15) plt.ylabel('Response', fontsize=20); plt.yticks(fontsize=15) custom_lines = [Line2D([0], [0], color='r', lw=4), Line2D([0], [0], color='b', lw=4)] plt.legend(custom_lines, ['Control', 'Case'], fontsize=20);    Case/Control dose-response curves.   For our preliminary biomarker of interest, let\u0026rsquo;s look at the ec50, which is the dose at which 50% of the cells show some response (i.e. where our $y$-axis = 0.5), for each sample in our dataset. We\u0026rsquo;ll plot these doses as a function of Cases and Controls.\nfig = plt.figure(figsize=(12, 8)) plt.hist(ec50_control, 50, color='r', alpha=0.5, label='Control', density=True); plt.hist(ec50_case, 50, color='b', alpha=0.5, label='Case', density=True); plt.legend(fontsize=20); plt.xlabel('ec50', fontsize=20); plt.xticks(fontsize=15); plt.ylabel('Density', fontsize=20); plt.yticks(fontsize=15);    Biomarker distributions: ec50 for Cases and Controls.   If we select a different threshold \u0026ndash; i.e. instead of 0.5, we can iterate over the range of 0.1 - 0.9, for example, in increments of 0.1 \u0026ndash; we generate different biomarkers (ec10, ec20 \u0026hellip; ec90). We can treat each biomarker as a different classification model, and assess how powerful that model is at assessing whether someone will develop cardiotoxicity or not. To do so, we\u0026rsquo;ll create distributions for each biomarker (not shown), and then generate ROC curves and AUC values for each curve.\n  ROC curves and AUC for each ec-X biomarker level.   This is where my limited domain knowledge comes at a cost \u0026ndash; I\u0026rsquo;m not sure if the biomarkers I\u0026rsquo;ve chosen (i.e. incremental ec values) are actually biologically relevant. The point, however, is that each biomarker yields a different AUC, which theoretically shows that the Cases and Controls can be differentially distinguished, depending on which biomarker we choose to examine. In this case, ec10 has the most discriminative diagnostic power.\nSomething I did wonder about while exploring this data was how dependent the ROC curves and AUC statistics are on sample size. Previously, I\u0026rsquo;d looked at rates of convergence of various estimators \u0026ndash; the AUC should also theoretically show some convergence to a \u0026ldquo;true\u0026rdquo; value as $n$ increases \u0026ndash; but I\u0026rsquo;m not sure if it follows any sort of relevant distribution. I imagine the AUC is domain-dependent, in that it depends on the distribution of the biomarker of interest? Might be a good idea for another post\u0026hellip;\nCheers.\n","date":1566929552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567015952,"objectID":"63f39830bb962df98242b00deeefbeee","permalink":"/post/dose-response-curves-and-biomarker-diagnostic-power/","publishdate":"2019-08-27T11:12:32-07:00","relpermalink":"/post/dose-response-curves-and-biomarker-diagnostic-power/","section":"post","summary":"The other day, one of my friends and colleagues (I\u0026rsquo;ll refer to him as \u0026ldquo;Dr. A\u0026rdquo;) asked me if I knew anything about assessing biomarker diagnostic power. He went on to describe his clinical problem, which I\u0026rsquo;ll try to recant here (but will likely mess up some of the relevant detail \u0026ndash; his research pertains to generating induced pluripotent cardiac stem cells, which I have little to no experience with):","tags":[],"title":"Dose-Response Curves and Biomarker Diagnostic Power","type":"post"},{"authors":[],"categories":[],"content":"I wanted to make a quick note about something I found incredibly helpful the other day.\nLists (or ArrayLists, as new Computer Science students are often taught in their CS 101 courses), as a data strucure are fundamentally based on arrays, but with additional methods associated with them. Lists are generally filled with an append method, that fills indices in this array sequentially. Lists are often useful in the case where the number of intial spots that will be filled is unknown, or if you\u0026rsquo;re working with many objects of different types.\nThe base arrays are generally associated with a size or length parameter, that initializes the array to a certain length. Under the hood (and generally hidden from the user), however, the List class also has a resize method that adds available space to the array when a certain percentage of available indices are occupied, technically allowing the size of the list to grow when more space is needed.\nPerpetually applying resize becomes slow in the case when you\u0026rsquo;re appending a lot of items. All of the data currently in the List object will need to be moved into the new, resized array.\nI needed to aggregate a large number (couple thousand) of Pandas DataFrame objects, each saved as a single file, into a single DataFrame. My first thought was to simply incrementally load and append all incoming DataFrames to a list, and then use pandas.concat to aggregate them all together. Appending all of these DataFrames together became pretty time consuming (at this point, I remembered the resize issue).\nA quick Google search led me to the following solution, allowing me to predefine how large I wanted my list to be:\n# For simplicity assume we have 10 items known_size = 10 initialized_list = [None]*known_size print(len(initialized_list)) 10  Neat, huh? And ridiculously simple. Now, rather than append, we can do the following:\nfor j, temp_file in enumerate(list_of_files): loaded_file = load_file(temp_file) initialized_list[j] = loaded_file  Because the memory has already been pre-allocated, the resize method is never accessed, and we save time. I also found this blog post with some information about timing with regards to Numpy arrays, lists, and tuples \u0026ndash; the author shows that indexing into a Numpy array is actually slower than indexing into a list. Numpy arrays are primarilly useful in the case where operations can be vectorized \u0026ndash; then they\u0026rsquo;re the clear winners in terms of speed.\n","date":1566375152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566461552,"objectID":"523040c7cddf13aebb341b119e025bc6","permalink":"/post/initializing-lists-in-python-with-prespecified-size/","publishdate":"2019-08-21T01:12:32-07:00","relpermalink":"/post/initializing-lists-in-python-with-prespecified-size/","section":"post","summary":"I wanted to make a quick note about something I found incredibly helpful the other day.\nLists (or ArrayLists, as new Computer Science students are often taught in their CS 101 courses), as a data strucure are fundamentally based on arrays, but with additional methods associated with them.","tags":[],"title":"Initializing Lists in Python With Prespecified Size","type":"post"},{"authors":null,"categories":null,"content":"University of Washington, Department of Radiology. Research and Education Day. Seattle, Washington. 2019.  ","date":1553904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553904000,"objectID":"f353113ad83249004d8557b31284c707","permalink":"/talk/radiology/","publishdate":"2019-03-30T00:00:00Z","relpermalink":"/talk/radiology/","section":"talk","summary":"University of Washington, Department of Radiology. Research and Education Day. Seattle, Washington. 2019.  ","tags":null,"title":"","type":"talk"},{"authors":[],"categories":[],"content":"Here, we\u0026rsquo;ll look at various applications of the Delta Method, especially in the context of variance stabilizing transformations, along with looking at the confidence intervals of estimates.\nThe Delta Method is used as a way to approximate the Standard Error of transformations of random variables, and is based on a Taylor Series approximation.\nIn the univariate case, if we have a random variable, $X_{n}$, that converges in distribution to a $N(0, \\sigma^{2})$ distribution, we can apply a function to this random variable as:\n$$\\begin{align} \\sqrt{n}(X_{n} - \\theta) \\xrightarrow{d} N(0,\\sigma^{2}) \\\\\n\\sqrt{n}(g(X_{n}) - g(\\theta)) \\xrightarrow{d} ; ? \\end{align}$$\nHowever, we don\u0026rsquo;t know the asymptotic variance of this transformed variable just yet. In this case, we can approximate our function $g(x)$ using a Taylor Series approximation, evaluated at $\\theta$:\n$$\\begin{align} g(x) = g(\\theta) + g\\prime(\\theta)(x-\\theta) + O() \\end{align}$$\nwhere $O()$ is the remainder of higher-order Taylor Series terms that converges to 0.\nBy Slutsky\u0026rsquo;s Theorem and the Continious Mapping Theorem, we know that since $\\bar{\\theta} \\xrightarrow{p} \\theta$, we know that $g\\prime(\\bar{\\theta}) \\xrightarrow{p} g\\prime(\\theta)$\nPlugging this back in to our original equation and applying Slutsky\u0026rsquo;s Perturbation Theorem, we have:\n$$\\begin{align} \u0026amp;= \\sqrt{n}(\\Big[g(\\theta) + g\\prime(\\theta)(x-\\theta)\\Big] - g(\\theta)) \\\\\n\u0026amp;= \\sqrt{n}(g\\prime(\\theta)(x-\\theta)) \\\\\n\u0026amp;= g\\prime(\\theta)\\sqrt{n}(X_{n} - \\theta) \\end{align}$$\nand since we know that $\\sqrt{n}(\\bar{X_{n}} - \\theta) \\xrightarrow{d} N(0,\\sigma^{2})$, we now know that $g\\prime(\\theta) \\sqrt{n}(\\bar{X_{n}} - \\theta) \\xrightarrow{d} N(0,g\\prime(\\theta)^{2} \\sigma^{2})$. As such, we have that:\n$$\\begin{align} \\sqrt{n}(g(X_{n}) - g(\\theta)) \\xrightarrow{d} N(0, g\\prime(\\theta)^{2}\\sigma^{2}) \\end{align}$$\nThe Delta Method can be generalized to the multivariate case, where, instead of the derivative, we use the gradient vector of our function:\n$$\\begin{align} \\sqrt{n}(g(\\bar{X_{n}} - g(\\theta)) \\xrightarrow{d} N(0, \\nabla(g)^{T} \\Sigma \\nabla(g)) \\end{align}$$\nBelow, I\u0026rsquo;m going to look at a few examples applying the Delta Method to simple functions of random variables. Then I\u0026rsquo;ll go into more involved examples applying the Delta Method via Variance Stabilizing Transformations. Oftentimes, the variance of an estimate depends on its mean, which can vary with the sample size. In this case, we\u0026rsquo;d like to find a function $g(\\theta)$, such that, when applied via the Delta Method, the variance is constant as a function of the sample size.\nWe\u0026rsquo;ll start by importing the necessary libraries and defining two functions:\n%matplotlib inline import matplotlib.pyplot as plt from matplotlib import rc rc('text', usetex=True) from scipy.stats import norm, poisson, expon import numpy as np  Here, we define two simple functions \u0026ndash; one to compute the difference between our estimate and its population paramter, and the other to compute the function of our random variable as described by the Central Limit Theorem.\ndef conv_prob(n, est, pop): \u0026quot;\u0026quot;\u0026quot; Method to compute the estimate for convergence in probability. \u0026quot;\u0026quot;\u0026quot; return (est-pop) def clt(n, est, pop): \u0026quot;\u0026quot;\u0026quot; Method to examine the Central Limit Theorem. \u0026quot;\u0026quot;\u0026quot; return np.sqrt(n)*(est-pop)  Let\u0026rsquo;s have a look at an easy example with the Normal Distribution. We\u0026rsquo;ll set $\\mu = 0$ and $\\sigma^{2} = 5$. Remember that when using the Scipy Normal distribution, the norm class accepts the standard deviation, not the variance. We\u0026rsquo;ll show via the Central Limit Theorem that the function $\\sqrt{n}(\\bar{X_{n}} - \\mu) \\xrightarrow{d} N(0,\\sigma^{2})$.\n# set sample sample sizes, and number of sampling iterations N = [5,10,50,100,500,1000] iters = 500 mu = 0; sigma = np.sqrt(5) # store estimates norm_clt = {n: [] for n in N} samples = norm(mu,sigma).rvs(size=(iters,1000)) for n in N: for i in np.arange(iters): est_norm = np.mean(samples[i,0:n]) norm_clt[n].append(clt(n, est_norm, mu))  Now let\u0026rsquo;s plot the results.\n# Plot results using violin plots fig = plt.subplots(figsize=(8,5)) for i,n in enumerate(N): temp = norm_clt[n] m = np.mean(temp) v = np.var(temp) print('Sample Size: %i has empirical variance: %.2f' % (n, v.mean())) plt.violinplot(norm_clt[n], positions=[i],)    Central Limit Theorem applied to Normal Distribution.   As expected, we see that the Normal distribution mean and variance estimates are independent of the sample size. In this case, we don\u0026rsquo;t need to apply a variance stabiliing transformation. We also see that the variance fluctuates around $5$. Now, let\u0026rsquo;s apply a simple function $g(\\theta) = \\theta^{2}$ to our data. So $g\\prime(\\theta) = 2\\theta$, and the variance of our function becomes $g\\prime(\\mu)^{2}\\sigma^{2} = (2\\mu)^{2} \\sigma^{2} = 4\\mu^{2}\\sigma^{2}$. Let\u0026rsquo;s look at a few plots, as a function of changing $\\mu$.\n# set sample sample sizes, and number of sampling iterations mus = [1,2,3,4] N = [5,10,50,100,500,1000] iters = 2000 sigma = np.sqrt(5) fig, ([ax1,ax2], [ax3,ax4]) = plt.subplots(2,2, figsize=(14,9)) for j ,m in enumerate(mus): # store estimates norm_clt = {n: [] for n in N} samples = norm(m, sigma).rvs(size=(iters, 1000)) plt.subplot(2,2,j+1) for k, n in enumerate(N): np.random.shuffle(samples) for i in np.arange(iters): est_norm = np.mean(samples[i, 0:n]) norm_clt[n].append(clt(n, est_norm**2, m**2)) plt.violinplot(norm_clt[n], positions=[k],)    Central Limit Theorem applied to function of Normal Distribution.   We see that the variance increases as the mean increases, and that, as the sample sizes increase, the distributions converge to the $$N(0, 4\\mu^{2}\\sigma^{2})$$ asymptotic distribution.\nVariance Stabilization for the Poisson Distribution Now let\u0026rsquo;s look at an example where the variance depends on the sample size. We\u0026rsquo;ll use the Poisson distribution in this case. We know that for the Poisson distribution, the variance is dependent on the mean, so let\u0026rsquo;s define a random variable, $X_{\\lambda}$, where $\\lambda = n*\\theta$. $n$ is the sample size, and $\\theta$ is a fixed constant.\nWe\u0026rsquo;ll define $ X_{\\lambda } = \\sum_{i=1}^{n} X_{\\theta}$, the sum of $n$ independent Poisson random variables, so that the expected value and variance of $X_{\\lambda } = n\\theta$\nIf we wanted to apply the Central Limit Theorem to $X_{\\lambda }$, our convergence would be as follows:\n$$\\begin{align} \\sqrt{n}(X_{\\lambda} - \\lambda) \\xrightarrow{d} N(0,\\sigma^{2}(\\lambda)) \\end{align}$$\nwhere the variance $\\sigma^{2}(\\lambda)$ depends on the mean, $\\lambda$. In order to stabilize the variance of this variable, we can apply the Delta Method, in order to generate a variable that converges to a standard Normal distribution asymptotically.\n$$\\begin{align} \\sqrt{n}(g(X_{\\lambda}) - g(\\lambda)) \\xrightarrow{d} N(0,g\\prime(\\theta)^{2}\\sigma^{2}) \\\n\\end{align}$$\nwhere\n$$\\begin{align} \u0026amp;g\\prime(\\theta)^{2} \\theta = 1 \\\\\n\u0026amp;g\\prime(\\theta)^{2} = \\frac{1}{\\theta} \\\\\n\u0026amp;g\\prime(\\theta) = \\frac{1}{\\sqrt{\\theta}} \\\\\n\u0026amp;g(\\theta) = \\int \\frac{\\partial{\\theta}}{\\sqrt{\\theta}} \\\\\n\u0026amp;g(\\theta) = 2\\sqrt{\\theta} \\end{align}$$\nis our variance stabilizing function.\ndef p_lambda(n, theta=0.5): \u0026quot;\u0026quot;\u0026quot; Function to compute lambda parameter for Poisson distribution. Theta is constant. \u0026quot;\u0026quot;\u0026quot; return n*theta  theta = 0.5 N = [5,10,50,100,250,500,750,1000] iters = 500 clt_pois = {n: [] for n in N} pois_novar= {n: [] for n in N} pois_var = {n: [] for n in N} for n in N: for i in np.arange(iters): est_mu = np.mean(poisson(mu=(n*theta)).rvs(n)) pois_novar[n].append(clt(n, est_mu, p_lambda(n))) pois_var[n].append(clt(n, 2*np.sqrt(est_mu), 2*np.sqrt(p_lambda(n)))) clt_pois[n].append(conv_prob(n, est_mu, n*theta))  fig,([ax1, ax2]) = plt.subplots(2,1, figsize=(15,6)) plt.subplot(1,2,1) for i,n in enumerate(N): plt.violinplot(pois_novar[n], positions=[i]) plt.subplot(1,2,2) for i,n in enumerate(N): plt.violinplot(pois_var[n], positions=[i])    Variance stabilization of Poisson distribution.   Variance Stabilization for the Exponential Distribution Applying the same method to the Exponential distribtuion, we\u0026rsquo;ll find that the variance stabilizing transformation is $g(\\theta) = log(\\theta)$. We\u0026rsquo;ll apply that here:\ntheta = 0.5 N = [5,10,50,100,250,500,750,1000] iters = 500 clt_exp = {n: [] for n in N} exp_novar= {n: [] for n in N} exp_var = {n: [] for n in N} for n in N: for i in np.arange(iters): samps = expon(scale=n*theta).rvs(n) est_mu = np.mean(samps) est_var = np.var(samps) exp_novar[n].append(clt(n, est_mu, (n*theta))) exp_var[n].append(clt(n, np.log(est_mu), np.log(n*theta))) clt_exp[n].append(conv_prob(n, est_mu, n*theta))  fig,([ax1, ax2]) = plt.subplots(2,1, figsize=(15,6)) plt.subplot(1,2,1) for i,n in enumerate(N): plt.violinplot(exp_novar[n], positions=[i]) plt.subplot(1,2,2) for i,n in enumerate(N): plt.violinplot(exp_var[n], positions=[i])    Variance stabilization of Exponential distribution.   Example of Standard Error Computation Using Delta Method for Polynomial Regression As an example of applying the Delta Method to a real-world dataset, I\u0026rsquo;ve downloaded the banknote dataset from the UCI Machine Learning Repository. In this exercise, I\u0026rsquo;ll apply the logistic function via logistic regression to assess whether or not a banknote is real or fake, using a set of features. I\u0026rsquo;ll compute confidence intervals of our prediction probabilities using the Delta Method. There are four unique predictors in this case: the variance, skew, kurtosis, and entropy of the Wavelet-transformed banknote image. I\u0026rsquo;ll treat each of these predictors independently, using polynomial basis function of degree 3.\nIn this example, we\u0026rsquo;re interested in the standard error of our probability estimate. Our function is the Logistic Function, as follows:\n$$\\begin{align} g(\\beta) \u0026amp;= \\frac{1}{1+e^{-x^{T}\\beta}} \\\\\n\u0026amp;= \\frac{e^{x^{T}\\beta}}{1+e^{x^{T}\\beta}} \\end{align}$$\nwhere the gradient of this multivariate function is:\n$$\\begin{align} \\nabla g(\\beta) \u0026amp;= \\frac{\\partial g}{\\partial \\beta} e^{x^{T}\\beta}(1+e^{x^{T}\\beta})^{-1} \\\\\n\u0026amp;= x^{T}e^{x^{T}\\beta}(1+e^{x^{T}\\beta})^{-1} - x^{T}e^{x^{T}\\beta}e^{x^{T}\\beta} \\\\\n\u0026amp;= x^{T}\\Big(e^{x^{T}\\beta}(1+e^{x^{T}\\beta})^{-1} - e^{x^{T}\\beta}e^{x^{T}\\beta}\\Big)(1+e^{x^{T}\\beta})^{-2} \\\\\n\u0026amp;= x^{T} \\frac{e^{x^{T}\\beta}}{(1+e^{x^{T}\\beta})^{2}} \\\\\n\\nabla g(\\beta) \u0026amp;= x^{T} g(\\beta)(1-g(\\beta)) \\end{align}$$\nso that the final estimate of our confidence interval becomes\n$$\\begin{align} \u0026amp; \\sim N(0,x^{T} g(\\beta)(1-g(\\beta)) \\Sigma g(\\beta)(1-g(\\beta))x) \\\\\n\u0026amp; \\sim N(0, \\nabla g(\\beta)^{T} \\Sigma \\nabla g(\\beta)) \\end{align}$$\nimport pandas as pd import statsmodels.api as sm from sklearn.preprocessing import PolynomialFeatures  import pandas as pd import statsmodels.api as sm from sklearn.preprocessing import PolynomialFeatures bank = pd.read_csv('/Users/kristianeschenburg/Documents/Statistics/BankNote.txt', sep=',',header=None, names=['variance', 'skew', 'kurtosis', 'entropy','class']) bank.head() fig = plt.subplots(2,2, figsize=(12,8)) for j, measure in enumerate(['variance', 'kurtosis', 'skew', 'entropy']): predictor = np.asarray(bank[measure]) response = np.asarray(bank['class']) idx = (response == 1) # plot test set plt.subplot(2,2,j+1) plt.violinplot(predictor[idx], positions=[1]); plt.violinplot(predictor[~idx], positions=[0]) plt.title('{:} By Classification'.format(measure), fontsize=18) plt.ylabel('Measure: {:}'.format(measure),fontsize=15) plt.yticks(fontsize=13) plt.xticks([0,1],['Fake','Real'], fontsize=15) plt.tight_layout()    Bank note feature distributions, based on note class.   Based on the above plot, we can see that variance, skew, and kurtosis seem to be the most informative, while the entropy distributions do not seem to be that different based on bank note class.\nNext, we fit a logistic regression model of note classification on note feature, with polynomial order of degree 3. We then compute the standard errors of the transformed variance. It was transformed using the logistic function, so we\u0026rsquo;ll need to compute the gradient of this function.\nfig = plt.subplots(2,2, figsize=(12,8)) for j, measure in enumerate(['variance', 'kurtosis', 'skew', 'entropy']): # Generate polynomial object to degree # transform age to 4-degree basis function poly = PolynomialFeatures(degree=2) idx_order = np.argsort(bank[measure]) predictor = bank[measure][idx_order] response = bank['class'][idx_order] features = poly.fit_transform(predictor.values.reshape(-1,1)); # fit logit curve to curve logit = sm.Logit(response, features).fit(); test_features = np.linspace(np.min(predictor), np.max(predictor), 100) test_features = poly.fit_transform(test_features.reshape(-1,1)) # predict on test set class_prob = logit.predict(test_features) cov = logit.cov_params() yx = (class_prob*(1-class_prob))[:,None] * test_features se = np.sqrt(np.diag(np.dot(np.dot(yx, cov), yx.T))) # probability can't exceed 1, or be less than 0 upper = np.maximum(0, np.minimum(1, class_prob+1.96*se)) lower = np.maximum(0, np.minimum(1, class_prob-1.96*se)) # plot test set plt.subplot(2,2,j+1) plt.plot(test_features[:, 1], class_prob); plt.plot(test_features[:, 1], upper, color='red', linestyle='--', alpha=0.5); plt.plot(test_features[:, 1], lower, color='red', linestyle='--', alpha=0.5); plt.title(r'P(isReal \\Big| X)', fontsize=18) plt.xlabel('{:}'.format(measure),fontsize=15) plt.ylabel('Probability',fontsize=15) plt.grid(True) plt.tight_layout()    Confidence intervals for each feature, computed using Delta Method.   ","date":1553024612,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553111012,"objectID":"1a7d5a527dcff055cdbef7dc282fe87c","permalink":"/post/the-delta-method/","publishdate":"2019-03-19T12:43:32-07:00","relpermalink":"/post/the-delta-method/","section":"post","summary":"Here, we\u0026rsquo;ll look at various applications of the Delta Method, especially in the context of variance stabilizing transformations, along with looking at the confidence intervals of estimates.\nThe Delta Method is used as a way to approximate the Standard Error of transformations of random variables, and is based on a Taylor Series approximation.","tags":[],"title":"The Delta Method","type":"post"},{"authors":null,"categories":null,"content":"\u0026ldquo;Automated Connectivity-Based Parcellation With Registration-Constrained Classification\u0026rdquo;. Seattle, Washington. 2019. Honorable Mention for \u0026ldquo;Best Lightning Talk\u0026rdquo;\n","date":1552003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552003200,"objectID":"a3493271e33440fa005f2bc9712190a5","permalink":"/talk/biotech/","publishdate":"2019-03-08T00:00:00Z","relpermalink":"/talk/biotech/","section":"talk","summary":"\u0026ldquo;Automated Connectivity-Based Parcellation With Registration-Constrained Classification\u0026rdquo;. Seattle, Washington. 2019. Honorable Mention for \u0026ldquo;Best Lightning Talk\u0026rdquo;","tags":null,"title":"Northwest Data Science Summit","type":"talk"},{"authors":[],"categories":[],"content":"For one of the projects I\u0026rsquo;m working on, I have an array of multivariate data relating to brain connectivity patterns. Briefly, each brain is represented as a surface mesh, which we represent as a graph $G = (V,E)$, where $V$ is a set of $n$ vertices, and $E$ are the set of edges between vertices.\nAdditionally, for each vertex $v \\in V$, we also have an associated scalar label, which we\u0026rsquo;ll denote $l(v)$, that identifies what region of the cortex each vertex belongs to, the set of regions which we define as $L = {1, 2, \u0026hellip; k}$. And finally, for each vertex $v \\in V$, we also have a multivariate feature vector $r(v) \\in \\mathbb{R}^{1 \\times k}$, that describes the strength of connectivity between it, and every region $l \\in L$.\n  Example of cortical map, and array of connectivity features.   I\u0026rsquo;m interested in examining how \u0026ldquo;close\u0026rdquo; the connectivity samples of one region, $l_{j}$, are to another region, $l_{k}$. In the univariate case, one way to compare a scalar sample to a distribution is to use the $t$-statistic, which measures how many standard deviations away from the mean a given sample is:\n$$\\begin{align} t_{s} = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\end{align}$$\nwhere $\\mu$ is the population mean, and $s$ is the sample standard deviation. If we square this, we get:\n$$\\begin{align} t^{2} = \\frac{(\\bar{x} - \\mu)^{2}}{\\frac{s^{2}}{n}} = \\frac{n (\\bar{x} - \\mu)^{2}}{S^{2}} \\sim F(1,n) \\end{align}$$\nWe know the last part is true, because the numerator and denominator are independent $\\chi^{2}$ distributed random variables. However, I\u0026rsquo;m not working with univariate data \u0026ndash; I have multivariate data. The multivariate generalization of the $t$-statistic is the Mahalanobis Distance:\n$$\\begin{align} d \u0026amp;= \\sqrt{(\\bar{x} - \\mu)\\Sigma^{-1}(\\bar{x}-\\mu)^{T}} \\end{align}$$\nwhere the squared Mahalanobis Distance is:\n$$\\begin{align} d^{2} \u0026amp;= (\\bar{x} - \\mu)\\Sigma^{-1}(\\bar{x}-\\mu)^{T} \\end{align}$$\nwhere $\\Sigma^{-1}$ is the inverse covariance matrix. If our $X$'s were initially distributed with a multivariate normal distribution, $N_{p}(\\mu,\\Sigma)$ (assuming $\\Sigma$ is non-degenerate i.e. positive definite), the squared Mahalanobis distance, $d^{2}$ has a $\\chi^{2}_{p}$ distribution. We show this below.\nWe know that $(X-\\mu)$ is distributed $N_{p}(0,\\Sigma)$. We also know that, since $\\Sigma$ is symmetric and real, that we can compute the eigendecomposition of $\\Sigma$ as:\n$$\\begin{align} \\Sigma = U \\Lambda U^{T} \\end{align}$$\nand consequentially, because $U$ is an orthogonal matrix, and because $\\Lambda$ is diagonal, we know that $\\Sigma^{-1}$ is:\n$$\\begin{align} \\Sigma^{-1} \u0026amp;= (U \\Lambda U^{T})^{-1} \\\\\n\u0026amp;= U \\Lambda^{-1} U^{T} \\\\\n\u0026amp;= (U \\Lambda^{\\frac{-1}{2}}) (U \\Lambda^{\\frac{-1}{2}})^{T} \\\\\n\u0026amp;= R R^{T} \\end{align}$$\nTherefore, we know that $R^{T}(X-\\mu) \\sim N_{p}(0,I_{p})$:\n$$\\begin{align} X \u0026amp;\\sim N_{p}(\\mu,\\Sigma) \\\\\n(X-\\mu) = Y \u0026amp;\\sim N_{p}(0,\\Sigma)\\\\\nR^{T}Y = Z \u0026amp;\\sim N_{p}(0, R^{T} \\Sigma R) \\\\\n\u0026amp;\\sim N_{p}(0, \\Lambda^{\\frac{-1}{2}} U^{T} (U \\Lambda U^{T}) U \\Lambda^{\\frac{-1}{2}}) \\\\\n\u0026amp;\\sim N_{p}(0, \\Lambda^{\\frac{-1}{2}} I_{p} \\Lambda I_{p} \\Lambda^{\\frac{-1}{2}}) \\\\\n\u0026amp;\\sim N_{p}(0,I_{p}) \\end{align}$$\nso that we have\n$$\\begin{align} \u0026amp;= (X-\\mu)\\Sigma^{-1}(X-\\mu)^{T} \\\\\n\u0026amp;= (X-\\mu)RR^{T}(X-\\mu)^{T} \\\\\n\u0026amp;= Z^{T}Z \\end{align}$$\nthe sum of $p$ standard Normal random variables, which is the definition of a $\\chi_{p}^{2}$ distribution with $p$ degrees of freedom. So, given that we start with a $MVN$ random variable, the squared Mahalanobis distance is $\\chi^{2}_{p}$ distributed. Because the sample mean and sample covariance are consistent estimators of the population mean and population covariance parameters, we can use these estimates in our computation of the Mahalanobis distance.\nAlso, of particular importance is the fact that the Mahalanobis distance is not symmetric. That is to say, if we define the Mahalanobis distance as:\n$$\\begin{align} M(A, B) = \\sqrt{(A - \\mu(B))\\Sigma(B)^{-1}(A-\\mu(B))^{T}} \\end{align}$$\nthen $M(A,B) \\neq M(B,A)$, clearly. Because the parameter estimates are not guaranteed to be the same, it\u0026rsquo;s straightforward to see why this is the case.\nNow, back to the task at hand. For a specified target region, $l_{T}$, with a set of vertices, $V_{T} = {v \\; : \\; l(v) \\; = \\; l_{T}, \\; \\forall \\; v \\in V}$, each with their own distinct connectivity fingerprints, I want to explore which areas of the cortex have connectivity fingerprints that are different from or similar to $l_{T}$'s features, in distribution. I can do this by using the Mahalanobis Distance. And based on the analysis I showed above, we know that the data-generating process of these distances is related to the $\\chi_{p}^{2}$ distribution.\nFirst, I\u0026rsquo;ll estimate the covariance matrix, $\\Sigma_{T}$, of our target region, $l_{T}$, using the Ledoit-Wolf estimator (the shrunken covariance estimate has been shown to be a more reliable estimate of the population covariance), and mean connectivity fingerprint, $\\mu_{T}$. Then, I\u0026rsquo;ll compute $d^{2} = M^{2}(A,A)$ for every $\\{v: v \\in V_{T}\\}$. The empirical distribution of these distances should follow a $\\chi_{p}^{2}$ distribution. If we wanted to do hypothesis testing, we would use this distribution as our null distribution.\nNext, in order to assess whether this intra-regional similarity is actually informative, I\u0026rsquo;ll also compute the similarity of $l_{T}$ to every other region, $\\{ l_{k} \\; : \\; \\forall \\; k \\in L \\setminus \\{T\\} \\}$ \u0026ndash; that is, I\u0026rsquo;ll compute $M^{2}(A, B) \\; \\forall \\; B \\in L \\setminus T$. If the connectivity samples of our region of interest are as similar to one another as they are to other regions, then $d^{2}$ doesn\u0026rsquo;t really offer us any discriminating information \u0026ndash; I don\u0026rsquo;t expect this to be the case, but we need to verify this.\nThen, as a confirmation step to ensure that our empirical data actually follows the theoretical $\\chi_{p}^{2}$ distribution, I\u0026rsquo;ll compute the location and scale Maximumim Likelihood(MLE) parameter estimates of our $d^{2}$ distribution, keeping the *d.o.f.* (e.g. $p$) fixed.\nSee below for Python code and figures\u0026hellip;\nStep 1: Compute Parameter Estimates %matplotlib inline import matplotlib.pyplot as plt from matplotlib import rc rc('text', usetex=True) import numpy as np from scipy.spatial.distance import cdist from scipy.stats import chi2, probplot from sklearn import covariance  # lab_map is a dictionary, mapping label values to sample indices # our region of interest has a label of 8 LT = 8 # get indices for region LT, and rest of brain lt_indices = lab_map[LT] rb_indices = np.concatenate([lab_map[k] for k in lab_map.keys() if k != LT]) data_lt = conn[lt_indices, :] data_rb = conn[rb_indices, :] # fit covariance and precision matrices # Shrinkage factor = 0.2 cov_lt = covariance.ShrunkCovariance(assume_centered=False, shrinkage=0.2) cov_lt.fit(data_lt) P = cov_lt.precision_  Next, compute the Mahalanobis Distances:\n# LT to LT Mahalanobis Distance dist_lt = cdist(data_lt, data_lt.mean(0)[None,:], metric='mahalanobis', VI=P) dist_lt2 = dist_lt**2 # fit covariance estimate for every region in cortical map EVs = {l: covariance.ShrunkCovariance(assume_centered=False, shrinkage=0.2) for l in labels} for l in lab_map.keys(): EVs[l].fit(conn[lab_map[l],:]) # compute d^2 from LT to every cortical region # save distances in dictionary lt_to_brain = {}.fromkeys(labels) for l in lab_map.keys(): temp_data = conn[label_map[l], :] temp_mu = temp_data.mean(0)[None, :] temp_mh = cdist(data_lt, temp_mu, metric='mahalanobis', VI=EVs[l].precision_) temp_mh2 = temp_mh**2 lt_to_brain[l] = temp_mh2 # plot distributions seperate (scales differ) fig = plt.subplots(2,1, figsize=(12,12)) plt.subplot(2,1,1) plt.hist(lt_to_brain[LT], 50, density=True, color='blue', label='Region-to-Self', alpha=0.7) plt.subplot(2,1,2) for l in labels: if l != LT: plt.hist(lt_to_brain[l], 50, density=True, linewidth=2, alpha=0.4, histtype='step')    Empirical distributions of within-region (top) and between-region (bottom) $d^{2}$ values. Each line is the distribution of the distance of samples in our ROI to a whole region.   As expected, the distribution of $d^{2}$ the distance of samples in our region of interest, $l_{T}$, to distributions computed from other regions are (considerably) larger and much more variable, while the profile of points within $l_{T}$ looks to have much smaller variance \u0026ndash; this is good! This means that we have high intra-regional similarity when compared to inter-regional similarities. This fits what\u0026rsquo;s known in neuroscience as the \u0026ldquo;cortical field hypothesis\u0026rdquo;.\nStep 2: Distributional QC-Check Because we know that our data should follow a $\\chi^{2}_{p}$ distribution, we can fit the MLE estimate of our location and scale parameters, while keeping the $df$ parameter fixed.\np = data_lt.shape[1] mle_chi2_theory = chi2.fit(dist_lt2, fdf=p) xr = np.linspace(data_lt.min(), data_lt.max(), 1000) pdf_chi2_theory(xr, *mle_chi2_theory) fig = plt.subplot(1,2,2,figsize=(18, 6)) # plot theoretical vs empirical null distributon plt.subplot(1,2,1) plt.hist(data_lt, density=True, color='blue', alpha=0.6, label = 'Empirical') plt.plot(xr, pdf_chi2_theory, color='red', label = '$\\chi^{2}_{p}') # plot QQ plot of empirical distribution plt.subplot(1,2,2) probplot(D2.squeeze(), sparams=mle_chi2_theory, dist=chi2, plot=plt);    Density and QQ plot of null distribution.   From looking at the QQ plot, we see that the empirical density fits the theoretical density pretty well, but there is some evidence that the empirical density has heavier tails. The heavier tail of the upper quantile could probability be explained by acknowledging that our starting cortical map is not perfect (in fact there is no \u0026ldquo;gold-standard\u0026rdquo; cortical map). Cortical regions do not have discrete cutoffs, although there are reasonably steep gradients in connectivity. If we were to include samples that were considerably far away from the the rest of the samples, this would result in inflated densities of higher $d^{2}$ values.\nLikewise, we also made the distributional assumption that our connectivity vectors were multivariate normal \u0026ndash; this might not be true \u0026ndash; in which case our assumption that $d^{2}$ follows a $\\chi^{2}_{p}$ would also not hold.\nFinally, let\u0026rsquo;s have a look at some brains! Below, is the region we used as our target \u0026ndash; the connectivity profiles from vertices in this region were used to compute our mean vector and covariance matrix \u0026ndash; we compared the rest of the brain to this region.\n  Region of interest.     Estimated squared Mahalanobis distances, overlaid on cortical surface.   Here, larger $d^{2}$ values are in red, and smaller $d^{2}$ are in black. Interestingly, we do see pretty large variance of $d^{2}$ spread across the cortex \u0026ndash; however the values are smoothly varying, but there do exists sharp boundaries. We kind of expected this \u0026ndash; some regions, though geodesically far away, should have similar connectivity profiles if they\u0026rsquo;re connected to the same regions of the cortex. However, the regions with connectivity profiles most different than our target region are not only contiguous (they\u0026rsquo;re not noisy), but follow known anatomical boundaries, as shown by the overlaid boundary map.\nThis is interesting stuff \u0026ndash; I\u0026rsquo;d originally intended on just learning more about the Mahalanobis Distance as a measure, and exploring its distributional properties \u0026ndash; but now that I see these results, I think it\u0026rsquo;s definitely worth exploring further!\n","date":1544184752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544271152,"objectID":"dc8c4d220071bbd6ea4e8467660f243d","permalink":"/post/mahalanobis-distances-of-brain-connectivity/","publishdate":"2018-12-07T05:12:32-07:00","relpermalink":"/post/mahalanobis-distances-of-brain-connectivity/","section":"post","summary":"For one of the projects I\u0026rsquo;m working on, I have an array of multivariate data relating to brain connectivity patterns. Briefly, each brain is represented as a surface mesh, which we represent as a graph $G = (V,E)$, where $V$ is a set of $n$ vertices, and $E$ are the set of edges between vertices.","tags":[],"title":"Mahalanobis Distances of Brain Connectivity","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m going over Chapter 5 in Casella and Berger\u0026rsquo;s (CB) \u0026ldquo;Statistical Inference\u0026rdquo;, specifically Section 5.5: Convergence Concepts, and wanted to document the topic of convergence in probability with some plots demonstrating the concept.\nFrom CB, we have the definition of convergence in probability: a sequence of random variables $X_{1}, X_{2}, \u0026hellip; X_{n}$ converges in probability to a random variable $X$, if for every $\\epsilon \u0026gt; 0$,\n$$\\begin{align} \\lim_{n \\to \\infty} P(| X_{n} - X | \\geq \\epsilon) = 0 \\\\\n\\end{align}$$\nIntuitively, this means that, if we have some random variable $X_{k}$ and another random variable $X$, the absolute difference between $X_{k}$ and $X$ gets smaller and smaller as $k$ increases. The probability that this difference exceeds some value, $\\epsilon$, shrinks to zero as $k$ tends towards infinity. Using *convergence in probability*, we can derive the Weak Law of Large Numbers (WLLN):\n$$\\begin{align} \\lim_{n \\to \\infty} P(|\\bar{X}_{n} - \\mu | \\geq \\epsilon) = 0 \\end{align}$$\nwhich we can take to mean that the sample mean converges in probability to the population mean as the sample size goes to infinity. If we have finite variance (that is $Var(X) \u0026lt; \\infty$), we can prove this using Chebyshev\u0026rsquo;s Law\n$$\\begin{align} \u0026amp;= P(|\\bar{X}_{n} - \\mu | \\geq \\epsilon) \\\\\n\u0026amp;= P((\\bar{X}_{n} - \\mu)^{2} \\geq \\epsilon^{2}) \\leq \\frac{E\\Big[(\\bar{X}_{n} - \\mu)^{2}\\Big]}{\\epsilon^{2}} \\\\\n\u0026amp;= P((\\bar{X}_{n} - \\mu)^{2} \\geq \\epsilon^{2}) \\leq \\frac{Var(\\bar{X_{n}})}{\\epsilon^{2}} \\\\\n\u0026amp;= P((\\bar{X}_{n} - \\mu)^{2} \\geq \\epsilon^{2}) \\leq \\frac{\\sigma^{2}}{n^{2}\\epsilon^{2}} \\end{align}$$\nwhere $\\frac{\\sigma^{2}}{n^{2} \\epsilon^{2}} \\rightarrow 0$ as $n \\rightarrow \\infty$. Intuitively, this means, that the sample mean converges to the population mean \u0026ndash; and the probability that their difference is larger than some value is bounded by the variance of the estimator. Because we showed that the variance of the estimator (right hand side) shrinks to zero, we can show that the difference between the sample mean and population mean converges to zero.\nWe can also show a similar WLLN result for the sample variance using Chebyshev\u0026rsquo;s Inequality, as:\n$$\\begin{align} S_{n}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_{i} - \\bar{X}_{n})^{2} \\end{align}$$\nusing the unbiased estimator, $S_{n}^{2}$, of $\\sigma^{2}$ as follows:\n$$\\begin{align} P(|S_{n}^{2} - \\sigma^{2}| \\geq \\epsilon) \\leq \\frac{E\\Big[(S_{n}^{2} - \\sigma^{2})^{2}\\Big]}{\\epsilon^{2}} = \\frac{Var(S_{n}^{2})}{\\epsilon^{2}} \\end{align}$$\nso all we need to do is show that $Var(S_{n}^{2}) \\rightarrow 0$ as $n \\rightarrow \\infty$.\nLet\u0026rsquo;s have a look at some (simple) real-world examples. We\u0026rsquo;ll start by sampling from a $N(0,1)$ distribution, and compute the sample mean and variance using their unbiased estimators.\n# Import numpy and scipy libraries import numpy as np from scipy.stats import norm %matplotlib inline import matplotlib.pyplot as plt plt.rc('text', usetex=True)  # Generate set of samples sizes samples = np.concatenate([np.arange(0, 105, 5), 10*np.arange(10, 110, 10), 100*np.arange(10, 210, 10)]) # number of repeated samplings for each sample size iterations = 500 # store sample mean and variance means = np.zeros((iterations, len(samples))) vsrs = np.zeros((iterations, len(samples))) for i in np.arange(iterations): for j, s in enumerate(samples): # generate samples from N(0,1) distribution N = norm.rvs(loc=0, scale=1, size=s) mu = np.mean(N) # unbiased estimate of variance vr = ((N - mu)**2).sum()/(s-1) means[i, j] = mu vsrs[i, j] = vr  Let\u0026rsquo;s have a look at the sample means and variances as a function of the sample size. Empirically, we see that both the sample mean and variance estimates converge to their population parameters, 0 and 1.\n   Sample mean estimates as a function of sample size.     Sample variance estimates as a function of sample size.   Below is a simple method to compute the empirical probability that an estimate exceeds the epsilon threshold.\ndef ecdf(data, pparam, epsilon): \u0026quot;\u0026quot;\u0026quot; Compute empirical probability P( |estimate - pop-param| \u0026lt; epsilon). Parameters: - - - - - data: array, float array of samples pparam: float true population parameter epsilon: float threshold value \u0026quot;\u0026quot;\u0026quot; compare = (np.abs(data - pparam) \u0026lt; epsilon) prob = compare.mean(0) return prob  # test multiple epsilon thresholds e = [0.9, 0.75, 0.5, 0.25, 0.1, 0.05, 0.01] mean_probs = [] vrs_probs = [] # compute empirical probabilities at each threshold for E in e: mean_probs.append(1 - ecdf(means, pparam=0, epsilon=E)) vrs_probs.append(1-ecdf(vsrs, pparam=1, epsilon=E))     Empirical probability that mean estimate exceeds population mean by epsilon.     Empirical probability that variance estimate exceeds population variance by epsilon.   The above plots show that, as sample size increases, the mean estimator and variance estimator both converge to their true population parameters. Likewise, examining the empirical probability plots, we can see that the probability that either estimate exceeds the epsilon thresholds shrinks to zero as the sample size increases.\nIf we wish to consider a stronger degree of convergence, we can consider convergence almost surely, which says the following:\n$$\\begin{align} P(\\lim_{n \\to \\infty} |X_{n} - X| \\geq \\epsilon) = 0 \\\n\\end{align}$$\nwhich considers the entire joint distribution of estimates $( X_{1}, X_{2}\u0026hellip;X_{n}, X)$, rather than all pairwise estimates $(X_{1},X), (X_{2},X)\u0026hellip; (X_{n},X)$ \u0026ndash; the entire set of estimates must converge to $X$ as the sample size approaches infinity.\n","date":1543435952,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543522352,"objectID":"d309e4d84564ba27a91e72a9cd39b694","permalink":"/post/convergence-in-probability/","publishdate":"2018-11-28T13:12:32-07:00","relpermalink":"/post/convergence-in-probability/","section":"post","summary":"I\u0026rsquo;m going over Chapter 5 in Casella and Berger\u0026rsquo;s (CB) \u0026ldquo;Statistical Inference\u0026rdquo;, specifically Section 5.5: Convergence Concepts, and wanted to document the topic of convergence in probability with some plots demonstrating the concept.\nFrom CB, we have the definition of convergence in probability: a sequence of random variables $X_{1}, X_{2}, \u0026hellip; X_{n}$ converges in probability to a random variable $X$, if for every $\\epsilon \u0026gt; 0$,","tags":[],"title":"Convergence In Probability","type":"post"},{"authors":[],"categories":[],"content":"In this post, I\u0026rsquo;m going to briefly cover the relationship between the Poisson distribution and the Multinomial distribution.\nLet\u0026rsquo;s say that we have a set of independent, Poisson-distributed random variables $Y_{1}, Y_{2}\u0026hellip; Y_{k}$ with rate parameters $\\lambda_{1}, \\lambda_{2}, \u0026hellip;\\lambda_{k}$. We can model the sum of these random variables as a new random variable $N = \\sum_{i=1}^{k} Y_{i}$.\nLet start with $k=2$. We can define the distrbution of $F_{N}(n)$ as follows:\n$$\\begin{align} \u0026amp;= P(N \\leq n) \\\\\n\u0026amp;= P(Y_{1} + Y_{2} \\leq n) \\\\\n\u0026amp;= P(Y_{1} = y_{1}, Y_{2} = n - y_{1}) \\\\\n\u0026amp;= P(Y_{1} = y_{1}) \\cdot P(Y_{2} = n-y_{1}) \\\\\n\u0026amp;= \\sum_{y_{1}=0}^{n} \\frac{e^{-\\lambda_{1}}\\lambda_{1}^{y_{1}}}{y_{1}!} \\cdot \\frac{e^{-\\lambda_{2}}\\lambda_{2}^{n-y_{1}}}{(n-y_{1})!} \\\\\n\u0026amp;= e^{-(\\lambda_{1}+\\lambda_{2})} \\sum_{y_{1}=0}^{n} \\frac{\\lambda_{1}^{y_{1}}\\lambda_{2}^{n-y_{1}}}{y_{1}!(n-y_{1})!} \\\\\n\u0026amp;= e^{-(\\lambda_{1}+\\lambda_{2})} \\sum_{y_{1}=0}^{n} \\frac{n!}{n!}\\frac{\\lambda_{1}^{y_{1}}\\lambda_{2}^{n-y_{1}}}{y_{1}!(n-y_{1})!} \\\\\n\u0026amp;= \\frac{e^{-(\\lambda_{1}+\\lambda_{2})}}{n!} \\sum_{y_{1}=0}^{n} {n\\choose y_{1}} \\lambda_{1}^{y_{1}}\\lambda_{2}^{n-y_{1}} \\end{align}$$\nHere, we can apply the Binomial Theorem to the summation to get the following (remember that the Binomial Theorem says, for two numbers $x$ and $y$, that $(x+y)^{n} = \\sum_{i=0}^{n} {n \\choose i}x^{i}y^{n-i}$):\n$$\\begin{align} \\frac{e^{-(\\lambda_{1}+\\lambda_{2})}(\\lambda_{1} + \\lambda_{2})^{n}}{n!} \\\\\n\\end{align}$$\nwhich we see is in fact just another Poisson distribution with rate parameter equal to $\\lambda_{1} + \\lambda_{2}$. This shows that the sum of independent Poisson distributed random variables is also a Poisson random variable, with rate parameter equal to the sum of the univariate rates. By induction, we see that for $k$ independent Poisson distributed random variables $Y_{1}\u0026hellip;Y_{k}$, their sum $\\sum_{i=1}^{k} Y_{i} \\sim Poisson(\\sum_{i=1}^{k} \\lambda_{i})$.\nNow let\u0026rsquo;s say we\u0026rsquo;re interested in modeling the conditional distribution of $(Y_{1}\u0026hellip;Y_{k}) \\mid \\sum_{i=1}^{k} = n$. By definition of conditional probability, we have that\n$$\\begin{align} P(\\bar{Y} \\mid N=n) \u0026amp;= \\frac{P(\\bar{Y} ; \\cap ; N=n)}{P(N=n)} \\\\\n\u0026amp;= \\frac{P(\\bar{Y})}{P(N=n)} \\end{align}$$\nWe have the following:\n$$\\begin{align} P(\\bar{Y} \\mid N=n) \u0026amp;= \\frac{P(\\bar{Y} \\; \\cap \\; N=n)}{P(N=n)} \\\\\n\u0026amp;= \\Big( \\prod_{i=1}^{k} \\frac{e^{-\\lambda_{i}} \\cdot \\lambda_{i}^{y_{i}}}{y_{i}!} \\Big) \\Big/ \\frac{e^{-\\sum_{i=1}^{k} \\lambda_{i}}(\\sum_{i}^{k} \\lambda_{i})^{n}}{n!} \\\\\n\u0026amp;= \\Big( \\frac{ e^{-\\sum_{i=1}^{k}} \\prod_{i=1}^{k} \\lambda_{i}^{y_{i}}}{\\prod_{i=1}^{k} y_{i}!} \\Big) \\Big/ \\frac{e^{-\\sum_{i=1}^{k} \\lambda_{i}}(\\sum_{i}^{k} \\lambda_{i})^{n}}{n!} \\\\\n\u0026amp;= { n \\choose y_{1}, y_{2}, \u0026hellip;y_{k}} \\frac{\\prod_{i=1}^{k} \\lambda_{i}^{y_{i}}} { \\sum_{i}^{k} \\lambda_{i})^{n}} \\\\\n\u0026amp;= { n \\choose y_{1}, y_{2}, \u0026hellip;y_{k}} \\prod_{i=1}^{k} \\Big( \\frac{ \\lambda_{i} }{\\sum_{i}^{k} \\lambda_{i}} \\Big)^{y_{i}} \\\\\n\u0026amp;\\sim MultiNom(n; \\frac{\\lambda_{1}}{\\sum_{i=1}^{k}}, \\frac{\\lambda_{2}}{\\sum_{i=1}^{k}}, \u0026hellip; \\frac{\\lambda_{k}}{\\sum_{i=1}^{k}}) \\end{align}$$\nSo finally, we see that, given the sum of independent Poisson random variables, that conditional distribution of each element of the Poisson vector is Multinomial distributed, with count probabilities scaled by the sum of the individual rates. Importantly, we can extend these ideas (specifically the sum of independent Poisson random variables) to other models, such as splitting and merging homogenous and non-homogenous Poisson Point Processes.\n","date":1541664752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541751152,"objectID":"24a2b0fdba66c317422178c599aa0f8b","permalink":"/post/overview-of-poisson-multinomial-relationship/","publishdate":"2018-11-08T01:12:32-07:00","relpermalink":"/post/overview-of-poisson-multinomial-relationship/","section":"post","summary":"In this post, I\u0026rsquo;m going to briefly cover the relationship between the Poisson distribution and the Multinomial distribution.\nLet\u0026rsquo;s say that we have a set of independent, Poisson-distributed random variables $Y_{1}, Y_{2}\u0026hellip; Y_{k}$ with rate parameters $\\lambda_{1}, \\lambda_{2}, \u0026hellip;\\lambda_{k}$.","tags":[],"title":"Overview of Poisson-Multinomial Relationship","type":"post"},{"authors":[],"categories":[],"content":"While most of my day-to-day research entails writing Python code, I also make heavy use of pre-written software. Most software comes pre-compiled, but whenever possible, I like to get access to the source code. I\u0026rsquo;m going to refer to some modifications I made to pre-existing packages \u0026ndash; you can find those in my repository here.\nThe most-commonly used open-source package for brain imaging is called FMRIB Software Library (FSL), which includes tools for processing MRI data, with applications ranging from motion correction and image registration, to modal decomposition methods, among many others. All of this is made available as a set of pre-compiled C++ binaries.\nI needed to modify FSL\u0026rsquo;s probtrackx2 tool. probtrackx2 is a tool for generating probabilistic tractography. Using diffusion MRI, we can model the movement of water in the brain. At the voxel level, diffusion tends to be high when water moves along neuronal axon bundles, and low when moving against the myelin or in the extracellular matrix \u0026ndash; this water movement can be modeled using a variety of approaches.\n  Diffusion tractography from Biomedical Image Computing Group at USC.   At the simplest level, the diffusion can be modeled as a diffusion tensor, where the eigenvalues of the tensor correspond to the amount of diffusion in the direction of the corresponding eigenvector. At the more complex levels, we can represent the diffusion as a 3D probability distribution function, whose marginal distributions are called orientation distribution functions (ODF), and represent these continuous functions using a spherical harmonics basis set of the ODF. Using probtrackx2, we can sample these ODFs using a Markov Chain Monte Carlo approach and \u0026ldquo;walk\u0026rdquo; throught the brain. Directions where the diffusion signal is high will be sampled more often, and we can generate a robust representation of the macroscale neuronal structural in the brain using these random walks.\n  Orientation distribution functions from Vega et al. 2009.   The diffusion signal at the gray matter / white matter interface of the cortex is more isotropic than within the white matter (e.g. the diffusion tensors in these regions are more spherical). To reduce noise in my fiber tracking results due to this low signal, I wanted to be able to force the first steps of the streamline propagation algorithm to follow a specific direction into the white matter, before beginning the MCMC sampling procedure. Essentially what this boils down to is providing probtrackx2 with prespecified spherical coordinates (azimuthal and polar angles) for the first propagation step. More specifically, I computed the initial spherical coordinates using surfaces computed from the mesh curvature flow results of St.-Onge et al. Importantly, I wanted to make use of the probtrackx2 infrastructure as much as possible e.g. I didn\u0026rsquo;t want to write my own classes for loading in surface data, and wanted to minimally update the members of any other classes I found useful.\n  Surface-flow seeded tractography from St-Onge et al. 2018.   Jumping under the hood into the probtrackx2 code was a feat. While the software is sophistcated, it is quite poorly documented. As is common with academic code, development generally begins as a way to solve a specific problem in the lab, rather than as a package to be made available for public use. FSL has been around for a while, and grows in complexity all the time, so the initial academic-oriented mindset has somewhat propagated through their development cycles. I was able to identify the important classes and make my modifications to these three classes:\n  Particle in particle.h :\n performs the tracking for a single streamline for a single seed, where MCMC sampling happens    Seedmanager in streamlines.h :\n manages the individual seeds, instantiates Particle objects    Counter in streamlines.h :\n keeps track of streamline coordinates in 3D-space, successful streamlines, binary brain masks, saves fiber count distributions as brain volumes    The bulk of the tracking is done using these three \u0026ndash; the rest of the probtrackx2 code is almost entirely devoted to parsing other options and handling other input data. While I now have a lot of work to do in actually using my modifications, this foray into FSL\u0026rsquo;s source code re-emphasized three important lessons:\n  Documentation is critical. But not just any documentation \u0026ndash; meaningful documentation. Even if you aren\u0026rsquo;t the best at object-oriented software development, at least describe what your code does, and give your variables meaningful names. Had their code been effectively documented, I could have been in and out of there in two or three days, but instead spent about a week figuring out what was actually going on.\n  You should be equally comfortable working with raw code developed by others, as you are writing your own. Do not expect everything to be written correctly, and do not assume that just because others have used a piece of software before, that you won\u0026rsquo;t need to make modifications. Be ready to get your hands dirty.\n  Do not underestimate the power of compiled languages. Most data scientists work with Python and R due to the speed of development and low barrier to entry, but each is based primarily in C (and I believe not in C++ due to timing of original development cycles). Many large-scale software packages are based on languages like C, C++, and Java. Likewise, if your work bridges the gap between data scientist and engineer, you\u0026rsquo;ll definitely need to be comfortable working with compiled languages for production-level development and deployment.\n  ","date":1540825952,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540912352,"objectID":"5d7d56528541370b384433931ab7b7bf","permalink":"/post/headfirst-into-uncommented-c++/","publishdate":"2018-10-29T08:12:32-07:00","relpermalink":"/post/headfirst-into-uncommented-c++/","section":"post","summary":"While most of my day-to-day research entails writing Python code, I also make heavy use of pre-written software. Most software comes pre-compiled, but whenever possible, I like to get access to the source code. I\u0026rsquo;m going to refer to some modifications I made to pre-existing packages \u0026ndash; you can find those in my repository here.","tags":[],"title":"Headfirst into (Uncommented) C++","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;ve been toying around with openCV for generating MRI images with synethetic motion injected into them. I\u0026rsquo;d never used this library before, so I tested a couple examples. Below I detail a few tools that I found interesting, and that can quickly be used to generate image transformations.\n# import necessary libraries import matplotlib.pyplot as plt import nibabel as nb import cv2 # load image file image_file = './data/T1w_restore_brain.nii.gz' img_obj = nb.load(image_file) img = img_obj.get_data() # reorient so Anterior-Posterior axis corresponds to dim(0) img = np.fliplr(img) img = np.swapaxes(img, 0, 2) # get single image slice and rescale data = img[130, :, :] data = (data-data.min())/data.max() plt.imshow(data)    For any linear transformations with cv2, we can use the cv2.warpAffine method, which takes in the original image, some transformation matrix, and the size of the output image.\nLet\u0026rsquo;s start with translations. The matrix will translate the image 10 pixels to the right (width), and 0 pixels down (height).\n# Use the identity rotation matrix # Third column specifies translation in corresponding direction translation = np.array([[1, 0, 20], [0, 1, 0]]) translated = cv2.warpAffine(data, translation, data.T.shape) plt.imshow(translated)    Now, in order to rotate the image, we can use cv2.getRotationMatrix2D. We\u0026rsquo;ll rotate our image by 45$^{\\circ}$ .\n# get shape of input image rows, cols = data.shape # specify angle of rotation around central pixel M = cv2.getRotationMatrix2D((cols/2,rows/2), 45, 1) rotated = cv2.warpAffine(data, M, (cols, rows)) plt.imshow(rotated)    Here are a few examples of randomly translating +/- 1, 5, or 9 voxels in the X and Y directions, and randomly rotating by 1, 5, or 9 degrees:\n# get shape of input image rows, cols = data.shape # specify range of rotations and translations txfn = [1, 5, 9] for rt in txfn: # generate rotation matrix # randonly rotate to left or right M = cv2.getRotationMatrix2D( (cols/2, rows/2), np.random.choice([-1, 1], 1)[0]*rt, 1) # apply rotation matrix rotated = cv2.warpAffine(data, M, data.T.shape) # generate translation matrix # randomly translate to left or right T = np.array( [[1,0,np.random.choice([-1, 1], 1)[0]*rt], [0, 1,np.random.choice([-1, 1], 1)[0]*rt]]).astype(np.float32) # apply translation matrix translated = cv2.warpAffine(data, T, data.T.shape) # compose rotated and translated images movement = (rotated + translated)/2 # compute difference between input and transformed difference = data-movement res = difference.reshape(np.product(difference.shape)) fig,[ax1,ax2,ax3] = plt.subplots(1,3,figsize=(15,5)) ax1.imshow(movement, cmap='gray') ax1.set_title('Composed Random Rotation and Translation \\n Magnitude = {:}'.format(rt), fontsize=15) ax2.imshow(D-rotated, cmap='gray') ax2.set_title('Difference Map'.format(rt), fontsize=15) ax3.hist(res[res!=0],100,density=True) ax3.set_title('Difference Density', fontsize=15) plt.tight_layout() plt.show()         While this approach of generating synthetic motion into MRI images is a poor model of how motion actually occurs during an MRI scan, there are a few things I learned here. For example, if you define a measure of image similarity, like mutual information, entropy, or correlation ratio as a cost function, we can see how we can use warpAffine to find the optimal transformation matrix between two images.\nI was hoping to use openCV to generate and apply 3d affine transformations to volumetric MRI data. One approach to doing this is to iteratively apply rotations and transformations along each axis \u0026ndash; however, openCV will interpolate the data after each transformation, resulting in a greater loss of signal than I am willing to compromise on. It doesn\u0026rsquo;t seem like openCV has ability to apply 3d affine transformations to volumetric data in a single interpolation step.\nA more realistic approach to generating synthetic motion artifacts that would more accurately parallell the noise-generating process, is to compute the Fast Fourier Transform of my 3d volume, and then apply phase-shifts to the k-space signal \u0026ndash; this will also manifest as motion after applying the inverse FFT.\nAfter doing a bit more digging through the openCV API, it seems there\u0026rsquo;s a lot of cool material for exploration \u0026ndash; these applications specifically caught my eye and would be fun to include in projects:\n  video analysis for motion tracking  object recognition for detecting faces  openCV Android for app development  But alas \u0026ndash; the search continues!\n","date":1535847152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535933552,"objectID":"1d2fa8c43765118974c4c0efad406fc9","permalink":"/post/image-transformations-with-opencv/","publishdate":"2018-09-01T17:12:32-07:00","relpermalink":"/post/image-transformations-with-opencv/","section":"post","summary":"I\u0026rsquo;ve been toying around with openCV for generating MRI images with synethetic motion injected into them. I\u0026rsquo;d never used this library before, so I tested a couple examples. Below I detail a few tools that I found interesting, and that can quickly be used to generate image transformations.","tags":[],"title":"Image Transformations With OpenCV","type":"post"},{"authors":[],"categories":[],"content":"I recently attended Neurohackademy 2018, hosted by the University of Washington\u0026rsquo;s eScience Institute, and organized by Dr. Ariel Rokem and Dr. Tal Yarkoni.\nThis was a 2-week long event, beginning with a series of daily lectures, and ending with a fast-paced, high-intensity scramble to put together a beta (but working) version of some project coupling neuroimaging with software development. The lectures varied in topic, from how to test academic code and how organize open source projects, to machine learning and algorithms for low-dimensional representations of neural recordings, to neuroethics (full lecture list here).\nMany of the lecturers are scientists and developers who I\u0026rsquo;ve looked up to for years \u0026ndash; a few have even been my intellectual, and now post-Neurohackademy, philosophical role models. While the lectures were enlightening in their own right, there was a level of intimacy during these 2 weeks that\u0026rsquo;s been unmatched during grad school so far. Rarely do young researchers like myself get to pick the brains of and engage in scientific banter with scientists whose papers they read, or whose updates they follow, in such a fluid and collaborative setting. I feel a little weird being so enthusiastic about it (specifically because I know some of them might read this), but (and I think I speak for all of us who participated) it was a richly rewarding experience. \n  The #NH18 participants varied in status from graduate students, to post-docs, to industry members, and traveled from all around the world to Seattle, but each one of us was, in one way or another, involved with neuroscience research. As one of my new friends put it on Twitter: \u0026ldquo;Spent yesterday in a room full of relative strangers who were collaborating, mentoring, \u0026amp; supporting. Devoid of egos or tribalism. Feels like what science (\u0026amp; society) should be.\u0026quot; The sense of community was strong, positivity was plentiful, and people supported one another \u0026ndash; without regard for experience or any sense of return-on-investment. We had an established Git person, Python person, data viz person, fMRI person, C++ person, etc. \u0026ndash; if you had a question or ran into an issue, with high probability there was someone who could and would help you out. Everyone was genuinely exicted to learn, to share, to create, to bond, and especially, ~to neuro/computer/data-science~.\n\u0026mdash; to science \u0026mdash;\n (origin: probably Newton) verb. To perform scientific research, almost always in a smooth or cool way.  I worked on a project directly related to my research, but that I\u0026rsquo;d only previously written some messy, non-shareable scripts for. Conveniently, my co-NeuroHacker Michael Notter had a similar idea and we hit the ground running with some of our colleagues. The project, titled parcellation_fragmenter, provides a means for fragmenting the brain cortex into a predefined number of regions, or a set of regions each of the same size. These regions can be anatomically constrained, or arbitratilly spread across the cortex. Our goal was to use this tool to speed up statistical tests, like SearchLight FDR, or as a feature extraction method for down-stream machine learning applications. I\u0026rsquo;m currently using this tool to examine how cortical network resolution impacts pairwise regional network properties.\nHere is something pretty! It was created with the new parcellation fragmenter (https://t.co/9VLCpr336Y), developed by Kristian Eschenburg, @kako_toro, Amanda Sidwell \u0026amp; me during the #NHW18. Thank\u0026#39;s to #nilearn \u0026amp; #nibabel creating this toolbox was straightforward and a lot of fun! pic.twitter.com/LdqTCMSyrJ\n\u0026mdash; Michael Notter (@miyka_el) August 10, 2018  My most important takeaway from our project was learning how to collaboratively write and develop software with a group of people. Not only did each of our team members have unique ideas about how to approach our specific problem, we also each had different ways of thinking about how to write software in general. Clear communication, open-mindedness, and understanding on all of our parts were integral to seeing this development through. Overall, the project was a success!\nHere are a few things things I learned, that I\u0026rsquo;m going to incorporate into my own work (and hopefuly convince people at lab to do the same):\n Unit-test my code using pytest and nose Incorporate continuous integration (I\u0026rsquo;ve already made use of [TravisCI]({% post_url 2018-08-12-custom-plugins-with-travisci %})!) Learn web-dev, and specifically, JavaScript (to use D3, and develop interactive posters and publications) Contribute to issues / create pull-requests on GitHub repos that I use or find interesting Pre-register my papers and submit to open-source journals  This event was what I\u0026rsquo;d hoped graduate school would be like all along. While idealistic and naiive to some degree, I still think it can be. The open-source model is shifting how research is performed \u0026ndash; the act of doing research is evolving in such a way that it is no longer tethered to specific institutions or labs, and given tools like Docker and AWS, you can almost perfectly recreate specific computing environments needed to perform the work. With the rise of open-source datasets, especially due to researchers willingly distributing their data and code, collaborative environments like that fostered by Neurohackademy (even if digital), and the ability to replicate workflows, results, and analyses, are becoming more and more feasible. It only takes a few proponents of the open-source model to give the idea momentum.\nIf this is the future, the future is looking good.\n","date":1534579953,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534579953,"objectID":"44afdcb13bb6a70c71ef77d824b7e031","permalink":"/post/two-weeks-of-open-science/","publishdate":"2018-08-18T01:12:33-07:00","relpermalink":"/post/two-weeks-of-open-science/","section":"post","summary":"I recently attended Neurohackademy 2018, hosted by the University of Washington\u0026rsquo;s eScience Institute, and organized by Dr. Ariel Rokem and Dr. Tal Yarkoni.\nThis was a 2-week long event, beginning with a series of daily lectures, and ending with a fast-paced, high-intensity scramble to put together a beta (but working) version of some project coupling neuroimaging with software development.","tags":[],"title":"Two Weeks of Open Science: A Rekindled Flame","type":"post"},{"authors":[],"categories":[],"content":"I just learned about TravisCI (actually, about continuous integration (CI) in general) after attending Neurohackademy 2018. We learned about CI from the perspective of ensuring that your code builds properly when you update files in your packages, incorporate new methods, refactor your code, etc. Pretty neat.\nFast forward a couple days, and I\u0026rsquo;m trying to incorporate custom Jekyll plugins into my blog \u0026ndash; I quickly realized GitHub doesn\u0026rsquo;t allow this for security reasons, but I couldn\u0026rsquo;t find a convenient work-around. Some posts suggested using a separate repo branch to build the site, and then push the static HTML files up to a remote repo to do the actual hosting, but for some reason I couldn\u0026rsquo;t get that approach to work.\nFinally, I saw some mentions of using TravisCI and CircleCI to build and push the site using continuous integration. I ended up using the approach suggested by Josh Frankel.\nJosh\u0026rsquo;s site gives a really clear explanation of the necessary steps, given some very minmal prequisite knowledge about using Git. His instructions actually worked almost perfectly for me, so I won\u0026rsquo;t repeat them again here (just follow the link above, if you\u0026rsquo;re interested) \u0026ndash; however, there were a few issues that arose on my end:\n  For some reason, I had an about.html file and index.html file in the main repo directory \u0026ndash; my built blog wouldn\u0026rsquo;t register any updates I made to about.md or index.md while these files were around, so I deleted the HTML files. This might have been an obvious bug to someone with more web programming experience, but I\u0026rsquo;m a novice at that. If you\u0026rsquo;re seeing any wonky behavior, check to make sure you don\u0026rsquo;t have any unnecessary files hanging around.\n  Ruby version: I had to change the version of Ruby I was using to ruby-2.4.1.\n  Plugins: Make sure any Jekyll plugins you want to use are already installed.\n  Emails: You can turn off email reporting from TravisCI by adding notifications: email: false to your .travis.yml file.\n  But now, you can incorporate custom, user-built Jekyll plugins and let TravisCI do the heavy lifting! I specifically wanted the ability to reference papers using BibTex-style citation links with Jekyll, like you can with LaTex or Endnote \u0026ndash; this capability isn\u0026rsquo;t currently supported by GitHub. Happy blogging!\n","date":1534065254,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534151654,"objectID":"7f661148376fd675c5e716c5efc1509b","permalink":"/post/enabling-custom-jekyll-plugins/","publishdate":"2018-08-12T02:14:14-07:00","relpermalink":"/post/enabling-custom-jekyll-plugins/","section":"post","summary":"I just learned about TravisCI (actually, about continuous integration (CI) in general) after attending Neurohackademy 2018. We learned about CI from the perspective of ensuring that your code builds properly when you update files in your packages, incorporate new methods, refactor your code, etc.","tags":[],"title":"Enabling Custom Jekyll Plugins with TravisCI","type":"post"},{"authors":[],"categories":[],"content":"In putting together this blog, I wanted to be able to talk about various mathematical topics that I found interesting, which inevitably lead to using LaTex in my posts.\nI\u0026rsquo;m currently using Atom as my editor (having converted from Sublime), and needed to install a bunch of packages first. First and foremost, I wanted to be able to render my markdown posts before hosting them on the blog, and consequentially needed a way to render LaTex. For this, I installed a few Atom packages:\n  Markdown-Preview  Latex  Language-Latex  To preview your post in Atom, you just type ctrl+shift+M, which will display both in-line and block math sections.\nHowever, if you build your site locally with the command bundle exec jekyll serve or push it to a remote repo, the LaTex no longer renders properly. After Googling around a bit, I determined that this was due to the way markdown converters in Jekyll, like kramdown and redcarpet, do the conversion using MathJax \u0026ndash; specifically, in-line math segments are not properly rendered. I wanted a way to both preview the LaTex in Atom, and properly render it usng Jekyll. I found two links that solved the problem for me:\n  Visually Enforced  LaTeX in Jekyll  In short, the following steps solved the problem of LaTex not rendering for me. I\u0026rsquo;m using the minima theme, so I first found the theme directory with bundle show minima. In this directory, I copied the ./layouts/post.html to a local directory in my project folder called ./_layouts/post.html.\nWithin this file, I pasted the following two sections of HTML code:\n\u0026lt;script type=\u0026quot;text/x-mathjax-config\u0026quot;\u0026gt; MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0026quot; type=\u0026quot;text/javascript\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;  And voila \u0026ndash; building the posts now correctly renders LaTex!\n","date":1533978854,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534065254,"objectID":"9ea0ef099d96743e11f3629686f09f48","permalink":"/post/rendering-latex-in-markdown-using-jekyll/","publishdate":"2018-08-11T02:14:14-07:00","relpermalink":"/post/rendering-latex-in-markdown-using-jekyll/","section":"post","summary":"In putting together this blog, I wanted to be able to talk about various mathematical topics that I found interesting, which inevitably lead to using LaTex in my posts.\nI\u0026rsquo;m currently using Atom as my editor (having converted from Sublime), and needed to install a bunch of packages first.","tags":[],"title":"Rendering LaTex In Markdown Using Jekyll","type":"post"},{"authors":[],"categories":[],"content":"In my previous post on dynamic mode decomposition, I discussed the foundations of DMD as a means for linearizing a dynamical system123. In this post, I want to look at a way in which we can use rank-updates to incorporate new information into the spectral decomposition of our linear operator, $A$, in the event that we are generating online measurements from our dynamical system4 \u0026ndash; see the citation below if you want a more-detailed overview of this topic along with open source code for testing this method.\nRecall that we are given an initial data matrix\n$$\\begin{align} X = \\begin{bmatrix} x_{n_{1},m_{1}} \u0026amp; x_{n_{1},m_{2}} \u0026amp; x_{n_{1},m_{3}} \u0026amp; \u0026hellip; \\\\\nx_{n_{2},m_{1}} \u0026amp; x_{n_{1},m_{2}} \u0026amp; x_{n_{2},m_{3}} \u0026amp; \u0026hellip; \\\\\nx_{n_{3},m_{1}} \u0026amp; x_{n_{1},m_{2}} \u0026amp; x_{n_{3},m_{3}} \u0026amp; \u0026hellip; \\\\\n\u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \\\\\n\\end{bmatrix} \\in R^{n \\times m} \\end{align}$$\nwhich we can split into two matrices, shifted one unit in time apart:\n$$\\begin{align} X^{\\ast} \u0026amp;= \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\dots \u0026amp; \\vert \\\\\n\\vec{x}_1 \u0026amp; \\vec{x}_2 \u0026amp; \\dots \u0026amp; \\vec{x}_{m-1} \\\\\n\\vert \u0026amp; \\vert \u0026amp; \\dots \u0026amp; \\vert \\\\\n\\end{bmatrix} \\in R^{n \\times (m-1)} \\\\\nY \u0026amp;= \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\dots \u0026amp; \\vert \\\\\n\\vec{x}_2 \u0026amp; \\vec{x}_3 \u0026amp; \\dots \u0026amp; \\vec{x}_{m} \\\\\n\\vert \u0026amp; \\vert \u0026amp; \\dots \u0026amp; \\vert \\\\\n\\end{bmatrix} \\in R^{n \\times (m-1)} \\end{align}$$\nand we are interested in solving for the linear operator $A$, such that\n$$\\begin{align} Y = AX^{\\ast} \\end{align}$$\nFor simplicity, since we are no longer using the full matrix, I\u0026rsquo;ll just refer to $X^{\\ast}$ as $X$. In the previous post, we made the constraint that $n \u0026gt; m$, and that rank($X$) $\\leq m \u0026lt; n$. Here, however, we\u0026rsquo;ll reverse this assumption, and such that $m \u0026gt; n$, and that rank($X$) $\\leq m \u0026lt; n$, such that $XX^{T}$ is invertible, so by multiplying both sides by $X^{T}$ we have\n$$\\begin{align} AXX^{T} \u0026amp;= YX^{T} \\\\\nA \u0026amp;= YX^{T}(XX^{T})^{-1} \\\\\nA \u0026amp;= QP_{x} \\end{align}$$\nwhere $Q = YX^{T}$ and $P_{x} = (XX^{T})^{-1}$. Now, let\u0026rsquo;s say you observe some new data $x_{m+1}, y_{m+1}$, and you want to incorporate this new data into your $A$ matrix. As in the previous post on rank-one updates, we saw that directly computing the inverse could potentially be costly, so we want to refrain from doing that if possible. Instead, we\u0026rsquo;ll use the Shermann-Morrison-Woodbury theorem again to incorporate our new $x_{m+1}$ sample into our inverse matrix, just as before:\n$$\\begin{align} (X_{m+1}X^{T}_{m+1})^{-1} = P_{x} + \\frac{P_{x}x_{m+1}x_{m+1}^{T}P_{x}}{1 + x_{m+1}^{T}P_{x}x_{m+1}} \\end{align}$$\nLikewise, since we\u0026rsquo;re appending new data to our $Y$ and $X$ matrices, we also have\n$$\\begin{align} Y_{m+1} = \\begin{bmatrix} Y \u0026amp; y_{m+1} \\end{bmatrix} \\\\\nX_{m+1} = \\begin{bmatrix} \\\\\nX \u0026amp; x_{m+1} \\end{bmatrix} \\\\\n\\end{align}$$\nsuch that\n$$\\begin{align} Y_{m+1} X_{m+1}^{T} \u0026amp;= YX^{T} + y_{m+1}x_{m+1}^{T} \\\\\n\u0026amp;= Q + y_{m+1}x_{m+1}^{T} \\end{align}$$\nwhich is simply the sum of our original matrix $Q$, plus a rank-one matrix. The authors go on to describe some pretty cool \u0026ldquo;local\u0026rdquo; DMD schemes, by incorporating weights, as well as binary thresholds, that are time-dependent into the computation of the linear operator, $A$.\n  P.J. Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of Fluid Mechanics 656.1. 2010. \u0026#x21a9;\u0026#xfe0e;\n Tu et al. On Dynamic Mode Decomposition: Theory And Applications \u0026#x21a9;\u0026#xfe0e;\n Kunert-Graf et al. Extracting Reproducible Time-Resolved Resting State Networks Using Dynamic Mode Decomposition. Front. Comput. Neurosci. 2019. \u0026#x21a9;\u0026#xfe0e;\n Zhang et al. Online dynamic mode decomposition for time-varying systems. 2017. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1527186643,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527273043,"objectID":"e08f9dc7d46d7e4fe5615da5cdf0902b","permalink":"/post/exploring-neurological-dynamical-systems-part-2/","publishdate":"2018-05-24T11:30:43-07:00","relpermalink":"/post/exploring-neurological-dynamical-systems-part-2/","section":"post","summary":"In my previous post on dynamic mode decomposition, I discussed the foundations of DMD as a means for linearizing a dynamical system123. In this post, I want to look at a way in which we can use rank-updates to incorporate new information into the spectral decomposition of our linear operator, $A$, in the event that we are generating online measurements from our dynamical system4 \u0026ndash; see the citation below if you want a more-detailed overview of this topic along with open source code for testing this method.","tags":[],"title":"Exploring Neurological Dynamical Systems: Part 2","type":"post"},{"authors":[],"categories":[],"content":"In the next two posts, I want to talk briefly about an algorithm called Dynamic Mode Decomposition (DMD). DMD is a spatiotemporal modal decomposition technique that can be used to identify spatial patterns in a signal (modes), along with the time course of these spatial patterns (dynamics). As such, the algorithm assumes that the input data has a both a spatial and a temporal component. We are interested in modeling how the system evolves over time.\nIf you\u0026rsquo;d like to find more information about DMD, Peter Schmid1 and Jonathan Tu2 have written excellent expositions on the topic. Likewise, if you\u0026rsquo;d like to follow along with the code for the following analysis, see my repo. For a more in-depth analysis that applies DMD to brain activity in the resting brain, see a recent publication by my colleagues and I3, along with the code used for our analysis.\nThe DMD Algorithm Let\u0026rsquo;s assume that you\u0026rsquo;ve taken $n$ measurements from specific points in space for $m$ time points, where for now we assume that $m\\lt n$. For now, we\u0026rsquo;ll assume that the sampling frequency, $\\omega$, is stable across the entire experiment. We define our entire data matrix as\n$$\\begin{align} X = \\begin{bmatrix} x_{n_{1},m_{1}} \u0026amp; x_{n_{1},m_{2}} \u0026amp; x_{n_{1},m_{3}} \u0026amp; \u0026hellip; \\\\\nx_{n_{2},m_{1}} \u0026amp; x_{n_{1},m_{2}} \u0026amp; x_{n_{2},m_{3}} \u0026amp; \u0026hellip; \\\\\nx_{n_{3},m_{1}} \u0026amp; x_{n_{1},m_{2}} \u0026amp; x_{n_{3},m_{3}} \u0026amp; \u0026hellip; \\\\\n\u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \\\\\n\\end{bmatrix} \\in R^{n \\times m} \\end{align}$$\nWe are interested in solving for the matrix, $A \\in R^{n \\times n}$, such that\n$$\\begin{align} x_{t+1} = A x_{t} \\; \\; \\forall \\; \\; t = 1,2,\u0026hellip;m-1 \\end{align}$$\nGiven our full data matrix $X$, we can define two matrices $X^{*}$ and $Y$ such that\n$$\\begin{align} X^{*} \u0026amp;= \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\dots \u0026amp; \\vert \\\\\n\\vec{x}_{1} \u0026amp; \\vec{x}_{2} \u0026amp; \\dots \u0026amp; \\vec{x}_{m-1} \\\\\n\\vert \u0026amp; \\vert \u0026amp; \\dots \u0026amp; \\vert \\\\\n\\end{bmatrix} \\in R^{n \\times (m-1)} \\\\\nY \u0026amp;= \\begin{bmatrix} \\vert \u0026amp; \\vert \u0026amp; \\dots \u0026amp; \\vert \\\\\n\\vec{x}_2 \u0026amp; \\vec{x}_3 \u0026amp; \\dots \u0026amp; \\vec{x}_{m} \\\\\n\\vert \u0026amp; \\vert \u0026amp; \\dots \u0026amp; \\vert \\\\\n\\end{bmatrix} \\in R^{n \\times (m-1)} \\end{align}$$\nso that we can write\n$$\\begin{align} Y = AX^{\\ast} \\end{align}$$\nIf $n$ is small, this is relatively easy to compute \u0026ndash; however, if $n$ is large, as is the case when modeling temporal dynamics in resting-state MRI, it would be computationally inefficient to compute A directly. To alleviate this, we can make use of the Singular Value Decomposition (SVD) of our predictor matrix $X^{\\ast}$. We define the SVD of $X^{\\ast}$ as\n$$\\begin{align} X^{\\ast} = U \\Sigma V^{T} \\\n\\end{align}$$\nas well as the Moore-Penrose psuedo-inverse of $X^{\\ast} = X^{\\dagger}$ as\n$$\\begin{align} X^{\\dagger} = V \\Sigma^{-1} U^{T} \\\\\n\\end{align}$$\nsuch that we can write\n$$\\begin{align} YX^{\\dagger} = YV \\Sigma^{-1} U^{T} = A X^{\\ast}X^{\\dagger} = A \\\\\n\\end{align}$$\nAdditionally, if we assume that $rank(X^{\\ast}) = r \\leq m$, then we can use the truncated SVD such that\n$$\\begin{align} U \u0026amp; \\in R^{n \\times r} \\\\\nV^{T} \u0026amp; \\in R^{r \\times m} \\\\\n\\end{align}$$\nand\n$$\\begin{align} \\Sigma = \\begin{bmatrix} \\sigma_{1} \u0026amp; 0 \u0026amp; 0 \u0026amp; \u0026hellip; \\\\\n0 \u0026amp; \\sigma_{2} \u0026amp; 0 \u0026amp; \u0026hellip; \\\\\n0 \u0026amp; 0 \u0026amp; \\ddots \u0026amp; \u0026hellip; \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\sigma_{r} \\\\\n\\end{bmatrix} \\in R^{r \\times r} \\end{align}$$\nAs it stands now, we still compute an $A \\in R^{n \\times n}$ matrix. However, because we have a potentially low-rank system, we can apply a Similarity Transformation to $A$ in order to reduce its dimensionality, without changing its spectrum. Using our spatial singular vectors $U$, we define\n$$\\begin{align} \\tilde{A} \u0026amp;= U^{T} A U \\\\\n\u0026amp;= U^{T} (YV \\Sigma^{-1} U^{T}) U \\\\\n\u0026amp;= U^{T} Y V \\Sigma^{-1} \\\\\n\\end{align}$$\nwhere $\\tilde{A} \\in R^{r \\times r}$. If we consider the above SVD, we see that $U$ is the matrix of left singular vectors, an orthogonal basis that spans $C(X^{\\ast})$, which is an r-dimensional subspace of $R^{n}$. Thus, the similarity transform represents a mapping $f(A) = U^{T} A U : R^{n} \\rightarrow R^{r}$. We now have a reduced-dimensional representation of our linear operator, from which we can compute the spatial modes and dynamic behavior of each mode. First, however, because of the notion of variance captured by the singular values of our original predictor matrix, we weight $\\tilde{A}$ by the singular values as\n$$\\begin{align} \\hat{A} = \\Sigma^{\\frac{1}{2}} \\tilde{A} \\Sigma^{\\frac{1}{2}} \\\\\n\\end{align}$$\nsuch that our computed spatial modes have been weighted by the amount they contribute to our measured signal. We can now compute the eigendecomposition of $\\hat{A}$ as\n$$\\begin{align} \\hat{A} W = W \\Lambda \\\\\n\\end{align}$$\nwhere the eigenvectors $W$ are the reduced-dimension representations of our spatial modes, and the eigenvalues $\\Lambda$ capture the dynamic behavior of our spatial modes. Because our original data matrix $X^{\\ast}$ had spatial dimension $n$ and our eigenvectors have dimension $r$, we need to up-project our eigenvectors $W$ to compute the final spatial modes, via\n$$\\begin{align} \\Phi = Y V \\Sigma^{\\frac{-1}{2}}W \\end{align}$$\nFrom the SVD of our prediction matrix $X^\\ast=U \\Sigma V^{T}$, the matrix $V \\in R^{m \\times r}$ is the matrix of right singular vectors, an orthogonal basis spanning the space of $X^{\\ast T}$ (i.e. $r$ basis vectors spanning the space of the measured time courses). Thus, we see that $H = (V \\Sigma^{\\frac{-1}{2}})W$ represents a linear combination of the temporal basis vectors (a mapping from $R^{r} \\rightarrow R^{m}$) for each eigenvector $w_{i}$ of $W$, weighted by the corresponding singular value $\\sigma_{i}^{\\frac{-1}{2}}$ (that acts to normalize the spatial mode amplitudes). Finally, we see that $\\Phi = X^{\\ast}H$ computes how much of each temporal basis vector is present in the measured time course at each point in space.\nBecause we are modeling a dynamical system, we can compute the continuous time dynamics of our system using our spatial modes and eigenvalues as\n$$\\begin{align} \\vec{x}(t) \\approx \\sum_{i=1}^{r} b_{i}\\exp^{((\\gamma_{i} + 2i\\pi f_{i})\\cdot t)} \\vec{\\phi}_{i} \\end{align}$$\nwhere $\\gamma_{i}$ is a growth-decay constant and $f_{i}$ is the frequency of oscillation of the spatial mode $\\phi_{i}$. We can compute these two constants as\n$$\\begin{align} \\gamma_{i} \u0026amp;= \\frac{\\text{real}(\\text{ln}(\\lambda_{i}))}{\\Delta t} \\\\\nf_{i} \u0026amp;= \\frac{\\text{imag}(\\text{ln}(\\lambda_{i}))}{2\\pi \\Delta t} \\end{align}$$\nSo, we can see that DMD linearizes our measured time series, by fitting what can be analogized to a \u0026ldquo;global\u0026rdquo; regression. That is, instead of computing how a single time point predicts the next time point, which could readily be solved using the simple Normal equations, DMD computes how a matrix of time points predicts another matrix of time points that is shifted one unit of time into the future. To this extent, DMD minimizes the Frobenius norm of\n$$\\begin{align} \\min \\limits_{A} \\lVert Y - AX^{\\ast} \\rVert^{2}_{F} \\\\\n\\end{align}$$\nHowever, rather than explicitly computing the matrix $A$, DMD computes the eigenvectors and eigenvalues of $A$, by utilizing the Singular Value Decomposition, along with a Similarity Transformation, in order to generate a reduced-dimensional representation of $A$.\nThis spectral decomposition of our linear operator is of particular importance, because it sheds light on the fact the DMD models the temporal dynamics of our system using a Fourier basis. Each spatial mode is represented by a particular Fourier frequency along and growth-decay constant that determines the future behavior of our spatial mode. Additionally, the Fourier basis also determines what sorts of time series can be modeled using DMD \u0026ndash; time series that are expected to have sinusoidal behavior will be more reliably modeled using DMD, whereas signals that show abrupt spike patterns might be more difficult to model.\n  P.J. Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of Fluid Mechanics 656.1. 2010. \u0026#x21a9;\u0026#xfe0e;\n Tu et al. On Dynamic Mode Decomposition: Theory And Applications \u0026#x21a9;\u0026#xfe0e;\n Kunert-Graf et al. Extracting Reproducible Time-Resolved Resting State Networks Using Dynamic Mode Decomposition. Front. Comput. Neurosci. 2019. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1527022972,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527109372,"objectID":"0693f034a27204b7841fd5fe41a1e45c","permalink":"/post/exploring-neurological-dynamical-systems-part-1/","publishdate":"2018-05-22T14:02:52-07:00","relpermalink":"/post/exploring-neurological-dynamical-systems-part-1/","section":"post","summary":"In the next two posts, I want to talk briefly about an algorithm called Dynamic Mode Decomposition (DMD). DMD is a spatiotemporal modal decomposition technique that can be used to identify spatial patterns in a signal (modes), along with the time course of these spatial patterns (dynamics).","tags":[],"title":"Exploring Neurological Dynamical Systems: Part 1","type":"post"},{"authors":[],"categories":[],"content":"In this post, I\u0026rsquo;ll be covering the basics of Multivariate Normal Distributions, with special emphasis on deriving the conditional and marginal distributions.\nGiven a random variable under the usual Gauss-Markov assumptions, with $y_{i} \\sim N(\\mu, \\sigma^{2})$ with $e \\sim N(0,\\sigma^{2})$, and $N$ independent samples $y_{1}\u0026hellip;y_{n}$, we can define vector $\\mathbf{y} = [y_{1}, y_{2},\u0026hellip;y_{n}] \\sim N_{n}(\\mathbf{\\mu},\\sigma^{2}I)$ with $\\mathbf{e} \\sim N_{n}(\\mathbf{0},\\sigma^{2}I)$. We can see from the covariance structure of the errors that all off-diagonal elements are 0, indicating that our samples are independent with equal variances.\nMarginal Distributions\nNow assume that $\\mathbf{y} = [\\mathbf{y_{1}}, \\mathbf{y_{2}} ] \\sim N(\\mathbf{\\mu},\\Sigma)$, where $\\mathbf{\\mu} = \\begin{bmatrix} \\mu_{1} \\ \\mu_{2} \\end{bmatrix}$, and $\\Sigma$ is an arbitrary covariance matrix, where we cannot assume independence. If $\\Sigma$ is non-singular, we can decompose $\\Sigma$ as\n$$ \\Sigma = \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\Sigma_{21}^{T} \\\\\n\\Sigma_{21} \u0026amp; \\Sigma_{22} \\end{bmatrix}$$\nand, using the inversion lemmas from [Blockwise Matrix Inversion]({% post_url 2018-05-08-blockwise-matrix-inversion %}), define its inverse $\\Sigma^{-1} = V$ as\n$$ V = \\begin{bmatrix} V_{11} \u0026amp; V_{21}^{T} \\\\\nV_{21} \u0026amp; V_{22} \\\\\n\\end{bmatrix} \\begin{bmatrix} (\\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21})^{-1} \u0026amp; -\\Sigma^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1} \\\\\n-\\Sigma_{22}^{-1}\\Sigma_{21}(\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21})^{-1} \u0026amp; (\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1} \\end{bmatrix}$$\nFrom the properties of transformations of Normal random variables, we can define the marginal of $$y_{1}$$ as\n$$\\begin{align} By \\sim N(B\\mu,B\\Sigma B^{T}) \\end{align}$$\nwhere $B = \\begin{bmatrix} \\mathbf{I} \u0026amp; 0 \\end{bmatrix}$ such that\n$$ \\begin{bmatrix} \\mathbf{I} \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} \\mathbf{\\mu_{1}} \\ \\mathbf{\\mu_{2}} \\end{bmatrix} = \\mathbf{\\mu_{1}}$$ $$\\begin{bmatrix} \\mathbf{I} \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} \\Sigma_{11} \u0026amp; \\Sigma_{12} \\\\\n\\Sigma_{21} \u0026amp; \\Sigma_{22} \\end{bmatrix} \\begin{bmatrix} \\mathbf{I} \\\\\n0 \\end{bmatrix} = \\Sigma_{11}$$\nso that $\\mathbf{y_{1}} \\sim N(\\mathbf{\\mu_{1}},\\Sigma_{11})$.\nConditional Distributions\nShowing the conditional distribution is a bit long-winded, so bear with me. We are interested in finding the distribution of $y_{2}\\mid y_{1}$, which we can explicitly represent as\n$$\\begin{align} f_{y_{1}}(y_{2} \\mid y_{1}) = \\frac{f_{y_{1},y_{2}}(y_{1},y_{2})}{f_{y_{1}}(y_{1})} \\end{align}$$\nWriting out the joint density for $y$, we have the following\n$$\\begin{align} f(y) = \\frac{1}{(2\\pi)^{n/2}\\mid \\Sigma \\mid ^{1/2}}\\exp^{(-1/2)(y-\\mu)^{T}\\Sigma^{-1}(y-\\mu)} \\end{align}$$\nPartitioning this expression up into the individual terms related to $y_{1}$ and $y_{2}$, the exponent becomes\n$$ (y-\\mu)^{T}V(y-\\mu) = \\begin{bmatrix} y_{1} - \\mu_{1} \\\\\ny_{2} - \\mu_{2} \\end{bmatrix}^{T} \\begin{bmatrix} V_{11} \u0026amp; V_{12} \\\\\nV_{21} \u0026amp; V_{22} \\end{bmatrix} \\begin{bmatrix} y_{1} - \\mu_{1} \\\\\ny_{2} - \\mu_{2} \\end{bmatrix}$$\nExpanding this quadratic form out, we see that we end up with\n$$\\begin{align} (y_{1} - \\mu_{1})^{T} V_{11}^{-1}(y_{1}-\\mu_{1}) + 2(y_{1}-\\mu_{1})^{T}V_{12}(y_{2}-\\mu_{2}) + (y_{2} - \\mu_{2})^{T}V_{22}(y_{2}-\\mu_{2}) \\end{align}$$\nLet us, for simpliciy set $z_{1} = (y_{1} - \\mu_{1})$ and $z_{2} = (y_{2} - \\mu_{2})$. Substituting back in our definitions of $V_{11}$,$V_{12}$,$V_{21}$, and $V_{22}$, and and using the Sherman-Morrison-Woodbury definition for $V_{11}$, we have the following\n$$\\begin{align} \u0026amp;z_{1}^{T}(\\Sigma_{11}^{-1} + \\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}\\Sigma_{21}\\Sigma_{11})z_{1} \\\\\n\u0026amp;- 2z_{1}^{T}(\\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}^{-1})^{-1})z_{2} \\\\\n\u0026amp;+ z_{2}^{T}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1})z_{2} \\end{align}$$\nwhich, by distribution of $z_{1}$ across the first term and splitting the second term into its two sums, we have\n$$\\begin{align} \u0026amp;z_{1}^{T}\\Sigma_{11}^{-1}z_{11} + z_{1}^{T}\\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}V_{11}^{-1}\\Sigma_{12})^{-1}\\Sigma_{21}\\Sigma_{11}^{-1}z_{1} \\\\\n\u0026amp;- z_{1}^{T}(\\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1})z_{2} - z_{1}^{T}(\\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1})z_{2} \\\\\n\u0026amp;+ z_{2}^{T}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1})z_{2} \\end{align}$$\nWe can pull out forms $z_{1}^{T}\\Sigma_{11}^{-1}\\Sigma_{12}(\\Sigma_{22}-\\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}$ to the left and $(\\Sigma_{22}-\\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}z_{2}$ to the right and, after applying a transpose, have\n$$\\begin{align} =z_{1}^{T}\\Sigma_{11}^{-1}z_{11} + (z_{2} -\\Sigma_{21}\\Sigma_{11}^{-1}z_{1})^{T}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}(z_{2} - \\Sigma_{21}\\Sigma_{11}^{-1}z_{1}) \\end{align}$$\nPlugging the above back into our exponential term in our original density function, we see that we have a product of two exponential terms\n$$\\begin{align} \u0026amp;\\frac{1}{C_{1}} \\exp(\\frac{-1}{2}(z_{1}^{T}\\Sigma_{11}^{-1}z_{11})) \\\\\n\\end{align}$$\nand\n$$\\begin{align} \u0026amp;\\frac{1}{C_{2}}\\exp(\\frac{-1}{2}(z_{2} - z_{1}\\Sigma_{11}^{-1}\\Sigma_{12})^{T}(\\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})^{-1}(z_{2} - \\Sigma_{21}\\Sigma_{11}^{-1}z_{1})) \\end{align}$$\nwhere\n$$\\begin{align} C_{1} \u0026amp;= (2\\pi)^{p/2}\\mid \\Sigma_{11} \\mid^{1/2} \\\\\nC_{2} \u0026amp;= (2\\pi)^{q/2}\\mid \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12} \\mid ^{1/2} \\end{align}$$\nThe first term is the marginal density of $y_{1}$ and the second is the conditional density of $y_{2} \\mid y_{1}$ with conditional mean $\\mu_{2\\mid 1} = \\mu_{2} + \\Sigma_{11}^{-1}\\Sigma_{12}(y_{1} - \\mu_{1})$ and conditional variance $\\Sigma_{2\\mid 1} = \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12}$.\nWhile long and drawn out, the formulas show that the conditional distribution of any subset of Normal random variables, given another subset, is also a Normal distribution, with conditional mean and variance defined by functions of the means and covariances of the original random vector.\n","date":1526120054,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526243362,"objectID":"fcab78555bb9ec009b9e97e51770a67c","permalink":"/post/multivariate-normal-distribution/","publishdate":"2018-05-12T03:14:14-07:00","relpermalink":"/post/multivariate-normal-distribution/","section":"post","summary":"In this post, I\u0026rsquo;ll be covering the basics of Multivariate Normal Distributions, with special emphasis on deriving the conditional and marginal distributions.\nGiven a random variable under the usual Gauss-Markov assumptions, with $y_{i} \\sim N(\\mu, \\sigma^{2})$ with $e \\sim N(0,\\sigma^{2})$, and $N$ independent samples $y_{1}\u0026hellip;y_{n}$, we can define vector $\\mathbf{y} = [y_{1}, y_{2},\u0026hellip;y_{n}] \\sim N_{n}(\\mathbf{\\mu},\\sigma^{2}I)$ with $\\mathbf{e} \\sim N_{n}(\\mathbf{0},\\sigma^{2}I)$.","tags":[],"title":"Multivariate Normal Distribution","type":"post"},{"authors":[],"categories":[],"content":"In this post, I\u0026rsquo;m going to go over some examples of rank-one updates of matrices. To compute rank-one updates, we rely on the Sherman-Morrison-Woodbury theorem. From the previous post on [Blockwise Matrix Inversion]({% post_url 2018-05-08-blockwise-matrix-inversion %}), recall that, given a matrix and its inverse\n$$R = \\begin{bmatrix} A \u0026amp; B \\\\\nC \u0026amp; D \\end{bmatrix} \\; \\; \\; \\; R^{-1} = \\begin{bmatrix} W \u0026amp; X \\\\\nY \u0026amp; Z \\end{bmatrix}$$\nwe have that\n$$\\begin{align} W = (A-BD^{-1}C)^{-1} = C^{-1}D(D-CA^{-1}B)^{-1}CA^{-1} \\end{align}$$\nExpanding this further, the Woodbury formula proves the following identity\n$$\\begin{align} (A+BD^{-1}C)^{-1}=A^{-1}-A^{-1}B(D−CA^{-1}B)^{-1}CA^{-1} \\end{align}$$\nGiven an initial matrix $A$ and its inverse $A^{-1}$, and a new matrix $R=BD^{-1}C$, we see that we can define the inverse of our new updated matrix $A+R$ in terms of the inverse of our original matrix $A$ and components of $R$. Importantly, we can perform rank-$k$ updates, where $rank(R) = k$.\nFor example, if we want to update our matrix $A$ with a new vector, $v$, we can rewrite the formula above as follows:\n$$\\begin{align} (A+vv^{T})^{-1} \u0026amp;=A^{-1}-A^{-1}v(1+v^{T}A^{-1}v)^{-1}v^{T}A^{-1} \\\\\n\u0026amp;=A^{-1}-\\frac{A^{-1}vv^{T}A^{-1}}{1+v^{T}A^{-1}v} \\\\\n\\end{align}$$\nwhere the updated inverse is defined so long as the quadratic form $v^{T}A^{-1}v \\neq -1$.\n Rank-One Updates for Linear Models\nRecall the Normal equations for linear models:\n$$\\begin{align} X^{T}X\\beta = X^{T}y \\end{align}$$\nand\n$$\\begin{align} \\beta = (X^{T}X)^{g}X^{T}y \\end{align}$$\nwhere $X$ is our design matrix, $y$ is our dependent variable, and $\\beta$ is a solution to the Normal equation, due to the fact that the Normal equations are consistent. $(X^{T}X)^{g}$ is the generalized inverse of $X^{T}X$, which is unique (i.e. $(X^{T}X)^{g} = (X^{T}X)^{-1}$) only if $X$ has full column-rank. For our immediate purpose, we assume that $X$ has full column rank.\nAssume that we observe a set of observations, $X \\in R^{n \\times p}$ and response variable, $y$, and compute our coefficient estimates $\\hat{\\beta}$ via the Normal equations above, using $(X^{T}X)^{-1}$. Now given a new observation, $v \\in R^{p}$, how can we update our coefficient estimates? We can append $v$ to $X$ as\n$$ X^{\\text{*}} = \\begin{bmatrix} X \\\\\nv \\end{bmatrix} \\in R^{(n+1) \\times p}$$\nand directly compute $(X^{\\text{*T}}X^{\\text{*}})^{-1}$, or we can use the Sherman-Morrison-Woodbury theorem:\n$$ \\begin{align} (X^{\\text{*T}}X^{\\text{*}})^{-1} = (X^{T}X + vv^{T})^{-1} = (X^{T}X)^{-1} - \\frac{(X^{T}X)^{-1}vv^{T}(X^{T}X)^{-1}}{1+v^{T}(X^{T}X)^{-1}v} \\\\\n\\end{align} $$\nfrom which we can easily compute our new coefficient estimates with $$.\n$$\\begin{align} \\beta^{\\text{*}} = (X^{\\text{*T}}X^{\\text{*}})^{-1}X^{\\text{*}T}y \\\\\n\\end{align}$$\nImportantly, in the case of regression, for example, this means that we can update our linear model via simple matrix calculations, rather than having to refit the model from scratch to incorporate our new data. In the next few posts, I\u0026rsquo;ll go over an example of an implementation of rank-updating methods that I\u0026rsquo;ve been using in lab to study brain dynamics.\n","date":1526082825,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526169225,"objectID":"324aaba31aef439d2b193812e0d314a6","permalink":"/post/rank-one-updates/","publishdate":"2018-05-11T16:53:45-07:00","relpermalink":"/post/rank-one-updates/","section":"post","summary":"In this post, I\u0026rsquo;m going to go over some examples of rank-one updates of matrices. To compute rank-one updates, we rely on the Sherman-Morrison-Woodbury theorem. From the previous post on [Blockwise Matrix Inversion]({% post_url 2018-05-08-blockwise-matrix-inversion %}), recall that, given a matrix and its inverse","tags":[],"title":"Rank One Updates","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m taking a Statistics course on the theory of linear models, which covers Gauss-Markov models and various extensions of them. Sometimes, when dealing with partitioned matrices, and commonly Multivariate Normal Distributions, we\u0026rsquo;ll often need to invert matrices in a blockwise manner. This has happened often enough during this course (coincidentally was necessary knowledge for a midterm question), so I figured I should just document some of the inversion lemmas.\nLet\u0026rsquo;s define our partitioned matrix as\n$$ R = \\begin{bmatrix} A \u0026amp; B \\\\\nC \u0026amp; D \\end{bmatrix}$$\nWe specifically interested in finding\n$$ R^{-1} = \\begin{bmatrix} W \u0026amp; X \\\\\nY \u0026amp; Z \\end{bmatrix}$$\nsuch that\n$$ R R^{-1} = R^{-1}R = \\begin{bmatrix} I \u0026amp; 0 \\\\\n0 \u0026amp; I \\end{bmatrix}$$\nPart 1: $R R^{-1}$\nFor the right inverse ($R R^{-1}$), we can define\n$$ \\begin{aligned} AW + BY = I \\\\\nAX + BZ = 0 \\\\\nCW + DY = 0 \\\\\nCX + DZ = I \\\\\n\\end{aligned} $$\nand, assuming $A$ and $D$ are invertible,\n$$\\begin{aligned} X = -A^{-1}BZ \\\\\nY = -D^{-1}CW \\\\\n\\end{aligned}$$\nWe can plug these identities back into the first system of equations as\n$$\\begin{aligned} AW + B(-D^{-1}CW) \u0026amp;= (A - BD^{-1}C)W = I \\\\\nC(-A^{-1}BZ) + DZ \u0026amp;= (D - CA^{-1}B)Z = I \\\\\n\\end{aligned}$$\nso that\n$$\\begin{aligned} W = (A-BD^{-1}C)^{-1} \\\\\nZ = (D-CA^{-1}B)^{-1} \\\\\n\\end{aligned}$$\nand finally\n$$ R^{-1} = \\begin{bmatrix} W \u0026amp; X \\\\\nY \u0026amp; Z \\end{bmatrix} = \\begin{bmatrix} (A-BD^{-1}C)^{-1} \u0026amp; -A^{-1}B(D-CA^{-1}B)^{-1} \\\\\n-D^{-1}C(A-BD^{-1}C)^{-1} \u0026amp; (D-CA^{-1}B)^{-1} \\\\\n\\end{bmatrix}$$\nIt is important to note that the above result only holds if $A$, $D$, $(D-CA^{-1}B)$, and $(A-BD^{-1}C)$ are invertible.\nPart 2: $R^{-1} R$\nFollowing the same logic as above, we have the following systems of equations for the left inverse ($R^{-1}R$)\n$$\\begin{aligned} WA + XC = I \\\\\nWB + XD = 0 \\\\\nYA + ZC = 0 \\\\\nYB + ZD = I \\\\\n\\end{aligned}$$\nso that\n$$\\begin{aligned} X = WBD^{-1} = A^{-1}BZ \\\\\nY = ZCA^{-1} = D^{-1}CW \\\\\n\\end{aligned}$$\nwhich indicates that\n$$\\begin{aligned} W = (A-BD^{-1}C)^{-1} = C^{-1}D(D-CA^{-1}B)^{-1}CA^{-1} \\\\\nX = (A-BD^{-1}C)^{-1}BD^{-1} = A^{-1}B(D-CA^{-1}B)^{-1} \\\\\n\\end{aligned}$$\nImportantly, blockwise matrix inversion allows us to define the inverse of a larger matrix, with respect to its subcomponents. Likewise, from here, we can go on to derive the Sherman-Morrison formula and Woodbury theorem, which allows us to do all kinds of cool stuff, like rank-one matrix updates. In the next few posts, I\u0026rsquo;ll go over a few examples of where blockwise matrix inversions are useful, and common scenarios where rank-one updates of matrices are applicable in the next few posts.\n","date":1525847057,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528525457,"objectID":"35851f40f2bddca47a9fbbb049a6e2f7","permalink":"/post/blockwise-matrix-inversion/","publishdate":"2018-05-08T23:24:17-07:00","relpermalink":"/post/blockwise-matrix-inversion/","section":"post","summary":"I\u0026rsquo;m taking a Statistics course on the theory of linear models, which covers Gauss-Markov models and various extensions of them. Sometimes, when dealing with partitioned matrices, and commonly Multivariate Normal Distributions, we\u0026rsquo;ll often need to invert matrices in a blockwise manner.","tags":[],"title":"Blockwise Matrix Inversion","type":"post"},{"authors":["Kristian M. Eschenburg","David Haynor","Tom Grabowski"],"categories":null,"content":"","date":1520812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520812800,"objectID":"eb6d35c550f525fa9ed0e290f6d86c9c","permalink":"/publication/cbp/cbp/","publishdate":"2018-03-12T00:00:00Z","relpermalink":"/publication/cbp/cbp/","section":"publication","summary":"In this analysis, we propose the use of a library of training brains to build a statistical model of the parcellated cortical surface to act as templates for mapping new MRI data.","tags":["clustering","deep learning","segmentation","Human Connectome Project"],"title":"Automated Connectivity-Based Cortical Mapping Using Registration-Constrained Classification","type":"publication"},{"authors":null,"categories":null,"content":" Society for Industrial and Applied Math (SIAM): Computer Science and Engineering. Minisymposium. Advances in Dynamic Graphs: Algorithms, Applications, and Challenges. Atlanta, Georgia. 2017.  ","date":1488153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488153600,"objectID":"57e221b7f25507c58d3f9b0e56704f9c","permalink":"/talk/siam/","publishdate":"2017-02-27T00:00:00Z","relpermalink":"/talk/siam/","section":"talk","summary":" Society for Industrial and Applied Math (SIAM): Computer Science and Engineering. Minisymposium. Advances in Dynamic Graphs: Algorithms, Applications, and Challenges. Atlanta, Georgia. 2017.  ","tags":null,"title":"","type":"talk"},{"authors":["Boris Gutman","Cassandra Leonardo","Neda Jahanshad","Derek Hibar","Kristian Eschenburg","Talia Nir","Julio Villaon-Reina","Paul Thompson"],"categories":null,"content":"","date":1421280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1421280000,"objectID":"5888e35cf8c7eaf81a9f4b527053d7d4","permalink":"/publication/cca/cca/","publishdate":"2015-01-15T00:00:00Z","relpermalink":"/publication/cca/cca/","section":"publication","summary":"We present a framework for registering cortical surfaces based on tractography-informed structural connectivity. We define connectivity as a continuous kernel on the product space of the cortex, and develop a method for estimating this kernel from tractography fiber models.","tags":["continuous connectomics","Alzheimers","surface registration"],"title":"Registering Cortical Surfaces Based on Whole-Brain Structural Connectivity and Continuous Connectivity Analysis","type":"publication"},{"authors":["Kenia Martinez","Julio Villalon-Reina","Jose Peneda-Pardo","Dominique Kessel","Anand Joshi","Kristian Eschenburg","Neda Jahanshad","Francisco Roman","Miguel Burgaleta","Paul Thompson","Roberto Colom"],"categories":null,"content":"","date":1394582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1394582400,"objectID":"e77afa7744d5c02c1444e6d62f8d8c71","permalink":"/publication/fluid_intelligence/fluid_intelligence/","publishdate":"2014-03-12T00:00:00Z","relpermalink":"/publication/fluid_intelligence/fluid_intelligence/","section":"publication","summary":"Here, we show that Exploratory Factor Analysis (EFA) reveals structural connectivity subnetworks related to a broad range of cognitive constructs, including general, fluid, crystallized, and spatial intelligence, along with working memory capacity, attention, and processing speed.","tags":["exploratory factor analysis","intelligence","structural brain connectivity","diffusion MRI"],"title":"Individual Differences in General/Fluid Intelligence Are Evoked by Functional Integration and the Efficiency of Long-Distance Connections in the Brain","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"/cv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/","section":"","summary":"","tags":null,"title":"","type":"page"},{"authors":["Kristian Eschenburg","Julio Villalon-Reina","Neda Jahanshad","Talia Nir","Madelaine Daianu","Cassandra Leonardo","Stella de Bode","Susan Y. Bookheimer","Noriko Salamon","Paul Thompson"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"da39720b39377062fde1e56f48a6fef8","permalink":"/publication/hemi/hemi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/hemi/hemi/","section":"publication","summary":"Using diffusion tensor imaging, we analyzed structural brain networks using various topological measures derived from brain connectivity matrices, to understand neural architecture in children with one brain hemisphere.","tags":["hemispherectomy","diffusion MRI","graph theory","epilepsy"],"title":"Analysis of Structural Connectivity in 6 Cases of Hemispherectomy","type":"publication"}]